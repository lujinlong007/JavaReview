
## 服务容器化章节概述
    承接微服务改造进入应用部署阶段
    架构师角度出发 分析问题和筹谋应付
    横向比较热门技术 纵向探讨容器发展
    从应用架构向企业架构过度
## 微服务架构落地
       环境需求差异大：Cpu业务型 GPU计算型  高吞吐IO型
       服务敏捷性要求高 成千上百 快速启动 优雅停止
       组织架构变化 产品导向 DevOps文化 团队微小化
## 服务容器化-三大助力
       码头林立-各种软硬件平台层出不穷 
       微服务 -高内聚 低耦合 分钟启停和部署
       康威定律-组织决定上层建筑
## 容器演进之路和技术选型
       服务容器化技术演进之路
       20年前IBM(狼人大型机) -> WPAR
       智人 -->      Namespace Cgroup
       山顶洞人 -->  LXC
       现代人 --> Docker,Cloud Foundry
       平台化->编排、抽象 Pass公有云服务
       无服务器化->  Lamdba Function
       生态圈化 Docker容器一统天下
## 如何选择容器技术      
     Docker SWOT分析
      优势  生态圈 环境准备 镜像胚胎  业界技术
      劣势  业务DIY（密钥管理 用户认证 加密）  网络和存储（编排 统一平台）  非功能需求（安全性 加密 高可用 并发）
      机会  江山一统 第三方Paas
      威胁  风口变化 公有云Pass  方兴未艾  
     Cloud Foundry SWOT 分析
     优势 多语言发布 网络 安全 高可用 自伸缩 可扩展
     劣势 开放性较小 前期准备 服务定制 中文文档
     机会 多公司加入生态圈  混合云环境
     威胁 风口变化 Docker江山一统
    资源管理和容器编排
     容器&编排 vs 剑&剑谱 
     验资质：资源分配、兼容性判断
     学剑招：部署、回滚
     练套路：服务发现、应用依赖
     悟变通：健康检查、弹性伸缩
     道法术：业务、软件包、应用服务
     一传十：配置管理、快速复制
    编排技术
     Kubernetes(k8s)
     Mesos(管理)+Marathon（长作业调度）
     Docker Swarm   
    如何选择容器编排技术
     Mesos Marathon SWOT 分析
      优势： 资源统一管理 多框架并存 快速上手 部署简单
      劣势：不适合混合云  文档相对较少 功能覆盖不全
      机会： 云大数据融合 新的调度框架
      威胁： 风口变化 k8s江山一统 
     kubernetes SWOT分析
       优势  混合云模式  技术普及率  存储管理  弹性伸缩
       弱势  仅为容器服务  安装相对复杂 大数据场景
       机会  江山一统  业界标准
       威胁  风口变化  Serverless
## docker 
    Docker-整体架构&功能模块
       微小  敏捷  通用 
       简单命令
        docker ps
        docker images
        docker ps -a（查看所有运行过的docker）
       Hello World
          docker run hello-world
       Redis 高速缓存服务
         docker run -d redis:3.2 redis-server  
         docker exec -it d86cd855fc54 redis-cli  
         set name feiyang  
         get name  
         exit 
       Nginx高性能http&反向代理服务 
         docker run -d -p 80:80 nginx
    Docker-环境准备&核心基本功
       Docker 整体架构
         Docker Client/Server架构
         Docker客户端 docker 命令 docker Api  Docker_Host(Docker daemon) 
         Dcoker服务端  docker Daemon-dockerd  Registry
         docker daemon 配置文件 /lib/systemd/system/docker.service
         docker镜像
           容器静态版本
           docker命令 docker commit  打包镜像提交到仓库
           构建文件 Dockerfile 每一步生成镜像
           完成后 开辟可读写层 使用读写 关闭后不会影响镜像
         docker镜像仓库 
           docker hub docker官方公有仓库
           docker datacenter docker信任仓库（企业版功能）
           docker 私有仓库   
         docker容器
            隔离-namespace(pid,net,ipc,mnt,uts)  
            限制-cgroup(cpu,mem.io)
            文件系统-UnionFs（AUFS,btrfs ,vfs,DeviceMapper）  
       数据源准备
         curl -fsSL get.docker.com -o get-docker.sh
         sh get-docker.sh --mirror Aliyun
         systemctl daemon-reload
         docker version  
       Docker 容器生命周期管理以及状态机模型
           docker pull nginx
           docker create nginx  
           docker start 398edf535ba8  
           docker stop 398edf535ba8  
           docker kill 398edf535ba8  
           docker start 398edf535ba8  
           docker pause 398edf535ba8  停止
           docker unpause 398edf535ba8  解开停止 
           docker exec -it 398edf535ba8 /bin/bash  （交互）
           exit  
           docker rm -f 398edf535ba8  强制删除  目录消失
           docker run -d redis  后台运行
           docker logs -f f13ba5508331  查看log 
           images-> docker create(created)->docker start(running)
       Docker Container 之隔离与限制
          docker run -d -c 4000 nginx  限制cpu
          docker run -d -m 200M --memory-swap=400M nginx   限制内存
          docker run -d --blkio-weight 300 nginx   限制Io
       Dockerfile 实战   
         最坑爹RUN CMD ENTRYPOINT     
         1Dockerfile 文件
         #Owned  by lujinlong
         FROM debian
         MAINTAINER jinlong
         RUN mkdir test1
         RUN touch test2
         COPY test3 . 
         ADD test4.tar.gz .
         ENTRYPOINT ["/bin/sh"]
         CMD ["-c","ls -l"]
         2docker build -t mysh .
         3docker run mysh 
          docker run mysh -c date    
       Docker 网络管理  
          默认网络  
          none 网络 
          host网络  服务器和容器共享网络 sh 22 服务器和容器进行端口争夺
          bridge网络 docker0网桥
          自定义Brige 网络
          自定义 overlay网络
          自定义 macvlan网络
          第三方网络 
           flannel网络  etcd
           weave网络   
           calico网络  
          网络方案 分类比较
            单主机 
             none host brige
             实现方式 对应主机网卡或端口  
             性能高 
             不适合跨主机沟通
             网络 不能
             域名访问 不能
             ip地址分配 无
             网络隔离 是
           Overlay 
             overlay flannel weave
             实现方式 VsLAN大二层 
             中 
             沟通好
             共享ip 
           Underlay
              macvlan calico
              实现方式 传统二层 三层技术
              性能高
              沟通中
              nat+端口映射
       容器生态
         主导之争 
            Google  
            Docker 
            CoreOS
         容器编排之争
            Google kubernetes
            Apache Mesos+Mesosphere Marathon
            Docker Swarm
         Docker Pass 公有云之争
            Google Cloud Platform
            Amazon Web Services 
            Microsoft Azure   
            Aliyun  容器ACK&ACS
            腾讯云   TKE
            百度云 容器引擎 CCE 和 容器实例 BCI 
    微服务的落地需求  foodie-cloud          
      先确保RabbitMQ，Redis和Mariadb/MySQL处于启动状态    
           docker run -d --name myrabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:management  
           docker run -d -p 6379:6379 --name myredis redis redis-server  
           docker run -d -p 3306:3306 --name mymysql -e MYSQL_ROOT_PASSWORD=imooc mysql  
      启动Eureka - 所有微服务和SC平台组件都依赖Eureka做服务注册 
        cat Dockerfile  
           FROM java:8  
           ADD registry-center-1.0-SNAPSHOT.jar registry-center-1.0-SNAPSHOT.jar  
           ENTRYPOINT ["java","-jar","registry-center-1.0-SNAPSHOT.jar"]  
         docker build -t myregistry .  
         docker run -d -p 20000:20000 --name myregistry myregistry   
      启动Config-Server - 部分微服务依赖配置中心拉取配置项  
         修改application.yml:
          rabbitmq:host:172.19.46.183,
         eureka:
            client:
              serviceUrl:
               defaultZone: http://172.19.46.183:20000/eureka/
            instance:
              instance-id: ${eureka.instance.ip-address}:${server.port}
              ip-address: 172.19.46.183
              prefer-ip-address: true
          Dockfile文件类似 文件名不同    
         docker run -d -p 20003:20003 --name myconfig myconfig  
      启动Hystrix监控模块 - Turbine和Hystrix-Dashboard
         等到后续微服务注册到注册中心后，Turbine下次做服务发现之后就可以正常收集数据了 
         docker run -d -p 20001:20001 --name myturbine myturbine
         docker run -d -p 20002:20002 --name mydashboard mydashboard 
      启动链路追踪组件 - Zipkin和ELK容器 
         docker run -d -p 20005:20005 --name myzipkin myzipkin
      依次启动Auth微服务 -> User微服务 -> Item微服务 -> Cart微服务 -> Order微服务
        Auth微服务
         修改application.yml: redis:host:172.19.46.183,注释password, zipkin:base-url:http://172.19.46.183:20005/, 
         以及修改Eureka部分
         docker run -d -p 10006:10006 --name myauth myauth  
        User微服务
          修改application-dev.yml: datasource:url: jdbc:mysql://172.19.46.183:3306/foodie_shop_dev?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true,password:imooc, redis:host:172.19.46.183,注释password, zipkin:base-url:http://172.19.46.183:20005/, rabbitmq:host:172.19.46.183
          以及修改Eureka部分
          docker run -d -p 10002:10002 --name myuser myuser 
        Item微服务
          同User微服务配置
          docker run -d -p 10001:10001 --name myitem myitem
        Cart微服务
          docker run -d -p 10004:10004 --name mycart mycart
        Order微服务
          docker run -d -p 10003:10003 --name myorder myorder
      最后启动Gateway网关 - 在微服务都启动好之后再启动网关，可以保证网关启动后立即生效。反过来先启动网关再注册微服务也行，但是Gateway会处于短暂的不可用状态，因为Gateway启动的时候微服务还没注册，需要等Gateway做服务发现后才能生效
        docker run -d -p 20004:20004 --name mygateway mygateway
      准备数据  
         cd /usr/local/mysql/bin
         ./mysql -h 101.133.136.40 -u root -p imooc
          CREATE DATABASE IF NOT EXISTS foodie_shop_dev DEFAULT CHARSET utf8 COLLATE utf8_general_ci;
      启动前端服务  
         下载foodie-shop前端代码，上传至/root/foodie-cloud/foodie-shop目录  
         docker run --name myfront -p 8080:80 -v /root/foodie-cloud/foodie-shop:/usr/share/nginx/html -d nginx
         修改前端foodie-cloud/foodie-shop/js/app.js,使它指向gateway的外网地址：  
          serverUrl: "http://101.133.136.40:20004" 
## Cloud Foundry
    cloud foundry章节概述
      承接Docker技术 提供容器部署的另一分支解决方案
      从开源技术到商业产品实战 夯实基础
      逐步想后续的容器编排和弹性扩缩容延申
      从应用架构向企业架构过度  
    Cloud Foundry 整体架构&功能模块
      Docker与Cloud Foundry分层模式
        Docker      Application
              App Server(Tomcat)  Cloud Foundry
              Runtime(JVM)
              Container
                VM
               Infrastructure   
      整体架构
         底层 各种云
        BOSH 层解耦
        CELL 虚拟机 Rounter对外提供
        Cloud Controller  SERVICE Broker
        日志控制层
        业务层
        路由层   
        diego 框架  管理 cell  
        database  数据库记录发布过程
        brain 统一管理           
      发布时序图
        cf push -> CommandLine->测试发布->正式发布
        Buildpack 
          兼容多种语言 多种部署环境 自动识别
        Manifest
          描述文档   
      组织权限
      
        org ->space->application-> (domain.route)提供对外访问     
    Cloud Foundry环境搭建及其基本使用
      前置准备

        apt-get install qemu-kvm qemu-system libvirt-bin virt-manager bridge-utils vlan
      CLI准备
        从官网下载cf-cli命令工具和pcfdev软件包，并按照文档进行安装：  
        https://network.pivotal.io/products/pcfdev 
        cf-cli_6.46.1_osx.tgz 和 pcfdev-v1.2.0-darwin.tgz，然后进行安装
      PCF dEV下载安装
         tar zxvf cf-cli_6.46.1_osx.tgz
         cp cf /usr/local/bin
         cf install-plugin -r CF-Community cfdev
         cf dev start -f pcfdev-v1.2.0-darwin.tgz 
      CLI和GUI 访问 
         cf login -a https://api.dev.cfdev.sh --skip-ssl-validation
         Admin user => Email: admin / Password: admin
         Regular user => Email: user / Password: pass  
         GUI访问 https://apps.dev.cfdev.sh
         下载测试项目Music
         git clone https://github.com/cloudfoundry-samples/spring-music
         cd ./spring-music
         ./gradlew assemble
         cf push --hostname spring-music 发布项目
         cf logs spring-music --recent 查看日志 
      Cloud Foundry容器生命周期管理 
        cf apps
        cf app spring-music
        读取应用配置参数
        cf env spring-music  
        设置应用配置参数实时生效
        cf set-env spring-music username feiyang  
        重新打包 cf restage  spring-music 
        删除应用cf delete spring-music
        cf logs spring-music 查看日志（以tail -f 查看日志 ）
        cf logs spring-music --recent
        启动两个实例 并修改内存为1200M
        cf scale -i 2 -m 1200M spring-music
      隔离和通信
        Route绑定
        三个概念 Domain & Hostname &Path  
        cf routes
        cf create-domain cfdev-org  music.com  （绑定domain 到orgs）
        cf domains
        cf map-route spring-music music.com -n lujinlong
        修改hosts文件 
        解除绑定
        cf unmap-route spring-music music.com -n lujinlong
        删除路由 cf delete-route music.com -n album
        删除domain
        cf delete-domain music.com    
      Concourse蓝绿发布  
          build-app->（app-repo）->  deploy-app->(staging-app)->  promote-new-version  

           Url-->映射关系
          CF Router  ->  blue/green
          绑定对外访问的 route
          cf map-route spring-music-blue dev.cfdev.sh -n spring-music 
          cf map-route spring-music-green dev.cfdev.sh -n spring-music 
          绿色测试成功后 卸载蓝色映射
          cf unmap-route spring-music-blue dev.cfdev.sh -n spring-music    
      绑定基础架构服务
        数据服务
          应用依赖如何解决？
           服务市场 marketPlace
            服务目录 create-service-broker&marketplace
            服务部署 create-service&delete-service
            服务绑定 bind-service&unbind-service
           用户自定义服务
             服务自定义 create-user-provided-service(cups)
             服务绑定 bind-service&unbind-service
         服务demo
           市场服务-mysql
             cf dev deploy-service mysql
             cf marketplace 
             cf create-service p.mysql db-small mysql
             cf services
             cf bind-service spring-music-green mysql
           用户自定义服务-pubsub     
             cf cups pubsub -p "host,port,username,password"
             cf bind-service spring-music-green pubsub           
      高可用和业务连续性      
         应用故障
           如果一个应用故障，PCF会在一个新的容器中重启应用 先在本地重启
           Diego cell
           如果进程故障，PCD会在新的的虚拟机重启这个进程
           如果是虚拟机的操作系统 网络等故障，PCF会关闭虚拟机，通过一个虚拟机模板重启一个克隆的虚机
         机器组故障
            如果一组物理机器故障 PCF还可以通过和高可用性区 确保PCF和应用继续运行
             avalible Zone 三个
         服务故障
            服务后台 高可用集群架构+负载均衡
            服务连接 虚拟机IP 和URL  hostname uri    
      应用和平台管理
         Cloud Foundry应用管理
            CLi 和 Apps Manager
         Bosh 平台管理   Ops Manager   
            Paas用户   数据库/大文件存储/健康中心->虚拟机管理器->虚机管理软件  
           cli->director->iass->create Vms
           Blobstore 
    Cloud Foundry 实战 电商微服务部署
      auth微服务
        1cat manifest.yml  
        applications:
         - name: myauth
           memory: 1G
           path: foodie-auth-service-1.0-SNAPSHOT.jar
           services:
            - myregistry
            - myredis
            env:
            JBP_CONFIG_SPRING_AUTO_RECONFIGURATION: '{enabled: false}' 
         2修改总的pom.xml  
            <!-- SpringCloud from Pivotal Cloud Foundry-->
            <dependency>
                <groupId>io.pivotal.spring.cloud</groupId>
                <artifactId>spring-cloud-services-dependencies</artifactId>
                <version>3.1.0.RELEASE</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency> 
            auth.pom.xml
              <dependency>
              <groupId>io.pivotal.spring.cloud</groupId>
               <artifactId>spring-cloud-services-starter-service-registry</artifactId>
             </dependency>       
          3import org.springframework.cloud.client.discovery.EnableDiscoveryClient;
          //import org.springframework.cloud.netflix.eureka.EnableEurekaClient;
          @EnableDiscoveryClient
          4配置前端项目 cf push myfront -b staticfile_buildpack 
## Mesos+marathon
     章节概述
       承接溶剂技术章节 提供容器编排解决方案
       从开源组件Mesos和Marathon入手 夯实基础
       逐步向后续的弹性扩缩容延申
       从应用架构向企业架构过度        
     Mesos&Marathon 整体架构&功能模块
       物理机->Hypervisor->虚拟机->操作系统->应用（小心脏）
       物理机 操作系统 （Mesos）->Marathon->应用(大心脏)
     
       cli/gui->Mesos Master-(offers)>Marathon Scheduler(standby主备节点) (Zookeeper保证高可用)  Control plan
       
       Mesos Slave (Docker Executor) 供给Mesos Master 供给给Marathon
       两层向下调度 两层资源向上供给         
 
       Zookeeper集群
          zookeeper 自身高可用
          Mesos master领导者选举
          节点数目（2n+1）和Quorum数目（n+1）  
       Mesos Master  
          主备模式
          沟通Mesos slave 获取资源
          沟通framework 提供资源
       Mesos Slave
          为各种框架提供执行器环境
          沟通Mesos Master 提供资源
          执行任务和容器管理  
       Marathon 调度器
          一种主流框架的调度器
          对接Mesos Master 提供容器编排和应用调度
          与Mesos Slave 端的执行器紧密呼应    
     Mesos安装部署及核心功能
       虚机环境准备
        Mesos Marathon 部署
        Install VirtualBox
        Install Vagrant     
        git clone https://github.com/mesosphere/playa-mesos.git
        cd playa-mesos 
        bin/test
        开启虚拟机
        vagrant up
        连接虚拟机
        vagrant ssh
        准备docker镜像
        sudo docker pull mesosphere/marathon-lb
        sudo docker pull nginx:1.9
        发布nginx应用
        nginx.json
        {
          "id": "nginx",
          "cpus": 0.2,
          "mem": 20.0,
          "instances": 2,
          "healthChecks": [{ "path": "/"}],
          "container": {
             "type": "DOCKER",
             "docker": {
                image": "nginx:1.9",
                "network": "BRIDGE",
                 "portMappings": [{"containerPort":80,"hostPort":0,"servicePort":80,"protocol":"tcp"}]
              }
        }
        发布应用到marathon
        curl -i -H 'Content-Type: application/json' 10.141.141.10:8080/v2/apps -d @nginx.json
       Mesos 原理剖析
       来源：Mesos Slave
       分类：cpus,mem,disk,ports
       单位： 数量 MB 范围
       资源分配算法
       最大最小公平算法
       DRF:优势资源公平算法（默认）
       Custom Allocator 定制分配器 
       资源分配调整
        角色：--roles "dev,stage,prod"
        权重：--weight="dev=10,stage=20,prod=70"
        资源保留：--resources="cpus(prod):8;mem(prod):16384"
       资源分配过程
       Frame A (Role:prod)  Frame B Role:dev
           ||
       Mesos Master(Allocation module)
           ||
       Mesos slave() cpus(prod):8;mem(prod):16384
       资源隔离
        Containerizers:Mesos,Docker
          cgroups/cpu ,cgroups/mem
       集群和资源高可用
          posix/cpu,posix/mem   
      
       DCOS案例分享
        Data Center Operation System (Mesosphere)
         三层架构
          Software
          MesosPhere
         Infrasture
 
        DCOS
          resource Management : Mesos
          process Management: Docker
          job Scheduling :Marathon,Chronos
          Inter-process Communication: RabbitMQ
          File Sytem: HDFS,CEPH
        国内运营商的具体实现
          Dcos 管理平台 : Dashboard 资源配置模块 鉴权模块 统一日志中心
                  弹性扩缩容模块 监控管理模块 持续集成平台
         应用容器化（Docker封装）:  应用(docker)
         服务注册服务引流 ：Etcd  Haproxy
         资源调度任务调度  Marathon&mesos(zookeeper)
         物理机或虚拟节点  
       Marathon调度原理
        概念介绍
          应用&应用组
          应用实例：Mesos任务
          部署：应用的创建，销毁，扩缩容
        调度功能：
           常驻服务生命周期管理：suspend,restart,destroy,scale  
           依赖管理：dependencies
           滚动升级：minimumHealthCapacity(1&0) 0.4
           沙箱管理: uris
           约束管理：constraints
           Docker容器支持：--containerizers=docker,mesos      
       Marathon应用隔离
        Marhthon应用实例-容器隔离
           cgroup &docker
        Mesos任务-沙盒隔离
          不同应用模式-框架隔离      
       应用依赖
         Marathon应用组 Application Group
         应用组内依赖
         应用组间依赖  
       高可用容器集群编排管理
        服务发现和路由
          Mesos-DNS 半轮询
          Haproxy 实时
        容器集群高可用
          Marathon-LB容器部署
          Nginx容器部署
        容器集群高可用访问  
     Mesos&Marathon实战0电商微服务部署   
       1. 修改redis server IP 
         redis:
           host: 10.141.141.10
       2. 注释掉zipkin部分  
       3. 修改eureka和在eureka注册的本服务IP地址         
           eureka:
             client:
              serviceUrl:
                 defaultZone: http://10.141.141.10:20000/eureka/
       4. 编译 mvn install 拷贝到服务器
          mvn install
          scp registry-center-1.0-SNAPSHOT.jar MyPackages/foodie-auth-service-1.0-SNAPSHOT.jar root@10.141.141.10:/root/microservice/ 
       5准备java:8镜像
          docker pull v6atfsm9.mirror.aliyuncs.com/library/java
          docker tag v6atfsm9.mirror.aliyuncs.com/library/java:latest java:8   

          dockerfile 并打包镜像
          docker build -t myregistry .
       6图形化界面发布应用8080端口
          Open mesos: http://10.141.141.10:5050/#/  
          Marathon: http://10.141.141.10:8080/ui/#/apps  
         In marathon GUI, create application     
## kubernetes
    本章节概述
      承接Mesos Marathon章节 提供另一种主流编排方案
      从Kubernates原理到实战夯实基础
      逐步向后续的弹性扩缩容延伸
      从应用架构向企业架构过度
    k8s整体架构&功能模块
      k8scluster 
       k8sMaster(api server&replication Controller) 大脑
         Api server
         Scheduler 按照资源进行任务发布 统一调度
         Controller Manager-replication(容器跨节点部署，标签管理 资源选择)/namespace （虚拟化集群 资源隔离）
         Controller         支持模块
         Etcd (zk)
         Network-flannel,Calico,canal
         Node Components
      Node(Container(pod) kubelet& kube-proxy&docker) 手
          Kubelet(容器启停 加网络资源 )脑电波
          Kube-proxy 神经元 （网络感知模块） 网络管理 服务发现
          Docker 
      Pod (运行在一个节点上一堆容器的集合) 大的集装箱
         k8s最小工作单元（一堆容器）
         运行在一个Node上
         Pod中的容器共享网络和存储
         例子：消息队列 发送节点 接受节点
         一旦发布只能在一个节点上
      Controller  管理Pod 的抽象
         Deployment
         ReplicaSet 多个节点部署相同的pod
         DaemonSet 磁盘清理应用（保证pod启动一个 多了会发生死锁）
         StatefulSet 保证pod名称不变
         Job batchjob 短跑
      Service
         Dns Server  负载均衡 网络管理

      label 
         pod 特殊标签 myapp  
      Namespace（Controller上层）
         两个虚拟集群 物理集群虚拟化  
    Pod调度原理分析
      Pod如何管理多个容器
       namespace cgroup 隔离一个环境  共享网络资源 相同的物理卷
       在一个物理节点存在
       pod生命周期
       apply  pending  running  Succeed  failed
       停止POD小技巧：  
       kubectl scale --replicas=0 deployment/nginx-deployment
      资源限制
        资源标签 ： kubectl label
        资源限制 : nodeSelector
        指定node节点小技巧：  
        kubectl get node --show-labels
        Kubectl label node training4 disktype=ssd
       修改mynginx-pod.yaml文件，添加：  
         nodeSelector:
            disktype: ssd
       调度约束
         pod标签：meta:label:xxx:yyy
         调度约束 spec:selector:xxx:yyy
         调度约束：spec:Selector:matchLabels:XXX:YYY
       健康检查
         LivenessProbe(容器级别) 高可用
             exec :command 从容器中建立一个文件
         ReadinessProbe(容器级别)  发现出错不提供服务
           httpget:port&path
             initialDepalySeconds停止多长时间
             periodSeconds 多长周期重新检查
         restartPolicy(Pod级别)      
    Controllers
        ReplicaSet(多个pod 多副本) 发布到多个节点
        Deployment(负责pod发布) 迭代升级 回滚 更新
        Job(短作业)
        StatefulSet 正常情况下每次发布pod名称会变  保证pod名称不变
        DaemonSet 值班人员 Node节点只跑一个应用 端口冲突  
        Deployment->ReplicaSet->Pod
        kubectl->deploy(nginx-deployment)->ReplicaSet(nginx-deployment-xxxxx)
           ->Pod(nginx-deployment-xxxxx--aaaa)&
        kubectl scale --replicas=5 deployment/nginx-deployment
        kubectl get deployment
        kubectl get replicaset
        滚动升级
           升级：更新镜像+kubectl apply
           更新yaml
        查询：kubectl rollout history deployment
             kubectl rollout history deployment
             kubectl apply -f mynginx-deployment.yaml  --record  
        回滚: kubectl rollout undo deployment
              kubectl rollout undo deployment --to-revision=1

        Daemonset
           kubectl get daemonset --namespace=kube-system  
           kubectl edit daemonset kube-proxy --namespace=kube-system
           kubectl get pod --namespace=kube-system -o wide | grep kube-proxy
            一个节点只能跑一个

        Batch Job
            kubectl logs  hello-p4wrv 跑一次 并发
        CronJob 
           定时反复跑        
    报错经验分享
       1初始化错误
          Kubeadm init
          kubeadm config images pull
          failed to pull image k8sgcr.io
         解决方案：
            国内镜像服务器
            直接连接国外镜像
        2证书错误
           unable to connect to the Server:x509
            三个命令
              mkdir -p $HOME/.kube
              cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
              chown $(id -u):$(id -g) $HOME/.kube/config
        3网络错误
               unable to update cni configNo networks  found in /etc/cni/net
              kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  
        4配置生效错误
            终端报错 修改生效
            常见错误 大小写 空格 对齐 拼写 apiVersion
             列表 agrs(/bin/sh -c)
        5配置修改错误
         kubectl  edit
            
            出错原因 文件前几行提示 大小写错误 端口被占用          
    k8s环境准备&存储、网络、监控
      前置准备
        关闭Selinux  
          setenforce 0
          sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
        更换docker镜像源
           安装Kubernetes安装包  
           yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes   
        启动
           systemctl enable  kubelet && systemctl start kubelet    
      创建集群
          kubeadm init --apiserver-advertise-address=192.168.206.150 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.2 --pod-network-cidr=10.244.0.0/16 --token-ttl 0 --ignore-preflight-errors=Swap  

          如果创建失败进行重置
          连接不上 重置
           swapoff -a && kubeadm reset  && systemctl daemon-reload && systemctl restart kubelet  && iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X  

          创建成功执行下面命令
             mkdir -p $HOME/.kube
             sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
             sudo chown $(id -u):$(id -g) $HOME/.kube/config 
          创建容器间网络
             kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml   
          其他节点加入集群
             kubeadm join 192.168.206.150:6443 --token j5js6d.1yep3cf8dz6o17t4 \
             --discovery-token-ca-cert-hash sha256:856cb121f2d525da2d544299aea3ea549caf481cfbff84a94b22955d5627eef8  
      简单作业发布
        cat hello.yaml
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: hello
          spec:
            template:
              metadata:
                name: hello
              spec:
                containers:
                - name: hello
                  image: busybox
                  command: ["echo", "hello from feiyang"]
                  restartPolicy: Never   
        发布应用命令kubectl apply -f    hello.yaml        
      存储原理和实战
        Volum 卷
           emptyDir(同一个pod间容器的目录共享，缺点pod消亡，mount消失)
              kubectl exec mynginx mount|grep mymount  
           hostPath 和物理服务磁盘绑定

           storage provider 云服务提供商
           pv-pvc
              NFS小技巧
              yum -y install nfs-utils rpcbind
              sudo systemctl enable rpcbind
              sudo systemctl start rpcbind
              sudo systemctl enable nfs-server
              sudo systemctl start nfs-server
            pv动态供给
        ConfigMap&Secret
         创建方式
         --from-literal 
         --from-file
         --from-env
         yaml
         volumes-传递方式-env   
         volumes会实时动态更新 env只是在创建时候更新   
      认证和授权   
          user&Pod(k8s Service Account)--->(Authentication&Authorization&Admission)Api Server
           Authentication认证  说明你是谁 说明你是谁 特有特征&我知道那些信息&我拥有
           Authorization 授权
           Admisson Contorl
           认证：
             User 普通用户
             Service Account 服务账号
             客户端证书
             静态密码文件 Basic auth
             Token(Token file,Bearer Token,Server Secret Token ,Java web token)
          授权：
             RBAC(Role-Based Access Control)
             Role 简化管理
             ClusterRole  跨越Namespace 
      服务发现与负载均衡
        Pod访问方式
         ClusterIp Service(内部) 
            Traffic->Proxy-->（Service-->Pod(测试环境) k8s集群）
         NodePort Service(由内而外) 基于以上 指定端口到服务
            Traffic->Port vm->(Service-->pod (k8s集群))
         LoadBalancer Service(外部)
           Traffic->Load Balancer-->(service->pod) 需要公有云平台支持
         Ingress (外部) 变种的LoadBalancer
            Traffic->Ingress-->(Service-->Pod) 通过公有云支持
       服务发现
          Pod内服务调用：localhost:容器内应用端口
          Pod间服务调用：服务名.namespace名：服务端口
           kubectl run busybox --rm -it --image=busybox -/bin/sh
           /# wget nginx-svc:default:8080
           kubectl describe service nginx-svc
           外部服务调用：负载均衡器Ip:负载均衡器内映射端口或Ingress Url
            spec:
          type: NodePort
            ports:
             nodePort: 30000
            kubectl describe service nginx-svc
       访问策略
          支持的网络协议 Calico Canal
              PodSelector
                Ingress /egress
               podSelector:
             matchLabels:
                app: web_server
               ingress:
                - from:
                - podSelector:
                  matchLabels:
                   access: "true"
               - ipBlock:
                      cidr: 10.244.0.0/16 
               kubectl run busybox --rm -it --labels="access=true" --image=busybox -/bin/sh
              /# wget nginx-svc:default:8080           
      网络层次以及错误分析
          切换网络协议栈  
            kubeadm reset
            kubeadm init --apiserver-advertise-address=172.20.230.77 --pod-network-cidr=10.244.0.0/16
             mkdir -p $HOME/.kube
             sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
             sudo chown $(id -u):$(id -g) $HOME/.kube/config
             kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/canal.yaml
          LoadBalancer&Ingress-->ClusterIp(Dns)-->PodIp
          Pod状态异常
            Pod Status :Init:CrashLoopBackOff
                关闭Selinux
                setenforce=0&/etc/selinux/config:Selinux=disable
          Pod状态正常 但无法解析DNS
             iptables -p Forward accept
             kubectl get svc kube-dns --namespace=kube-system
             kubectl get ep kube-dns --namespace=kube-system  
             kubectl get service kube-dns --namespace=kube-system
          Dns正常 服务不可达
             kubectl get endpoints service-name
             Pod和service的label不一致
             Service的容器端口错误
             kubectl get ep nginx-svc
          kubernates api不可达
             kubectl get service kubernetes 
              kubectl get ep kubernetes 
             rbac           
      Helm Tiller
         Helm(类似于 yum&apt-get) 管理很多yaml文件包管理 
         Helm（船舵）
         Tiller（把手）  
         Repository(包仓库)
         Chart静态
         chart.yaml
         templates
           deployment
          _helpers.tpl
          ingress.yaml
          service.yaml
         values.yaml  
         安装部署
           wget https://storage.googleapis.com/kubernetes-helm/helm-v2.16.6-linux-amd64.tar.gz
           tar -xvf helm-v2.16.6-linux-amd64.tar.gz
           sudo cp linux-amd64/helm /usr/local/bin/
           helm version  
           kubectl -n kube-system create serviceaccount tiller
           kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
           docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.6 
            helm init --service-account tiller --tiller-image  registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.6 --skip-refresh
      监控维度
         应用性能管理apm  -Metrics 知其然 Promethus+Grafana
         业务追踪Tracing  知其调用
         日志管理-logging 知其所以然  Elk&EF(fluentd)k  
            logstash(兼容各种输出) fluentd（大容量 高输出）
         安装prometheus
           git clone https://github.com/coreos/prometheus-operator       
           cd prometheus-operator
           kubectl create namespace monitorin
           helm install --name prometheus-operator --set rbacEnable=true --namespace=monitoring stable/prometheus-operator
           kubectl edit service prometheus-operator-grafana --namespace=monitoring
           kubectl edit service prometheus-operator-prometheus --namespace=monitoring
           kubectl edit service prometheus-operator-alertmanager --namespace=monitoring
             ClusterIP -> NodePort
           kubectl edit secret --namespace=monitoring prometheus-operator-grafana
             admin-password: cHJvbS1vcGVyYXRvcg==
             admin-user: YWRtaW4=
         部署EFK（Fluentd, Elasticsearch, Kibana）监控
            下载Yaml文件到$download_directory：  
             https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch  
              修改kibana-deployment.yaml，删除$SERVER_BASEPATH  
              修改kibana-logging service，成为NodePort类型  
              kubectl apply -f $download_directory 
      Kubernetes Dashoboard 图形化界面 
          kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml        
          kubectl --namespace=kubernetes-dashboard edit service kubernetes-dashboard
             ClusterIP -> NodePort
          kubectl -n kubernetes-dashboard get service
          kubectl apply -f dashboard.yml
          kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}') 
    部署前前置
       有状态应用
            容器存储：HostPath PV/PVC storage Provider
            虚拟机、物理机
            云服务（）redis mysql rabbitmq zipkin elk
       Eureka 有状态？无状态
         单一部署 多节点互相注册+Statefulset+headless Servicess
         多重部署 ：互相注册+单一软件多重deployment+service
         单实例部署：单一deployment+service
       Config Gateway
         透明二传
          多Pod部署
       Ingress/Service接入
       无状态
         配置如何管理
           ConfigMap & Spring Cloud Config（默认）
         负载均衡管理 
           Ingress(7层)/Service(4层硬件)&Ribbon（选好面）        
           后台云服务(Mysql,redis)
           基础架构服务（计算节点） Kubernetes集群
           容器直接部署（Rabbitmq）
          代码改造（修改Eureka配置，数据库设置）
           制作并发布镜像（编译，镜像制作，镜像上传）  
        无服务器化 serverless
          应用无状态
          常见编程方式：函数式编程,响应式编程
          常见业务模式：事件驱动 流驱动
           从0资源到无限大     
    微服务部署 
      1. 阿里云上部署Mysql和Redis  
      2. 阿里云上激活kuberntes集群  
      3. 部署rabbitmq:management和ClusterIP类型的服务  
      4. 部署Eureka Server 
      阿里云上传镜像
        $ docker images
        $ docker login --username=ieeezhang registry.cn-shanghai.aliyuncs.com
        $ docker tag my registry registry.cn-shanghai.aliyuncs.com/feiyang/myregistry:1.0
        $ docker push registry.cn-shanghai.aliyuncs.com/feiyang/myregistry:1.0    
      图形化界面，创建myregistry部署和myregistry-svc服务（load balander模式）

      创建config应用
        修改配置文件 defaultZone: http://myregistry-svc:20000/eureka/
        打包镜像上传  
  电商的秒杀抢购需求
         防止击穿 防雪崩
         全链路压测
         反腐层
         限流排队降级熔断
          无服务器化
         弹性扩容    
  弹性扩缩容
     cloudfoundry Metric监控 规则决策 容器&路由伸缩
        资源触发
          App Autoscaler服务 
          cpu 内存 http带宽 延时 RabbitMq 队列深度
          定制Metric Prometheus MetricRegister
        简单决策
           定义伸缩上下限
           定义伸缩规则
          定制时间规则
     Messos+Marathon 负载监控，规则触发 弹性负载均衡
        负载触发扩缩容
        mesosphere/marathon-lb-autoscale
        关联Marathon和Haproxy
        指定目标RPS(每个实例每秒请求数)规则  
     Kubernetes 事件触发 配置声明 自动部署 路由伸缩
         容器扩缩容 HPA VPA 
         底层扩缩容 Autoscale
         Istio+Knative（沟子）
         服务网格 统一网关 便车（抗日战争时期伪军）
     有状态应用如何弹性伸缩
         将有状态应用进行区分：共享磁盘模式和Share nothing模式
         共享磁盘模式-转移-无状态应用
         Share nothing采用合适的集群管理方式和CAP目标
     向无状态应用转移
         结构化数据  -共享数据库
         非结构化数据  共享缓存 对象存储 搜索引擎
        减少文件系统依赖（CDN直接对接对象储存）
     Share Nothing架构
        CAP 优化可用性和分区性 弱化一致性
        集群管理 优化选举仲裁 阶段提交 副本 分片管理
        资源预配置        






