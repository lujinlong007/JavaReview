
# J.U.C
##  并发理论
### 可见性、原子性和有序性问题：并发编程Bug的源头
               
               并发程序幕后的故事
                这些年，我们的 CPU、内存、I/O 设备都在不断迭代，不断朝着更快的方向努力。但是，在这个快速发展的过程中，有一个核心矛盾一直存在，就是这三者的速度差异。
               CPU 和内存的速度差异可以形象地描述为：CPU 是天上一天，内存是地上一年（假设 CPU 执行一条普通指令需要一天，那么 CPU 读写内存得等待一年的时间）。
               内存和 I/O 设备的速度差异就更大了，内存是天上一天，I/O 设备是地上十年。
                程序里大部分语句都要访问内存，有些还要访问 I/O，根据木桶理论（一只水桶能装多少水取决于它最短的那块木板），程序整体的性能取决于最慢的操作——读写 I/O 设备，
		也就是说单方面提高    CPU 性能是无效的。为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系结构、操作系统、编译程序都做出了贡献，主要体现为：
                1CPU 增加了缓存，以均衡与内存的速度差异；
                2操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异；
                3编译程序优化指令执行次序，使得缓存能够得到更加合理地利用

                源头之一：缓存导致的可见性问题
                  在单核时代，所有的线程都是在一颗 CPU 上执行，CPU 缓存与内存的数据一致性容易解决。因为所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来说一定是可见的。
                  一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性。
                  多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存。
		  比如下图中，线程 A 操作的是 CPU-1 上的缓存，而线程 B 操作的是 CPU-2 上的缓存，很明显，这个时候线程 A 对变量 V 的操作对于线程 B 而言就不具备可见性了。
		  这个就属于硬件程序员给软件程序员挖的“坑”
                源头之二：线程切换带来的原子性问题
                   由于 IO 太慢，早期的操作系统就发明了多进程，即便在单核的 CPU 上我们也可以一边听着歌，一边写 Bug，这个就是多进程的功劳。
		   操作系统允许某个进程执行一小段时间，例如 50 毫秒，过了 50 毫秒操作系统就会重新选择一个进程来执行（我们称为“任务切换”），这个 50 毫秒称为“时间片”
                   在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，
		   唤醒后的进程就有机会重新获得 CPU 的使用权了。这里的进程在等待 IO 时之所以会释放 CPU 使用权，是为了让 CPU 在这段等待时间里可以做别的事情，这样一来 CPU 的使用率就上来了；
		   此外，如果这时有另外一个进程也读文件，读文件的操作就会排队，磁盘驱动在完成一个进程的读操作后，发现有排队的任务，就会立即启动下一个读操作，这样 IO 的使用率也上来了。

                   支持多进程分时复用在操作系统的发展史上却具有里程碑意义，Unix 就是因为解决了这个问题而名噪天下的。早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，
		   所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。
		   现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。
                  
		  Java 并发程序都是基于多线程的，自然也会涉及到任务切换，也许你想不到，任务切换竟然也是并发编程里诡异 Bug 的源头之一。任务切换的时机大多数是在时间片结束的时候，
		   我们现在基本都使用高级语言编程，高级语言里一条语句往往需要多条 CPU 指令完成，例如上面代码中的count += 1，至少需要三条 CPU 指令。指令 
		   1：首先，需要把变量 count 从内存加载到 CPU 的寄存器；指令 
		   2：之后，在寄存器中执行 +1 操作；指令 
		   3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存）
                   
		   操作系统做任务切换，可以发生在任何一条 CPU 指令执行完，是的，是 CPU 指令，而不是高级语言里的一条语句。对于上面的三条指令来说，
		   我们假设 count=0，如果线程 A 在指令 1 执行完后做线程切换，
		   线程 A 和线程 B 按照下图的序列执行，那么我们会发现两个线程都执行了 count+=1 的操作，但是得到的结果不是我们期望的 2，而是 1
                   我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性。CPU 能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符，这是违背我们直觉的地方。
		   因此，很多时候我们需要在高级语言层面保证操作的原子性。
		   
                源头之三：编译优化带来的有序性问题   
                  顾名思义，有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会改变程序中语句的先后顺序，例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”，
		  在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果。不过有时候编译器及解释器的优化可能导致意想不到的 Bug。
                双重检查问题
                    实际上这个 getInstance() 方法并不完美。问题出在哪里呢？出在 new 操作上，我们以为的 new 操作应该是：
                    分配一块内存 M；
                    在内存 M 上初始化 Singleton 对象；
                    然后 M 的地址赋值给 instance 变量。
                    但是实际上优化后的执行路径却是这样的：
                    分配一块内存 M；
                    将 M 的地址赋值给 instance 变量；
                    最后在内存 M 上初始化 Singleton 对象。
                  我们假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，
		  那么线程 B 在执行第一个判断时会发现 instance != null ，所以直接返回 instance，而此时的 instance 是没有初始化过的，
		  如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。
### Java内存模型：看Java如何解决可见性和有序性问题
              什么是 Java 内存模型？

                导致可见性的原因是缓存，导致有序性的原因是编译优化，那解决可见性、有序性最直接的办法就是禁用缓存和编译优化，但是这样问题虽然解决了，我们程序的性能可就堪忧了。
		合理的方案应该是按需禁用缓存以及编译优化。那么，如何做到“按需禁用”呢？对于并发程序，何时禁用缓存以及编译优化只有程序员知道，那所谓“按需禁用”其实就是指按照程序员的要求来禁用。
		所以，为了解决可见性和有序性问题，只需要提供给程序员按需禁用缓存和编译优化的方法即可。
                Java 内存模型是个很复杂的规范，可以从不同的视角来解读，站在我们这些程序员的视角，本质上可以理解为，Java 内存模型规范了 JVM 如何提供按需禁用缓存和编译优化的方法。
		具体来说，这些方法包括 volatile、synchronized 和 final 三个关键字，以及六项 Happens-Before 规则
              使用 volatile 的困惑
                 volatile 关键字并不是 Java 语言的特产，古老的 C 语言里也有，它最原始的意义就是禁用 CPU 缓存。例如，我们声明一个 volatile 变量 volatile int x = 0，
		 它表达的是：告诉编译器，对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入。这个语义看上去相当明确，但是在实际使用的时候却会带来困惑。

                 Java 内存模型在 1.5 版本对 volatile 语义进行了增强。怎么增强的呢？答案是一项 Happens-Before 规则。
              Happens-Before 规则 前面一个操作的结果对后续操作是可见的。

                Happens-Before 约束了编译器的优化行为，虽允许编译器优化，但是要求编译器优化后一定遵守 Happens-Before 规则。
                1. 程序的顺序性规则 
                  这条规则是指在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作。
                  程序前面对某个变量的修改一定是对后续操作可见的
                2. volatile 变量规则
                  这条规则是指对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作。
                3 传递性
                   这条规则是指如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C。
                4  管程中锁的规则
                   这条规则是指对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。要理解这个规则，就首先要了解“管程指的是什么”。管程是一种通用的同步原语，
		   在 Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现。管程中的锁在 Java 里是隐式实现的，
		   例如下面的代码，在进入同步块之前，会自动加锁，而在代码块执行完会自动释放锁，加锁以及释放锁都是编译器帮我们实现的。
                5  线程 start() 规则 
                   这条是关于线程启动的。它是指主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作。
                   如果线程 A 调用线程 B 的 start() 方法（即在线程 A 中启动线程 B），那么该 start() 操作 Happens-Before 于线程 B 中的任意操作。

                   Thread B = new Thread(()->{
                      // 主线程调用B.start()之前
                     // 所有对共享变量的修改，此处皆可见
                      // 此例中，var==77
                   });
                   // 此处对共享变量var修改
                  var = 77;
                   // 主线程启动子线程
                  B.start();
                6. 线程 join() 规则
                     这条是关于线程等待的。它是指主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。当然所谓的“看到”，指的是对共享变量的操作

                     换句话说就是，如果在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回。具体可参考下面示例代码。

                     Thread B = new Thread(()->{  // 此处对共享变量var修改  var = 66;});// 例如此处对共享变量修改，// 则这个修改结果对线程B可见// 主线程启动子线程B.start();B.join()// 子线程所有对共享变量的修改// 在主线程调用B.join()之后皆可见// 此例中，var==66 
              被我们忽视的 final    

                final 修饰变量时，初衷是告诉编译器：这个变量生而不变，可以可劲儿优化。     
### 互斥锁：解决原子性问题     
               那原子性问题到底该如何解决呢？
                 原子性问题的源头是线程切换，如果能够禁用线程切换那不就能解决这个问题了吗？而操作系统做线程切换是依赖 CPU 中断的，所以禁止 CPU 发生中断就能够禁止线程切换。
                 这里我们以 32 位 CPU 上执行 long 型变量的写操作为例来说明这个问题，long 型变量是 64 位，在 32 位 CPU 上执行写操作会被拆分成两次写操作（写高 32 位和写低 32 位，如下图所示）。

                 在单核 CPU 场景下，同一时刻只有一个线程执行，禁止 CPU 中断，意味着操作系统不会重新调度线程，也就是禁止了线程切换，获得 CPU 使用权的线程就可以不间断地执行，所以两次写操作一定是：要么都被执行，要么都没有被执行，具有原子性。

                 但是在多核场景下，同一时刻，有可能有两个线程同时在执行，一个线程执行在 CPU-1 上，一个线程执行在 CPU-2 上，此时禁止 CPU 中断，只能保证 CPU 上的线程连续执行，并不能保证同一时刻只有一个线程执行，如果这两个线程同时写 long 型变量高 32 位的话，那就有可能出现我们开头提及的诡异 Bug 了。
                 
                 同一时刻只有一个线程执行”这个条件非常重要，我们称之为互斥。如果我们能够保证对共享变量的修改是互斥的，那么，无论是单核 CPU 还是多核 CPU，就都能保证原子性了。

               简易锁模型 

                  我们把一段需要互斥执行的代码称为临界区。线程在进入临界区之前，首先尝试加锁 lock()，如果成功，则进入临界区，此时我们称这个线程持有锁；否则呢就等待，直到持有锁的线程解锁；持有锁的线程执行完临界区的代码后，执行解锁 unlock()

               改进后的锁模型

                 锁和锁要保护的资源是有对应关系的，比如你用你家的锁保护你家的东西，我用我家的锁保护我家的东西。在并发编程世界里，锁和资源也应该有这个关系

                 首先，我们要把临界区要保护的资源标注出来，如图中临界区里增加了一个元素：受保护的资源 R；其次，我们要保护资源 R 就得为它创建一把锁 LR；最后，针对这把锁 LR，我们还需在进出临界区时添上加锁操作和解锁操作。另外，在锁 LR 和受保护资源之间，我特地用一条线做了关联，这个关联关系非常重要

               Java 语言提供的锁技术：synchronized

                  锁是一种通用的技术方案，Java 语言提供的 synchronized     关键字，就是锁的一种实现。synchronized 关键字可以用来修饰方法，也可以用来修饰代码块
                  
                  当修饰静态方法的时候，锁定的是当前类的 Class 对象，在上面的例子中就是 Class X；当修饰非静态方法的时候，锁定的是当前实例对象 this
                   // 修饰静态方法  synchronized(X.class) static void bar() {    // 临界区  }
                   // 修饰非静态方法  synchronized(this) void foo() {    // 临界区  }
                  
                  管程，就是我们这里的 synchronized（至于为什么叫管程，我们后面介绍），我们知道 synchronized 修饰的临界区是互斥的，也就是说同一时刻只有一个线程执行临界区的代码；而所谓“对一个锁解锁  Happens-Before 后续对这个锁的加锁”，指的是前一个线程的解锁操作对后一个线程的加锁操作可见，综合 Happens-Before 的传递性原则，我们就能得出前一个线程在临界区修改的共享变量（该操作在解锁之前），对后续进入临界区（该操作在加锁之后）的线程是可见的。

                  按照这个规则，如果多个线程同时执行 addOne() 方法，可见性是可以保证的，也就说如果有 1000 个线程执行 addOne() 方法，最终结果一定是 value 的值增加了 1000


                  get() 方法和 addOne() 方法都需要访问 value 这个受保护的资源，这个资源用 this 这把锁来保护。线程要进入临界区 get() 和 addOne()，必须先获得 this 这把锁，这样 get() 和 addOne() 也是互斥的。

                  这个模型更像现实世界里面球赛门票的管理，一个座位只允许一个人使用，这个座位就是“受保护资源”，球场的入口就是 Java 类里的方法，而门票就是用来保护资源的“锁”，Java 里的检票工作是由 synchronized 解决的。  

               锁和受保护资源的关系

                   受保护资源和锁之间的关联关系是 N:1 的关系  
                   还拿前面球赛门票的管理来类比，就是一个座位，我们只能用一张票来保护，如果多发了重复的票，那就要打架了。现实世界里，我们可以用多把锁来保护同一个资源，但在并发领域是不行的，并发领域的锁和现实世界的锁不是完全匹配的。不过倒是可以用同一把锁来保护多个资源，这个对应到现实世界就是我们所谓的“包场”了。

                    synchronized long get() {    return value;  }  

                        synchronized static void addOne() {    value += 1;  }

                    如果你仔细观察，就会发现改动后的代码是用两个锁保护一个资源。这个受保护的资源就是静态变量 value，两个锁分别是 this 和 SafeCalc.class。我们可以用下面这幅图来形象描述这个关系。由于临界区 get() 和 addOne() 是用两个锁保护的，因此这两个临界区没有互斥关系，临界区 addOne() 对 value 的修改对临界区 get() 也没有可见性保证，这就导致并发问题了。 

               互斥锁，在并发领域的知名度极高，只要有了并发问题，大家首先容易想到的就是加锁，因为大家都知道，加锁能够保证执行临界区代码的互斥性。这样理解虽然正确，但是却不能够指导你真正用好互斥锁。临界区的代码是操作受保护资源的路径，类似于球场的入口，入口一定要检票，也就是要加锁，但不是随便一把锁都能有效。所以必须深入分析锁定的对象和受保护资源的关系，综合考虑受保护资源的访问路径，多方面考量才能用好互斥锁。synchronized 是 Java 在语言层面提供的互斥原语，其实 Java 里面还有很多其他类型的锁，但作为互斥锁，原理都是相通的：锁，一定有一个要锁定的对象，至于这个锁定的对象要保护的资源以及在哪里加锁 / 解锁，就属于设计层面的事情了     

### 如何用一把锁保护多个资源 
               可以用一把锁来保护多个资源，但是不能用多把锁来保护一个资源
               当我们要保护多个资源时，首先要区分这些资源是否存在关联关系
               保护没有关联关系的多个资源
                 用一把锁有个问题，就是性能太差，会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的。而我们用两把锁，取款和修改密码是可以并行的。用不同的锁对受保护资源进行精细化管理，能够提升性能。这种锁还有个名字，叫细粒度锁
               保护有关联关系的多个资源
                 // 转账 synchronized void transfer( Account target, int amt){ if (this.balance > amt) { this.balance -= amt; target.balance += amt; } }

                 在这段代码中，临界区内有两个资源，分别是转出账户的余额 this.balance 和转入账户的余额 target.balance，并且用的是一把锁 this，符合我们前面提到的，多个资源可以用一把锁来保护，这看上去完全正确呀。真的是这样吗？可惜，这个方案仅仅是看似正确，为什么呢？


                 问题就出在 this 这把锁上，this 这把锁可以保护自己的余额 this.balance，却保护不了别人的余额 target.balance，就像你不能用自家的锁来保护别人家的资产，也不能用自己的票来保护别人的座位一样。  

               使用锁的正确姿势  
                  这个办法确实能解决问题，但是有点小瑕疵，它要求在创建 Account 对象的时候必须传入同一个对象，如果创建 Account 对象时，传入的 lock 不是同一个对象，那可就惨了，会出现锁自家门来保护他家资产的荒唐事。在真实的项目场景中，创建 Account 对象的代码很可能分散在多个工程中，传入共享的 lock 真的很难。


                  我们需要更好的方案。还真有，就是用 Account.class 作为共享的锁。Account.class 是所有 Account 对象共享的，而且这个对象是 Java 虚拟机在加载 Account 类的时候创建的，所以我们不用担心它的唯一性。使用 Account.class 作为共享的锁，我们就无需在创建 Account 对象时传入了，代码更简单。


                  “原子性”的本质是什么？其实不是不可分割，不可分割只是外在表现，其本质是多个资源间有一致性的要求，操作的中间状态对外不可见。例如，在 32 位的机器上写 long 型变量有中间状态（只写了 64 位中的 32 位），在银行转账的操作中也有中间状态（账户 A 减少了 100，账户 B 还没来得及发生变化）。所以解决原子性问题，是要保证中间状态对外不可见。  
### 一不小心就死锁了，怎么办？   

              // 锁定转出账户    synchronized(this) {                    // 锁定转入账户      synchronized(target) {                   if (this.balance > amt) {          this.balance -= amt;          target.balance += amt;        }      }
              没有免费的午餐
                 细粒度锁。使用细粒度锁可以提高并行度，是性能优化的一个重要手段
                 的确，使用细粒度锁是有代价的，这个代价就是可能会导致死锁
                 死锁的一个比较专业的定义是：一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象。

                 关于这种现象，我们还可以借助资源分配图来可视化锁的占用情况（资源分配图是个有向图，它可以描述资源和线程的状态）。其中，资源用方形节点表示，线程用圆形节点表示；资源中的点指向线程的边表示线程已经获得该资源，线程指向资源的边则表示线程请求资源，但尚未得到。转账发生死锁时的资源分配图就如下图所示，一个“各据山头死等”的尴尬局面。
              如何预防死锁
                  死锁发生的四个条件
                    1互斥，共享资源 X 和 Y 只能被一个线程占用；
                    2占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X；
                    3不可抢占，其他线程不能强行抢占线程 T1 占有的资源；
                    4循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待

                  如何破坏死锁
                    1对于“占用且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了。
                    2对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。
                    3对于“循环等待”这个条件，可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后自然就不存在循环了。


                    1. 破坏占用且等待条件从理论上讲，要破坏这个条件，可以一次性申请所有资源。在现实世界里，就拿前面我们提到的转账操作来讲，它需要的资源有两个，一个是转出账户，另一个是转入账户，当这两个账户同时被申请时，我们该怎么解决这个问题呢？可以增加一个账本管理员，然后只允许账本管理员从文件架上拿账本，也就是说柜员不能直接在文件架上拿账本，必须通过账本管理员才能拿到想要的账本。例如，张三同时申请账本 A 和 B，账本管理员如果发现文件架上只有账本 A，这个时候账本管理员是不会把账本 A 拿下来给张三的，只有账本 A 和 B 都在的时候才会给张三。这样就保证了“一次性申请所有资源”。

                      // 一次性申请转出账户和转入账户，直到成功    while(!actr.apply(this, target))      ；
                    2. 破坏不可抢占条件破坏不可抢占条件看上去很简单，核心是要能够主动释放它占有的资源，这一点 synchronized 是做不到的。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。  

                    3. 破坏循环等待条件破坏这个条件，需要对资源进行排序，然后按序申请资源。这个实现非常简单，我们假设每个账户都有不同的属性 id，这个 id 可以作为排序字段，申请的时候，我们可以按照从小到大的顺序来申请。比如下面代码中，①~⑥处的代码对转出账户（this）和转入账户（target）排序，然后按照序号从小到大的顺序锁定账户。这样就不存在“循环”等待了。
                     if (this.id > target.id) { ③      left = target;           ④      right = this;            ⑤    }                          ⑥    // 锁定序号小的账户    synchronized(left){      // 锁定序号大的账户 
### 用“等待-通知”机制优化循环等待
               在破坏占用且等待条件的时候，如果转出账本和转入账本不满足同时在文件架上这个条件，就用死循环的方式来循环等待，核心代码如下：
                // 一次性申请转出账户和转入账户，直到成功while(!actr.apply(this, target))  ；
               如果 apply() 操作耗时非常短，而且并发冲突量也不大时，这个方案还挺不错的，因为这种场景下，循环上几次或者几十次就能一次性获取转出账户和转入账户了。但是如果 apply() 操作耗时长，或者并发冲突量大的时候，循环等待这种方案就不适用了，因为在这种场景下，可能要循环上万次才能获取到锁，太消耗 CPU 了。

               其实在这种场景下，最好的方案应该是：如果线程要求的条件（转出账本和转入账本同在文件架上）不满足，则线程阻塞自己，进入等待状态；当线程要求的条件（转出账本和转入账本同在文件架上）满足后，通知等待的线程重新执行。其中，使用线程阻塞的方式就能避免循环等待消耗 CPU 的问题。

               完美的就医流程
                 1患者先去挂号，然后到就诊门口分诊，等待叫号；
                 2当叫到自己的号时，患者就可以找大夫就诊了；
                 3就诊过程中，大夫可能会让患者去做检查，同时叫下一位患者；
                 4当患者做完检查后，拿检测报告重新分诊，等待叫号；
                 5当大夫再次叫到自己的号时，患者再去找大夫就诊。
                 一个完整的等待 - 通知机制：线程首先获取互斥锁，当线程要求的条件不满足时，释放互斥锁，进入等待状态；当要求的条件满足时，通知等待的线程，重新获取互斥锁。
               用 synchronized 实现等待 - 通知机制
                  在 Java 语言里，等待 - 通知机制可以有多种实现方式，比如 Java 语言内置的 synchronized 配合 wait()、notify()、notifyAll() 这三个方法就能轻松实现

                  如何用 synchronized 实现互斥锁，你应该已经很熟悉了。在下面这个图里，左边有一个等待队列，同一时刻，只允许一个线程进入 synchronized 保护的临界区（这个临界区可以看作大夫的诊室），当有一个线程进入临界区后，其他线程就只能进入图中左边的等待队列里等待（相当于患者分诊等待）。这个等待队列和互斥锁是一对一的关系，每个互斥锁都有自己独立的等待队列。

                  在并发程序中，当一个线程进入临界区后，由于某些条件不满足，需要进入等待状态，Java 对象的 wait() 方法就能够满足这种需求。如上图所示，当调用 wait() 方法后，当前线程就会被阻塞，并且进入到右边的等待队列中，这个等待队列也是互斥锁的等待队列。 线程在进入等待队列的同时，会释放持有的互斥锁，线程释放锁后，其他线程就有机会获得锁，并进入临界区了。

                  那线程要求的条件满足时，该怎么通知这个等待的线程呢？很简单，就是 Java 对象的 notify() 和 notifyAll() 方法。我在下面这个图里为你大致描述了这个过程，当条件满足时调用 notify()，会通知等待队列（互斥锁的等待队列）中的线程，告诉它条件曾经满足过。

                  为什么说是曾经满足过呢？因为 notify() 只能保证在通知时间点，条件是满足的。而被通知线程的执行时间点和通知的时间点基本上不会重合，所以当线程执行的时候，很可能条件已经不满足了（保不齐有其他线程插队）。这一点你需要格外注意。


                   上面我们一直强调 wait()、notify()、notifyAll() 方法操作的等待队列是互斥锁的等待队列，所以如果 synchronized 锁定的是 this，那么对应的一定是 this.wait()、this.notify()、this.notifyAll()；如果 synchronized 锁定的是 target，那么对应的一定是 target.wait()、target.notify()、target.notifyAll() 。而且 wait()、notify()、notifyAll() 这三个方法能够被调用的前提是已经获取了相应的互斥锁，所以我们会发现 wait()、notify()、notifyAll() 都是在 synchronized{}内部被调用的。如果在 synchronized{}外部调用，或者锁定的 this，而用 target.wait() 调用的话，JVM 会抛出一个运行时异常：java.lang.IllegalMonitorStateException。 
               小试牛刀：一个更好地资源分配器
                  1互斥锁：上一篇文章我们提到 Allocator 需要是单例的，所以我们可以用 this 作为互斥锁。
                  2线程要求的条件：转出账户和转入账户都没有被分配过。
                  3何时等待：线程要求的条件不满足就等待。
                  4何时通知：当有线程释放账户时就通知。   

                    // 经典写法    while(als.contains(from) ||         als.contains(to)){      try{        wait();


                     // 归还资源  synchronized void free(    Object from, Object to){    als.remove(from);    als.remove(to);    notifyAll();  

                   尽量使用 notifyAll()
                      notify() 是会随机地通知等待队列中的一个线程，而 notifyAll() 会通知等待队列中的所有线程。从感觉上来讲，应该是 notify() 更好一些，因为即便通知所有线程，也只有一个线程能够进入临界区。但那所谓的感觉往往都蕴藏着风险，实际上使用 notify() 也很有风险，它的风险在于可能导致某些线程永远不会被通知到  
### 安全性、活跃性以及性能问题          
              
              安全性问题
    
               那什么是线程安全呢？其实本质上就是正确性，而正确性的含义就是程序按照我们期望的执行，不要让我们感到意外。
               理论上线程安全的程序，就要避免出现原子性问题、可见性问题和有序性问题。
               存在共享数据并且该数据会发生变化，通俗地讲就是有多个线程会同时读写同一数据。
               那如果能够做到不共享数据或者数据状态不发生变化，不就能够保证线程的安全性了嘛。有不少技术方案都是基于这个理论的，例如线程本地存储（Thread Local Storage，TLS）、不变模式等等，

               竞态条件（Race Condition）。所谓竞态条件，指的是程序的执行结果依赖线程执行的顺序。例如上面的例子，如果两个线程完全同时执行，那么结果是 1；如果两个线程是前后执行，那么结果就是 2。在并发环境里，线程的执行顺序是不确定的，如果程序存在竞态条件问题，那就意味着程序执行的结果是不确定的，而执行结果不确定这可是个大 Bug。


               竞态条件。在并发场景中，程序的执行依赖于某个状态变量，也就是类似于下面这样：
               if (状态变量 满足 执行条件) {  执行操作}
               那面对数据竞争和竞态条件问题，又该如何保证线程的安全性呢？其实这两类问题，都可以用互斥这个技术方案，而实现互斥的方案有很多，CPU 提供了相关的互斥指令，操作系统、编程语言也会提供相关的 API。从逻辑上来看，我们可以统一归为：锁  

              活跃性问题 

               所谓活跃性问题，指的是某个操作无法执行下去。我们常见的“死锁”就是一种典型的活跃性问题，当然除了死锁外，还有两种情况，分别是“活锁”和“饥饿”  

               有时线程虽然没有发生阻塞，但仍然会存在执行不下去的情况，这就是所谓的“活锁”
               解决“活锁”的方案很简单，谦让时，尝试等待一个随机的时间就可以了。Raft 这样知名的分布式一致性算法中

               那“饥饿”该怎么去理解呢？所谓“饥饿”指的是线程因无法访问所需资源而无法执行下去的情况。“不患寡，而患不均”，如果线程优先级“不均”，在 CPU 繁忙的情况下，优先级低的线程得到执行的机会很小，就可能发生线程“饥饿”；持有锁的线程，如果执行的时间过长，也可能导致“饥饿”问题。

               解决“饥饿”问题的方案很简单，有三种方案：一是保证资源充足，二是公平地分配资源，三就是避免持有锁  的线程长时间执行。这三个方案中，方案一和方案三的适用场景比较有限，因为很多场景下，资源的稀缺性是没办法解决的，持有锁的线程执行的时间也很难缩短。倒是方案二的适用场景相对来说更多一些

               在并发编程里，主要是使用公平锁。所谓公平锁，是一种先来后到的方案，线程的等待是有顺序的，排在等待 队列前面的线程会优先获得资源。   

              性能问题  
                第一，既然使用锁会带来性能问题，那最好的方案自然就是使用无锁的算法和数据结构了。在这方面有很多相关的技术，例如线程本地存储 (Thread Local Storage, TLS)、写入时复制 (Copy-on-write)、乐观锁等；Java 并发包里面的原子类也是一种无锁的数据结构；Disruptor 则是一个无锁的内存队列，性能都非常好……第二，减少锁持有的时间。互斥锁本质上是将并行的程序串行化，所以要增加并行度，一定要减少持有锁的时间。这个方案具体的实现技术也有很多，例如使用细粒度的锁，一个典型的例子就是 Java 并发包里的 ConcurrentHashMap，它使用了所谓分段锁的技术（这个技术后面我们会详细介绍）；还可以使用读写锁，也就是读是无锁的，只有写的时候才会互斥。

                性能方面的度量指标有很多，我觉得有三个指标非常重要，就是：
                1吞吐量、延迟和并发量。吞吐量：指的是单位时间内能处理的请求数量。吞吐量越高，说明性能越好。
                2延迟：指的是从发出请求到收到响应的时间。延迟越小，说明性能越好。
                3并发量：指的是能同时处理的请求数量，一般来说随着并发量的增加、延迟也会增加。所以延迟这个指标，一般都会是基于并发量来说的。例如并发量是 1000 的时候，延迟是 50 毫秒。

              并发编程是一个复杂的技术领域，微观上涉及到原子性问题、可见性问题和有序性问题，宏观则表现为安全性、活跃性以及性能问题。我们在设计并发程序的时候，主要是从宏观出发，也就是要重点关注它的安全性、活跃性以及性能。安全性方面要注意数据竞争和竞态条件，活跃性方面需要注意死锁、活锁、饥饿等问题，性能方面我们虽然介绍了两个方案，但是遇到具体问题，你还是要具体分析，根据特定的场景选择合适的数据结构和算法。    
### 管程：并发编程的万能钥匙
               Java 采用的是管程技术，synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法都是管程的组成部分。而管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程。但是管程更容易使用，所以 Java 选择了管程。
              
               管程，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发。翻译为 Java 领域的语言，就是管理类的成员变量和成员方法，让这个类是线程安全的。
                
               MESA 模型
                 在管程的发展史上，先后出现过三种不同的管程模型，分别是：Hasen 模型、Hoare 模型和 MESA 模型。其中，现在广泛应用的是 MESA 模型，并且 Java 管程的实现参考的也是 MESA 模型。所以今天我们重点介绍一下 MESA 模型。
                
                 在并发编程领域，有两大核心问题：一个是互斥，即同一时刻只允许一个线程访问共享资源；另一个是同步，即线程之间如何通信、协作。这两大问题，管程都是能够解决的。

                 管程如何让解决互斥 

                   管程解决互斥问题的思路很简单，就是将共享变量及其对共享变量的操作统一封装起来。在下图中，管程 X 将共享变量 queue 这个队列和相关的操作入队 enq()、出队 deq() 都封装起来了；线程 A 和线程 B 如果想访问共享变量 queue，只能通过调用管程提供的 enq()、deq() 方法来实现；enq()、deq() 保证互斥性，只允许一个线程进入管程。不知你有没有发现，管程模型和面向对象高度契合的。估计这也是 Java 选择管程的原因吧。而我在前面章节介绍的互斥锁用法，其背后的模型其实就是它。

                 管程如何解决同步

                    在管程模型里，共享变量和对共享变量的操作是被封装起来的，图中最外层的框就代表封装的意思。框的 上面只有一个入口，并且在入口旁边还有一个入口等待队列。当多个线程同时试图进入管程内部时，只允许一个线程进入，其他线程则在入口等待队列中等待。这个过程类似就医流程的分诊，只允许一个患者就诊，其他患者都在门口等待。

                    管程里还引入了条件变量的概念，而且每个条件变量都对应有一个等待队列，如下图，条件变量 A 和条件变量 B 分别都有自己的等待队列。


                    假设有个线程 T1  执行出队操作，不过需要注意的是执行出队操作，有个前提条件，就是队列不能是空的  ，而队列不空这个前提条件就是管程里的条件变量。 如果线程 T1 进入管程后恰好发现队列是空的，那怎么办呢？等待啊，去哪里等呢？就去条件变量对应的等待队列里面等。此时线程 T1 就去“队列不空”这个条件变量的等待队列中等待。这个过程类似于大夫发现你要去验个血，于是给你开了个验血的单子，你呢就去验血的队伍里排队。线程 T1 进入条件变量的等待队列后，是允许其他线程进入管程的。这和你去验血的时候，医生可以给其他患者诊治，道理都是一样的。再假设之后另外一个线程 T2 执行入队操作，入队操作执行成功之后，“队列不空”这个条件对于线程 T1 来说已经满足了，此时线程 T2 要通知 T1，告诉它需要的条件已经满足了。当线程 T1 得到通知后，会从等待队列里面出来，但是出来之后不是马上执行，而是重新进入到入口等待队列里面。这个过程类似你验血完，回来找大夫，需要重新分诊。

                 我们用了 Java 并发包里面的 Lock 和 Condition，如果你看着吃力，也没关系，后面我们还会详细介绍，这个例子只是先让你明白条件变量及其等待队列是怎么回事。需要注意的是：await() 和前面我们提到的 wait() 语义是一样的；signal() 和前面我们提到的 notify() 语义是一样的

                 wait() 的正确姿势

                   Hasen 模型、Hoare 模型和 MESA 模型的一个核心区别就是当条件满足后，如何通知相关线程。管程要求同一时刻只允许一个线程执行，那当线程 T2 的操作使线程 T1 等待的条件满足时，T1 和 T2 究竟谁可以执行呢？
                   1Hasen 模型里面，要求 notify() 放在代码的最后，这样 T2 通知完 T1 后，T2 就结束了，然后 T1 再执行，这样就能保证同一时刻只有一个线程执行。
                   2Hoare 模型里面，T2 通知完 T1 后，T2 阻塞，T1 马上执行；等 T1 执行完，再唤醒 T2，也能保证同一时刻只有一个线程执行。但是相比 Hasen 模型，T2 多了一次阻塞唤醒操作。
                   3MESA 管程里面，T2 通知完 T1 后，T2 还是会接着执行，T1 并不立即执行，仅仅是从条件变量的等待队列进到入口等待队列里面。这样做的好处是 notify() 不用放到代码的最后，T2 也没有多余的阻塞唤醒操作。但是也有个副作用，就是当 T1 再次执行的时候，可能曾经满足的条件，现在已经不满足了，所以需要以循环方式检验条件变量。  

                 notify() 何时可以使用 

                   还有一个需要注意的地方，就是 notify() 和 notifyAll() 的使用，前面章节，我曾经介绍过，除非经过深思熟虑，否则尽量使用 notifyAll()。那什么时候可以使用 notify() 呢？需要满足以下三个条件：
                   1所有等待线程拥有相同的等待条件；
                   2所有等待线程被唤醒后，执行相同的操作；
                   3只需要唤醒一个线程  
### java 线程
              Java线程的生命周期
  

                Java 语言里的线程本质上就是操作系统的线程，它们是一一对应的。 
                通用的线程生命周期 

               初始状态、可运行状态、运行状态、休眠状态和终止状态。

               1初始状态，指的是线程已经被创建，但是还不允许分配 CPU 执行。这个状态属于编程语言特有的，不过这里所谓的被创建，仅仅是在编程语言层面被创建，而在操作系统层面，真正的线程还没有创建。
               2可运行状态，指的是线程可以分配 CPU 执行。在这种状态下，真正的操作系统线程已经被成功创建了，所以可以分配 CPU 执行。
               3当有空闲的 CPU 时，操作系统会将其分配给一个处于可运行状态的线程，被分配到 CPU 的线程的状态就转换成了运行状态。
               4运行状态的线程如果调用一个阻塞的 API（例如以阻塞方式读文件）或者等待某个事件（例如条件变量），那么线程的状态就会转换到休眠状态，同时释放 CPU 使用权，休眠状态的线程永远没有机会获得 CPU 使用权。当等待的事件出现了，线程就会从休眠状态转换到可运行状态。
               5线程执行完或者出现异常就会进入终止状态，终止状态的线程不会切换到其他任何状态，进入终止状态也就意味着线程的生命周期结束了。   
               这五种状态在不同编程语言里会有简化合并。例如，C 语言的 POSIX Threads 规范，就把初始状态和可运行状态合并了；Java 语言里则把可运行状态和运行状态合并了，这两个状态在操作系统调度层面有用，而 JVM 层面不关心这两个状态，因为 JVM 把线程调度交给操作系统处理了 
              
                Java 中线程的生命周期

                Java 语言中线程共有六种状态，分别是：NEW（初始化状态）RUNNABLE（可运行 / 运行状态）BLOCKED（ 阻塞状态）WAITING（无时限等待）TIMED_WAITING（有时限等待）TERMINATED（终止状态）

                但其实在操作系统层面，Java 线程中的 BLOCKED、WAITING、TIMED_WAITING 是一种状态，即前面我们提到的休眠状态。也就是说只要 Java 线程处于这三种状态之一，那么这个线程就永远没有 CPU 的使用权


                BLOCKED、WAITING、TIMED_WAITING 可以理解为线程导致休眠状态的三种原因。那具体是哪些情形会导致线程从 RUNNABLE 状态转换到这三种状态呢？而这三种状态又是何时转换回 RUNNABLE 的呢？以及 NEW、TERMINATED 和 RUNNABLE 状态是如何转换的？

                1. RUNNABLE 与 BLOCKED 的状态转换

                   只有一种场景会触发这种转换，就是线程等待 synchronized 的隐式锁。synchronized 修饰的方法、代码块同一时刻只允许一个线程执行，其他线程只能等待，这种情况下，等待的线程就会从 RUNNABLE 转换到 BLOCKED 状态。而当等待的线程获得 synchronized 隐式锁时，就又会从 BLOCKED 转换到 RUNNABLE 状态。如果你熟悉操作系统线程的生命周期的话，可能会有个疑问：线程调用阻塞式 API 时，是否会转换到 BLOCKED 状态呢？在操作系统层面，线程是会转换到休眠状态的，但是在 JVM 层面，Java 线程的状态不会发生变化，也就是说 Java 线程的状态会依然保持 RUNNABLE 状态。JVM 层面并不关心操作系统调度相关的状态，因为在 JVM 看来，等待 CPU 使用权（操作系统层面此时处于可执行状态）与等待 I/O（操作系统层面此时处于休眠状态）没有区别，都是在等待某个资源，所以都归入了 RUNNABLE 状态。而我们平时所谓的 Java 在调用阻塞式 API 时，线程会阻塞，指的是操作系统线程的状态，并不是 Java 线程的状态。

                2. RUNNABLE 与 WAITING 的状态转换
                  第一种场景，获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法。其中，wait() 方法我们在上一篇讲解管程的时候已经深入介绍过了，这里就不再赘述。
                  第二种场景，调用无参数的 Thread.join() 方法。其中的 join() 是一种线程同步方法，例如有一个线程对象 thread A，当调用 A.join() 的时候，执行这条语句的线程会等待 thread A 执行完，而等待中的这个线程，其状态会从 RUNNABLE 转换到 WAITING。当线程 thread A 执行完，原来等待它的线程又会从 WAITING 状态转换到 RUNNABLE。
                  第三种场景，调用 LockSupport.park() 方法。其中的 LockSupport 对象，也许你有点陌生，其实 Java 并发包中的锁，都是基于它实现的。调用 LockSupport.park() 方法，当前线程会阻塞，线程的状态会从 RUNNABLE 转换到 WAITING。调用 LockSupport.unpark(Thread thread) 可唤醒目标线程，目标线程的状态又会从 WAITING 状态转换到 RUNNABLE

                3. RUNNABLE 与 TIMED_WAITING 的状态转换

                  有五种场景会触发这种转换：
                  调用带超时参数的 Thread.sleep(long millis) 方法；
                   获得 synchronized 隐式锁的线程，调用带超时参数的 Object.wait(long timeout) 方法；
                    调用带超时参数的 Thread.join(long millis) 方法；
                   调用带超时参数的 LockSupport.parkNanos(Object blocker, long deadline) 方法；
                   调用带超时参数的 LockSupport.parkUntil(long deadline) 方法。

                4. 从 NEW 到 RUNNABLE 状态
                  Java 刚创建出来的 Thread 对象就是 NEW 状态，而创建 Thread 对象主要有两种方法。一种是继承 Thread 对象，重写 run() 方法

                  另一种是实现 Runnable 接口，重写 run() 方法，并将该实现类作为创建 Thread 对象的参数。

                   NEW 状态的线程，不会被操作系统调度，因此不会执行。Java 线程要执行，就必须转换到 RUNNABLE 状态。从 NEW 状态转换到 RUNNABLE 状态很简单，只要调用线程对象的 start() 方法就可以了，

               
                5. 从 RUNNABLE 到 TERMINATED

                   状态线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的时候异常抛出，也会导致线程终止。有时候我们需要强制中断 run() 方法的执行，例如 run() 方法访问一个很慢的网络，我们等不下去了，想终止怎么办呢？Java 的 Thread 类里面倒是有个 stop() 方法，不过已经标记为 @Deprecated，所以不建议使用了。正确的姿势其实是调用 interrupt() 方法。

                那 stop() 和 interrupt() 方法的主要区别是什么呢？

                   stop() 方法会真的杀死线程，不给线程喘息的机会，如果线程持有 ReentrantLock 锁，被 stop()  的线程并不会自动调用 ReentrantLock 的 unlock() 去释放锁，那其他线程就再也没机会获得 ReentrantLock 锁，这实在是太危险了。所以该方法就不建议使用了，类似的方法还有 suspend() 和 resume() 方法，这两个方法同样也都不建议使用了，所以这里也就不多介绍了。而 interrupt() 方法就温柔多了，interrupt() 方法仅仅是通知线程，线程有机会执行一些后续操作，同时也可以无视这个通知。被 interrupt 的线程，是怎么收到通知的呢？一种是异常，另一种是主动检测。当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的 interrupt() 方法，会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常。上面我们提到转换到 WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法，我们看这些方法的签名，发现都会 throws InterruptedException 这个异常。这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法
                   当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在 java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 的 java.nio.channels.Selector 会立即返回。上面这两种情况属于被中断的线程通过异常的方式获得了通知。还有一种是主动检测，如果线程处于 RUNNABLE 状态，并且没有阻塞在某个 I/O 操作上，例如中断计算圆周率的线程 A，这时就得依赖线程 A 主动检测中断状态了。如果其他线程调用线程 A 的 interrupt() 方法，那么线程 A 可以通过 isInterrupted() 方法，检测是不是自己被中断了 
### 创建多少线程才是合适的  
                为什么要使用多线程？
    
                  使用多线程，本质上就是提升程序性能。不过此刻谈到的性能，可能在你脑海里还是比较笼统的，基本上就是快、快、快，这种无法度量的感性认识很不科学，所以在提升性能之前，首要问题是：如何度量性能。度量性能的指标有很多，但是有两个指标是最核心的，它们就是延迟和吞吐量。延迟指的是发出请求到收到响应这个过程的时间；延迟越短，意味着程序执行得越快，性能也就越好。 吞吐量指的是在单位时间内能处理请求的数量；吞吐量越大，意味着程序能处理的请求越多，性能也就越好。这两个指标内部有一定的联系（同等条件下，延迟越短，吞吐量越大），但是由于它们隶属不同的维度（一个是时间维度，一个是空间维度），并不能互相转换。我们所谓提升性能，从度量的角度，主要是降低延迟，提高吞吐量。这也是我们使用多线程的主要目的。      
                多线程的应用场景
                  
                  要想“降低延迟，提高吞吐量”，对应的方法呢，基本上有两个方向，一个方向是优化算法，另一个方向是将硬件的性能发挥到极致。前者属于算法范畴，后者则是和并发编程息息相关了。那计算机主要有哪些硬件呢？主要是两类：一个是 I/O，一个是 CPU。简言之，在并发编程领域，提升性能本质上就是提升硬件的利用率，再具体点来说，就是提升 I/O 的利用率和 CPU 的利用率。

                  估计这个时候你会有个疑问，操作系统不是已经解决了硬件的利用率问题了吗？的确是这样，例如操作系统已经解决了磁盘和网卡的利用率问题，利用中断机制还能避免 CPU 轮询 I/O 状态，也提升了 CPU 的利用率。但是操作系统解决硬件利用率问题的对象往往是单一的硬件设备，而我们的并发程序，往往需要 CPU 和 I/O 设备相互配合工作，也就是说，我们需要解决 CPU 和 I/O 设备综合利用率的问题。关于这个综合利用率的问题，操作系统虽然没有办法完美解决，但是却给我们提供了方案，那就是：多线程。

                  ，如果只有一个线程，执行 CPU 计算的时候，I/O 设备空闲；执行 I/O 操作的时候，CPU 空闲，所以 CPU 的利用率和 I/O 设备的利用率都是 50%。

                  如果有两个线程，如下图所示，当线程 A 执行 CPU 计算的时候，线程 B 执行 I/O 操作；当线程 A 执行 I/O 操作的时候，线程 B 执行 CPU 计算，这样 CPU 的利用率和 I/O 设备的利用率就都达到了 100%

                  如果 CPU 和 I/O 设备的利用率都很低，那么可以尝试通过增加线程来提高吞吐量。

                  在单核时代，多线程主要就是用来平衡 CPU 和 I/O 设备的。如果程序只有 CPU 计算，而没有 I/O 操作的话，多线程不但不会提升性能，还会使性能变得更差，原因是增加了线程切换的成本。但是在多核时代，这种纯计算型的程序也可以利用多线程来提升性能。为什么呢？因为利用多核可以降低响应时间。
                创建多少线程合适？  
                   创建多少线程合适，要看多线程具体的应用场景。我们的程序一般都是 CPU 计算和 I/O 操作交叉执行的，由于 I/O 设备的速度相对于 CPU 来说都很慢，所以大部分情况下，I/O 操作执行的时间相对于 CPU 计算来说都非常长，这种场景我们一般都称为 I/O 密集型计算；和 I/O 密集型计算相对的就是 CPU 密集型计算了，CPU 密集型计算大部分场景下都是纯 CPU 计算。I/O 密集型程序和 CPU 密集型程序，计算最佳线程数的方法是不同的。

                   对于 CPU 密集型计算，多线程本质上是提升多核 CPU 的利用率，所以对于一个 4 核的 CPU，每个核一个线程，理论上创建 4 个线程就可以了，再多创建线程也只是增加线程切换的成本。所以，对于 CPU 密集型的计算场景，理论上“线程的数量 =CPU 核数”就是最合适的。不过在工程上，线程的数量一般会设置为“CPU 核数 +1”，这样的话，当线程因为偶尔的内存页失效或其他原因导致阻塞时，这个额外的线程可以顶上，从而保证 CPU 的利用率

                   对于 I/O 密集型的计算场景，比如前面我们的例子中，如果 CPU 计算和 I/O 操作的耗时是 1:1，那么 2 个线程是最合适的。如果 CPU 计算和 I/O 操作的耗时是 1:2，那多少个线程合适呢？是 3 个线程，如下图所示：CPU 在 A、B、C 三个线程之间切换，对于线程 A，当 CPU 从 B、C 切换回来时，线程 A 正好执行完 I/O 操作。这样 CPU 和 I/O 设备的利用率都达到了 100%。

                   通过上面这个例子，我们会发现，对于 I/O 密集型计算场景，最佳的线程数是与程序中 CPU 计算和 I/O 操作的耗时比相关的，我们可以总结出这样一个公式：最佳线程数 =1 +（I/O 耗时 / CPU 耗时）我们令 R=I/O 耗时 / CPU 耗时，综合上图，可以这样理解：当线程 A 执行 IO 操作时，另外 R 个线程正好执行完各自的 CPU 计算。这样 CPU 的利用率就达到了 100%。不过上面这个公式是针对单核 CPU 的，至于多核 CPU，也很简单，只需要等比扩大就可以了，计算公式如下：最佳线程数 =CPU 核数 * [ 1 +（I/O 耗时 / CPU 耗时）]   
                为什么局部变量是线程安全的？  

                   方法是如何被执行的

                     CPU 去哪里找到调用方法的参数和返回地址？”如果你熟悉 CPU 的工作原理，你应该会立刻想到：通过 CPU 的堆栈寄存器。CPU 支持一种栈结构，栈你一定很熟悉了，就像手枪的弹夹，先入后出。因为这个栈是和方法调用相关的，因此经常被称为调用栈。

                     有三个方法 A、B、C，他们的调用关系是 A->B->C（A 调用 B，B 调用 C），在运行时，会构建出下 面这样的调用栈。每个方法在调用栈里都有自己的独立空间，称为栈帧，每个栈帧里都有对应方法需要的参数和返回地址。当调用方法时，会创建新的栈帧，并压入调用栈；当方法返回时，对应的栈帧就会被自动弹出。也就是说，栈帧和方法是同生共死的。


                     利用栈结构来支持方法调用这个方案非常普遍，以至于 CPU 里内置了栈寄存器。虽然各家编程语言定义的方法千奇百怪，但是方法的内部执行原理却是出奇的一致：都是靠栈结构解决的。Java 语言虽然是靠虚拟机解释执行的，但是方法的调用也是利用栈结构解决的      

                   局部变量存哪里？

                     局部变量的作用域是方法内部，也就是说当方法执行完，局部变量就没用了，局部变量应该和方法同生共死。此时你应该会想到调用栈的栈帧，调用栈的栈帧就是和方法同生共死的，所以局部变量放到调用栈里那儿是相当的合理。事实上，的确是这样的，局部变量就是放到了调用栈里。于是调用栈的结构就变成了下图这样

                     局部变量是和方法同生共死的，一个变量如果想跨越方法的边界，就必须创建在堆里。    

                   调用栈与线程

                     两个线程可以同时用不同的参数调用相同的方法，那调用栈和线程之间是什么关系呢？答案是：每个线程都有自己独立的调用栈。

                     因为如果不是这样，那两个线程就互相干扰了。如下面这幅图所示，线程 A、B、C 每个线程都有自己独立的调用栈。

                     现在，让我们回过头来再看篇首的问题：Java 方法里面的局部变量是否存在并发问题？现在你应该很清楚了，一点问题都没有。因为每个线程都有自己的调用栈，局部变量保存在线程各自的调用栈里面，不会共享，所以自然也就没有并发问题。再次重申一遍：没有共享，就没有伤害。    

                   线程封闭
                     线程封闭，比较官方的解释是：仅在单线程内访问数据。由于不存在共享，所以即便不同步也不会有并发问题，性能杠杠的。  

                     例如从数据库连接池里获取的连接 Connection，在 JDBC 规范里并没有要求这个 Connection 必须是线程安全的。数据库连接池通过线程封闭技术，保证一个 Connection 一旦被一个线程获取之后，在这个线程关闭 Connection 之前的这段时间里，不会再分配给其他线程，从而保证了 Connection 不会有并发问题。   
### 如何用面向对象思想写好并发程序？
              在 Java 语言里，面向对象思想能够让并发编程变得更简单。
              可以从封装共享变量、识别共享变量间的约束条件和制定并发访问策略这三个方面下手。
              一、封装共享变量
                编程领域里面对于共享变量的访问路径就类似于球场的入口，必须严格控制。好在有了面向对象思想，对共享变量的访问路径可以轻松把控。

                面向对象思想里面有一个很重要的特性是封装，封装的通俗解释就是将属性和实现细节封装在对象内部，外界对象只能通过目标对象提供的公共方法来间接访问这些内部属性，这和门票管理模型匹配度相当的高，球场里的座位就是对象属性，球场入口就是对象的公共方法。我们把共享变量作为对象的属性，那对于共享变量的访问路径就是对象的公共方法，所有入口都要安排检票程序就相当于我们前面提到的并发访问策略。

                利用面向对象思想写并发程序的思路，其实就这么简单：将共享变量作为对象属性封装在内部，对所有公共方法制定并发访问策略

                利用面向对象思想写并发程序的思路，其实就这么简单：将共享变量作为对象属性封装在内部，对所有公共方法制定并发访问策略。就拿很多统计程序都要用到计数器来说，下面的计数器程序共享变量只有一个，就是 value，我们把它作为 Counter 类的属性，并且将两个公共方法 get() 和 addOne() 声明为同步方法，这样 Counter 类就成为一个线程安全的类了。

                很多共享变量的值是不会变的，例如信用卡账户的卡号、姓名、身份证。对于这些不会发生变化的共享变量，建议你用 final 关键字来修饰。这样既能避免并发问题，也能很明了地表明你的设计意图，让后面接手你程序的兄弟知道，你已经考虑过这些共享变量的并发安全问题了。
              二、识别共享变量间的约束条件  
                  在没有识别出库存下限要小于库存上限这个约束条件之前，我们制定的并发访问策略是利用原子类，但是这个策略，完全不能保证库存下限要小于库存上限这个约束条件。所以说，在设计阶段，我们一定要识别出所有共享变量之间的约束条件，如果约束条件识别不足，很可能导致制定的并发访问策略南辕北辙。
                  共享变量之间的约束条件，反映在代码里，基本上都会有 if 语句，所以，一定要特别注意竞态条件。
              三、制定并发访问策略  
                 1避免共享：避免共享的技术主要是利于线程本地存储以及为每个任务分配独立的线程。
                 2不变模式：这个在 Java 领域应用的很少，但在其他领域却有着广泛的应用，例如 Actor 模式、CSP 模式以及函数式编程的基础都是不变模式。
                 3管程及其他同步工具：Java 领域万能的解决方案是管程，但是对于很多特定场景，使用 Java 并发包提供的读写锁、并发容器等同步工具会更好  

                 三个原则
## 并发工具 
###  Lock和Condition

                在并发编程领域，有两大核心问题：一个是互斥，即同一时刻只允许一个线程访问共享资源；另一个是同步，即线程之间如何通信、协作。这两大问题，管程都是能够解决的。Java SDK 并发包通过 Lock 和 Condition 两个接口来实现管程，其中 Lock 用于解决互斥问题，Condition 用于解决同步问题。

                再造管程的理由
                  我们前面在介绍死锁问题的时候，提出了一个破坏不可抢占条件方案，但是这个方案 synchronized 没有办法解决。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。但我们希望的是：
                  对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。

                  如果我们重新设计一把互斥锁去解决这个问题，那该怎么设计呢？我觉得有三种方案。
                  1 能够响应中断。synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。这样就破坏了不可抢占条件了。
                  2支持超时。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。
                  3非阻塞地获取锁。如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。      


                  这三种方案可以全面弥补 synchronized 的问题。到这里相信你应该也能理解了，这三个方案就是“重复造轮子”的主要原因，体现在 API 上，就是 Lock 接口的三个方法。详情如下：

                  // 支持中断的API
                  void lockInterruptibly()   throws InterruptedException;
                  // 支持超时的API
                  boolean tryLock(long time, TimeUnit unit)   throws InterruptedException;
                  // 支持非阻塞获取锁的
                  APIboolean tryLock(); 
                如何保证可见性  
                  你已经知道 Java 里多线程的可见性是通过 Happens-Before 规则保证的，而 synchronized 之所以能够保证可见性，也是因为有一条 synchronized 相关的规则：synchronized 的解锁 Happens-Before 于后续对这个锁的加锁。
                  那 Java SDK 里面 Lock 靠什么保证可见性呢？例如在下面的代码中，线程 T1 对 value 进行了 +=1 操作，那后续的线程 T2 能够看到 value 的正确结果吗？
                  它是利用了 volatile 相关的 Happens-Before 规则
                  。Java SDK 里面的 ReentrantLock，内部持有一个 volatile 的成员变量    state，获取锁的时候，会读写 state 的值；解锁的时候，也会读写 state 的值（简化后的代码如下面所示）。也就是说，在执行 value+=1 之前，程序先读写了一次 volatile 变量 state，在执行 value+=1 之后，又读写了一次 volatile 变量 state。根据相关的 Happens-Before 规则：
                  1顺序性规则：对于线程 T1，value+=1 Happens-Before 释放锁的操作 unlock()；
                  2volatile 变量规则：由于 state = 1 会先读取 state，所以线程 T1 的 unlock() 操作 Happens-Before 线程 T2 的 lock() 操作；
                  3传递性规则：线程 T1 的 value+=1  Happens-Before 线程 T2 的 lock() 操作。 
                什么是可重入锁  

                   所谓可重入锁，顾名思义，指的是线程可以重复获取同一把锁。例如下面代码中，当线程 T1 执行到 ① 处时，已经获取到了锁 rtl ，当在 ① 处调用 get() 方法时，会在 ② 再次对锁 rtl 执行加锁操作。此时，如果锁 rtl 是可重入的，那么线程 T1 可以再次加锁成功；如果锁 rtl 是不可重入的，那么线程 T1 此时会被阻塞。     

                   除了可重入锁，可能你还听说过可重入函数，可重入函数怎么理解呢？指的是线程可以重复调用？显然不是，所谓可重入函数，指的是多个线程可以同时调用该函数，每个线程都能得到正确结果；同时在一个线程内支持线程切换，无论被切换多少次，结果都是正确的。多线程可以同时执行，还支持线程切换，这意味着什么呢？线程安全啊。所以，可重入函数是线程安全的。 
                公平锁与非公平锁  
                
                  在使用 ReentrantLock 的时候，你会发现 ReentrantLock 这个类有两个构造函数，一个是无参构造函数，一个是传入 fair 参数的构造函数。fair 参数代表的是锁的公平策略，如果传入 true 就表示需要构造一个公平锁，反之则表示要构造一个非公平锁。  

                  我们介绍过入口等待队列，锁都对应着一个等待队列，如果一个线程没有获得锁，就会进入等待队列，当有线程释放锁的时候，就需要从等待队列中唤醒一个等待的线程。如果是公平锁，唤醒的策略就是谁等待的时间长，就唤醒谁，很公平；如果是非公平锁，则不提供这个公平保证，有可能等待时间短的线程反而先被唤醒。  
                用锁的最佳实践   
                   1永远只在更新对象的成员变量时加锁
                   2永远只在访问可变的成员变量时加锁
                   3永远不在调用其他对象的方法时加锁
                
                们提到过 Java 语言内置的管程里只有一个条件变量，而 Lock&Condition 实现的管程是支持多个条件变量的，这是二者的一个重要区别。在很多并发场景下，支持多个条件变量能够让我们的并发程序可读性更好，实现起来也更容易。例如，实现一个阻塞队列，就需要两个条件变量。

                那如何利用两个条件变量快速实现阻塞队列呢？

                   一个阻塞队列，需要两个条件变量，一个是队列不空（空队列不允许出队），另一个是队列不满（队列已满不允许入队）

                   Lock 和 Condition 实现的管程，线程等待和通知需要调用 await()、signal()、signalAll()，它们的语义和 wait()、notify()、notifyAll() 是相同的。但是不一样的是，Lock&Condition 实现的管程里只能使用前面的 await()、signal()、signalAll()，而后面的 wait()、notify()、notifyAll() 只有在 synchronized 实现的管程里才能使用。如果一不小心在 Lock&Condition 实现的管程里调用了 wait()、notify()、notifyAll()，那程序可就彻底玩儿完了。   
                同步和异步 

                  通俗点来讲就是调用方是否需要等待结果，如果需要等待结果，就是同步；如果不需要等待结果，就是异步。
                  同步，是 Java 代码默认的处理方式。如果你想让你的程序支持异步，可以通过下面两种方式来实现：1调用方创建一个子线程，在子线程中执行方法调用，这种调用我们称为异步调用；
                  2方法实现的时候，创建一个新的线程执行主要逻辑，主线程直接 return，这种方法我们一般称为异步方法  
                Dubbo 源码分析  
                  其实在编程领域，异步的场景还是挺多的，比如 TCP 协议本身就是异步的，我们工作中经常用到的 RPC 调用，在 TCP 协议层面，发送完 RPC 请求后，线程是不会等待 RPC 的响应结果的。
                  其实很简单，一定是有人帮你做了异步转同步的事情。例如目前知名的 RPC 框架 Dubbo 就给我们做了异步转同步的事情，

                  当 RPC 返回结果之前，阻塞调用线程，让调用线程等待；当 RPC 返回结果后，唤醒调用线程，让调用线程重新执行。不知道你有没有似曾相识的感觉，这不就是经典的等待 - 通知机制吗？

                  调用线程通过调用 get() 方法等待 RPC 返回结果，这个方法里面，你看到的都是熟悉的“面孔”：调用 lock() 获取锁，在 finally 里面调用 unlock() 释放锁；获取锁后，通过经典的在循环中调用 await() 方法来实现等待。当 RPC 结果返回时，会调用 doReceived() 方法，这个方法里面，调用 lock() 获取锁，在 finally 里面调用 unlock() 释放锁，获取锁后通过调用 signal() 来通知调用线程，结果已经返回，不用继续等待了。  
### Semaphore：如何快速实现一个限流器     
                 在编程世界里，线程能不能执行，也要看信号量是不是允许。
                 信号量模型 
                   一个计数器，一个等待队列，三个方法
                   在信号量模型里，计数器和等待队列对外是透明的，所以只能通过信号量模型提供的三个方法来访问它们，这三个方法分别是：init()、down() 和 up()。你可以结合下图来形象化地理解。

                   这三个方法详细的语义具体如下所示。
                   init()：设置计数器的初始值。
                   own()：计数器的值减 1；如果此时计数器的值小于 0，则当前线程将被阻塞，否则当前线程可以继续执行。
                   up()：计数器的值加 1；如果此时计数器的值小于或者等于 0，则唤醒等待队列中的一个线程，并将其从等待队列中移除。
                   这里提到的 init()、down() 和 up() 三个方法都是原子性的，并且这个原子性是由信号量模型的实现方保证的。在 Java SDK 里面，信号量模型是由 java.util.concurrent.Semaphore 实现的，Semaphore 这个类能够保证这三个方法都是原子操作。
   
                       void down(){    this.count--;    if(this.count<0){      //将当前线程插入等待队列      //阻塞当前线程    }  }

                       void up(){    this.count++;    if(this.count<=0) {      //移除等待队列中的某个线程T      //唤醒线程T    }  }

                   在 Java SDK 并发包里，down() 和 up() 对应的则是 acquire() 和 release()。 
                 如何使用信号量 
                    只需要在进入临界区之前执行一下 down() 操作，退出临界区之前执行一下 up() 操作就可以了。下面是 Java 代码的示例，acquire() 就是信号量里的 down() 操作，release() 就是信号量里的 up() 操作  
                     //用信号量保证互斥 
                     static void addOne() 
                     { s.acquire(); 
                      try { count+=1; }
                       finally 
                       { s.release();
                        }}
                    下面我们再来分析一下，信号量是如何保证互斥的。假设两个线程 T1 和 T2 同时访问 addOne() 方法，当它们同时调用 acquire() 的时候，由于 acquire() 是一个原子操作，所以只能有一个线程（假设 T1）把信号量里的计数器减为 0，另外一个线程（T2）则是将计数器减为 -1。对于线程 T1，信号量里面的计数器的值是 0，大于等于 0，所以线程 T1 会继续执行；对于线程 T2，信号量里面的计数器的值是 -1，小于 0，按照信号量模型里对 down() 操作的描述，线程 T2 将被阻塞。所以此时只有线程 T1 会进入临界区执行count+=1；。 
                    
                    当线程 T1 执行 release() 操作，也就是 up() 操作的时候，信号量里计数器的值是 -1，加 1 之后的值是 0，小于等于 0，按照信号量模型里对 up() 操作的描述，此时等待队列中的 T2 将会被唤醒。于是 T2 在 T1 执行完临界区代码之后才获得了进入临界区执行的机会，从而保证了互斥性。   
                 快速实现一个限流器   
                   其实实现一个互斥锁，仅仅是 Semaphore 的部分功能，Semaphore 还有一个功能是 Lock 不容易实现的，那就是：Semaphore 可以允许多个线程访问一个临界区。

                   现实中还有这种需求？有的。比较常见的需求就是我们工作中遇到的各种池化资源，例如连接池、对象池、线程池等等。其中，你可能最熟悉数据库连接池，在同一时刻，一定是允许多个线程同时使用连接池的，当然，每个连接在被释放前，是不允许其他线程使用的。

                   我在工作中也遇到了一个对象池的需求。所谓对象池呢，指的是一次性创建出 N 个对象，之后所有的线程重复利用这 N 个对象，当然对象在被释放前，也是不允许其他线程使用的。对象池，可以用 List 保存实例对象，这个很简单。但关键是限流器的设计，这里的限流，指的是不允许多于 N 个线程同时进入临界区。那如何快速实现一个这样的限流器呢？这种场景，我立刻就想到了信号量的解决方案

                   信号量的计数器，在上面的例子中，我们设置成了 1，这个 1 表示只允许一个线程进入临界区，但如果我们把计数器的值设置成对象池里对象的个数 N，就能完美解决对象池的限流问题了
                    // 利用对象池的对象，调用func  R exec(Function<T,R> func) {    T t = null;    sem.acquire();    try {      t = pool.remove(0);      return func.apply(t);    } finally {      pool.add(t);      sem.release();    }  }}
                   假设对象池的大小是 10，信号量的计数器初始化为 10，那么前 10 个线程调用 acquire() 方法，都能继续执行，相当于通过了信号灯，而其他线程则会阻塞在 acquire() 方法上。对于通过信号灯的线程，我们为每个线程分配了一个对象 t（这个分配工作是通过 pool.remove(0) 实现的），分配完之后会执行一个回调函数 func，而函数的参数正是前面分配的对象 t ；执行完回调函数之后，它们就会释放对象（这个释放工作是通过 pool.add(t) 实现的），同时调用 release() 方法来更新信号量的计数器。如果此时信号量里计数器的值小于等于 0，那么说明有线程在等待，此时会自动唤醒等待的线程。   
## 锁
### 锁：Synchronized、ReentrantLock
#### Synchronized      
                   java内建的同步机制 提供互斥的语义和可见性 当一个线程获取当前锁时，其他线程只能等待
                   Synchronized 底层实现 是由一对monitorenter .monitorexit指令是想的 monitor对象是同步的基本实现
                   java6之前 monitor的实现完全是依靠操作系统内部的互斥锁。因为要进行用户态到内核态的切换，同步操作
                   是一个无差别的重量级操作

                   三种不同的monitor实现
                    重入、偏向
                    偏斜锁 轻量级锁  重量级锁
                   锁的升级降级 就是jvm优化sychronized运行机制 当jvm检测到不同的竞争状况时 会自动切换到适合的锁实现，这种切换就是锁的升级和降级
                    当没有竞争出现时 默认会使用偏斜锁 Jvm会利用cas操作，在对象头上的mark word不分设置线程Id,以表示这个对象偏向于当前线程，并不涉及正真的互斥锁，这样做假设是基于很多应用场景，大部分对象生命周期都最多被一个线程锁定，使用偏斜锁可以降低无竞争开销

                    如果有另外的线程视图锁定某个已经被偏斜过的对象，jvm需要撤销偏斜锁，并切换到轻量级锁的实现，轻量级锁依赖cas操作mark word来试图获取锁如果成功，使用普通的轻量级锁，否则进一步升级为重量级锁

                    锁降级是会发生的 jvm进入安全点会检查是否有闲置的monitor 试图进行降价。

                    sychronized是jvm内部的intrinsic lock,轻量级 重量级 偏斜锁核心代码不在核心类库中，在 jvm代码中
                     偏斜锁并不适合所有的应用场景 revoke操作是比较重的行为
                     偏斜锁会延缓JIT预热的进程 很多场景关闭偏斜锁 -XX:-UseBiasedLocking

                     biasedLocking 定义偏斜锁相关操作 revoke_and-rebias 是获取偏斜锁的相关操作
                     revoke_as_safepoint则定义了当检测到安全点的处理逻辑
                     fast_exit  slow_exit时对应的锁释放逻辑
                      ReadWriteLock->ReentranReadWriteLock
                      StampedLock 不支持再入语义 不是以持有锁的线程为单位

                      lock
                        ReentranLock
                        ReentranReadWriteLock.ReadLock
                        ReentranReadWriteLock.WriteLock
                      要么不占 要么独占，实际应用场景中，有的时候不需要大量竞争的写操作，而是以并发读取操作为主
                      
                      java并发包提供的读写锁等扩展锁的能力 基于的原理是多个读锁是不需要互斥的，因为读操作并不会改写数据，所以并不存在相互干扰。而写操作则会导致并发一致性的问题，所以在线程之间 读写线程之间 需要精心设计的互斥逻辑
                      读写锁开销较大

                      后期引入了stampedLock 在提供类似读写锁的同时。还支持优化读模式，该模式假设大多数情况并不会和写操作冲突
                      其逻辑是先试着读，然后通过validate方法确认是否进入了写模式，如果没有进入，则成功避免开销；如果进入则尝试获取读锁

                  ReentrantLock 再入锁  通过调用代码lock获取  提供公平性 或者条件定义，编码中必须显示调用unlock

                  线程安全
                    是一个多线程环境下正确性的概念 保证多线程环境下共享的 可修改的状态的正确性 状态可以看做是数据
                    如果状态不是共享的 或者不可修改的也就不存在线程安全问题
                  两个办法
                    封装 将内部对象状态隐藏 保护起来
                    不可变  
                  几个基本特性
                    原子性  相关操作中途不会被其他线程干扰 通过同步机制实现
                    可见性   一个线程修改了某个共享变量 其状态立即能够被其他线程知晓 将线程本地状态反应到主内存上 volatiel 负责保证可见性
                    有序性   保证线程内串行语义 避免指令重排


                  ReentrantLock 表示当一个线程视图获取一个它已经获取的锁时 这个获取动作就会自动成功 锁的持有是以线程为单位的而不是基于调用次数
                   再入锁可以设置公平性 
                         ReentrantLock fairLock = new ReentrantLock(true);
                   当公平性为true时 倾向于选择等待时间最久的线程  公平性是为了减少线程饥饿
                   公平性会造成性能下降 为了保证锁释放 每一个lock 都会对应一个unlock
                     带超时的获取锁尝试
                     判断是否有线程或者某个特定线程 在排队等待获取锁
                     可以响应中断
                   条件变量  
                     Condition 是将wait notify notifyall等操作装换为相应的对象 将负责晦涩的同步操作转变为只管可控的对象行为
                     条件变量应用最典型的场景就是ArrayBlockingQueue
                      通过再入锁获取条件变量
                       lock.lockUbterruptibly() 拿到可中断的锁
                      take 中 notempty.await 阻塞
                      enqueue 中 notEmpty.signal() 通知条件满足
### ReentrantLock 
               ReentrantLock 中文我们习惯叫做可重入互斥锁，可重入的意思是同一个线程可以对同一个共享资源重复的加锁或释放锁，互斥就是 AQS 中的排它锁的意思，只允许一个线程获得锁
               类注释  
                  可重入互斥锁，和 synchronized 锁具有同样的功能语义，但更有扩展性；
                  构造器接受 fairness 的参数，fairness 是 ture 时，保证获得锁时的顺序，false 不保证；
                  公平锁的吞吐量较低，获得锁的公平性不能代表线程调度的公平性；
                  tryLock() 无参方法没有遵循公平性，是非公平的（lock 和 unlock 都有公平和非公平，而 tryLock 只有公平锁，所以单独拿出来说一说）。 
                  ReentrantLock 的公平和非公平，是针对获得锁来说的，如果是公平的，可以保证同步队列中的线程从头到尾的顺序依次获得锁，非公平的就无法保证，在释放锁的过程中，我们是没有公平和非公平的说法的。  
                类结构
                   ReentrantLock 类本身是不继承 AQS 的，实现了 Lock 接口   
                    public class ReentrantLock implements Lock, java.io.Serializable {}
                   Lock 接口定义了各种加锁，释放锁的方法，接口有如下几个：
                    // 获得锁方法，获取不到锁的线程会到同步队列中阻塞排队
                    void lock();
                   // 获取可中断的锁
                   void lockInterruptibly() throws InterruptedException;
                   / / 尝试获得锁，如果锁空闲，立马返回 true，否则返回 false
                    boolean tryLock();
                   // 带有超时等待时间的锁，如果超时时间到了，仍然没有获得锁，返回 false
                   boolean tryLock(long time, TimeUnit unit) throws InterruptedException;
                   // 释放锁
                   void unlock();
                   // 得到新的 Condition
                   Condition newCondition(); 
                   ReentrantLock 就负责实现这些接口，我们使用时，直接面对的也是这些方法，这些方法的底层实现都是交给 Sync 内部类去实现的，Sync 类的定义如下：
                   abstract static class Sync extends AbstractQueuedSynchronizer {}  

                   Sync 继承了 AbstractQueuedSynchronizer ，所以 Sync 就具有了锁的框架，根据 AQS 的框架，Sync 只需要实现 AQS 预留的几个方法即可，但 Sync 也只是实现了部分方法，还有一些交给子类 NonfairSync 和 FairSync 去实现了，NonfairSync 是非公平锁，FairSync 是公平锁，定义如下：
                   // 同步器 Sync 的两个子类锁
                   static final class FairSync extends Sync {}
                   static final class NonfairSync extends Sync {}
                构造器
                   无参构造器默认构造是非公平的锁，有参构造器可以选择。
                    从构造器中可以看出，公平锁是依靠 FairSync 实现的，非公平锁是依靠 NonfairSync 实现的。
                Sync 同步器  
                  nonfairTryAcquire 
                     // 同步器的状态是 0，表示同步器的锁没有人持有
                     if (c == 0) {
                     // 当前线程持有锁
                     if (compareAndSetState(0, acquires)) {
                     // 标记当前持有锁的线程是谁
                     setExclusiveOwnerThread(current);
                       return true;
                     }
                     }
                     // 如果当前线程已经持有锁了，同一个线程可以对同一个资源重复加锁，代码实现的是  可重入锁
                     else if (current == getExclusiveOwnerThread()) {
                     // 当前线程持有锁的数量 + acquires
                     int nextc = c + acquires;
                     // int 是有最大值的，<0 表示持有锁的数量超过了 int 的最大值
                     if (nextc < 0) // overflow
                        throw new Error("Maximum lock count exceeded");
                     setState(nextc);
                     return true;
                     三点注意
                     通过判断 AQS 的 state 的状态来决定是否可以获得锁，0 表示锁是空闲的；
                     else if 的代码体现了可重入加锁，同一个线程对共享资源重入加锁，底层实现就是把 state + 1，并且可重入的次数是有限制的，为 Integer 的最大值；
                     这个方法是非公平的，所以只有非公平锁才会用到，公平锁是另外的实现。
                     无参的 tryLock 方法调用的就是此方法，tryLock 的方法源码如下：
                      public boolean tryLock() {
                      // 入参数是 1 表示尝试获得一次锁
                      return sync.nonfairTryAcquire(1);
                      } 
                  tryRelease
                   // 释放锁方法，非公平和公平锁都使用
                   protected final boolean tryRelease(int releases) {
                     // 当前同步器的状态减去释放的个数，releases 一般为 1
                     int c = getState() - releases;
                     // 当前线程根本都不持有锁，报错
                    if (Thread.currentThread() != getExclusiveOwnerThread())
                     // 如果 c 为 0，表示当前线程持有的锁都释放了
                     if (c == 0) {
                         free = true;
                        setExclusiveOwnerThread(null);
                     }
                     // 如果 c 不为 0，那么就是可重入锁，并且锁没有释放完，用 state 减去 releases 即可，无需做其他操作
                      setState(c);
                     tryRelease 方法是公平锁和非公平锁都公用的，在锁释放的时候，是没有公平和非公平的说法的。
                      从代码中可以看到，锁最终被释放的标椎是 state 的状态为 0，在重入加锁的情况下，需要重入解锁相应的次数后，才能最终把锁释放，比如线程 A 对共享资源 B 重入加锁 5 次，那么释放锁的话，也需要释放 5 次之后，才算真正的释放该共享资源了。 
                FairSync 公平锁
                  FairSync 公平锁只实现了 lock 和 tryAcquire 两个方法，lock 方法非常简单
                  // acquire 是 AQS 的方法，表示先尝试获得锁，失败之后进入同步队列阻塞等待
                  final void lock() {
                       acquire(1);
                    }
                    // hasQueuedPredecessors 是实现公平的关键
                  // 会判断当前线程是不是属于同步队列的头节点的下一个节点(头节点是释放锁的节点)
                  // 如果是(返回false)，符合先进先出的原则，可以获得锁
                  // 如果不是(返回true)，则继续等待
                   if (!hasQueuedPredecessors() && 
                NonfairSync 非公平锁  
                  // 加锁
                  final void lock() {
                  // cas 给 state 赋值
                  if (compareAndSetState(0, 1))
                  // cas 赋值成功，代表拿到当前锁，记录拿到锁的线程
                  setExclusiveOwnerThread(Thread.currentThread());
                  else
                  // acquire 是抽象类AQS的方法,
                  // 会再次尝试获得锁，失败会进入到同步队列中
                  acquire(1); 
                如何串起来
                  lock加锁 
                    公平locl-> fairsync.lock-> aqs.acquire->fairSyncTryAcquire
                    非公平locl-> Nonfairsync.lock-> aqs.acquire->NonfairSyncTryAcquire 
                   tryLock 
                     / 无参构造器
                     public boolean tryLock() {
                       return sync.nonfairTryAcquire(1);
                     }
                     // timeout 为超时的时间，在时间内，仍没有得到锁，会返回 false
                     public boolean tryLock(long timeout, TimeUnit unit)
                      throws InterruptedException {
                        return sync.tryAcquireNanos(1, unit.toNanos(timeout));
                     }
                   unlock释放锁
                     unlock 释放锁的方法，底层调用的是 Sync 同步器的 release 方法，release 是 AQS 的方法，分成两步：
                      尝试释放锁，如果释放失败，直接返回 false；
                       释放成功，从同步队列的头节点的下一个节点开始唤醒，让其去竞争锁。
                     // 释放锁
                     public void unlock() {
                           sync.release(1);
                     }  
                  Condition
                  ReentrantLock 对 Condition 并没有改造，直接使用 AQS 的 ConditionObject 即可   
### CountDownLatch、Atomic 等其它源码解析
               CountDownLatch
                 CountDownLatch 中文有的叫做计数器，也有翻译为计数锁，其最大的作用不是为了加锁，而是通过计数达到等待的功能，主要有两种形式的等待：
                 让一组线程在全部启动完成之后，再一起执行（先启动的线程需要阻塞等待后启动的线程，直到一组线程全部都启动完成后，再一起执行）；
                 主线程等待另外一组线程都执行完成之后，再继续执行

                 ountDownLatch 有两个比较重要的 API，分别是 await 和 countDown，管理着线程能否获得锁和锁的释放（也可以称为对 state 的计数增加和减少）。
                
                 await 
                   await 我们可以叫做等待，也可以叫做加锁，有两种不同入参的方法
                   无参 await 底层使用的是 acquireSharedInterruptibly 方法，有参的使用的是 tryAcquireSharedNanos 方法，这两个方法都是 AQS 的方法，底层实现很相似，主要分成两步：
                   使用子类的 tryAcquireShared 方法尝试获得锁，如果获取了锁直接返回，获取不到锁走  2；
                   获取不到锁，用 Node 封装一下当前线程，追加到同步队列的尾部，等待在合适的时机去获得锁。

                   第二步是 AQS 已经实现了，第一步 tryAcquireShared 方法是交给 Sync 实现的，源码如下：
                   // 如果当前同步器的状态是 0 的话，表示可获得锁
                   protected int tryAcquireShared(int acquires) {
                      return (getState() == 0) ? 1 : -1;
                   }
                   
                   获得锁的代码也很简单，直接根据同步器的 state 字段来进行判断，但还是有两点需要注意一下：

                   CountDownLatch 的 state 并不是 AQS 的默认值 0，而是可以赋值的，是在  CountDownLatch 初始化的时候赋值的，代码如下：
                   // 初始化,count 代表 state 的初始化值
                   public CountDownLatch(int count) {
                   if (count < 0) throw new IllegalArgumentException("count < 0");
                     // new Sync 底层代码是 state = count;
                     this.sync = new Sync(count);
                   }  
                   这里的初始化的 count 和一般的锁意义不太一样，count  表示我们希望等待的线程数，在两种不同的等待场景中，count 有不同的含义：
                   让一组线程在全部启动完成之后，再一起执行的等待场景下， count 代表一组线程的个数；
                   主线程等待另外一组线程都执行完成之后，再继续执行的等待场景下，count  代表一组线程的个数。
                   所以我们可以把 count 看做我们希望等待的一组线程的个数，可能我们是等待一组线程全 部启动完成，可能我们是等待一组线程全部执行完成  
                 countDown
                   countDown 中文翻译为倒计时，每调用一次，都会使 state 减一，底层调用的方法如下：
                   public void countDown() {
                     sync.releaseShared(1);
                   }  
                   releaseShared 是 AQS 定义的方法，方法主要分成两步：
                   尝试释放锁（tryReleaseShared），锁释放失败直接返回，释放成功走 2；
                   释放当前节点的后置等待节点。
                   第二步 AQS 已经实现了，第一步是 Sync 实现的，我们一起来看下 tryReleaseShared 方法的实现源码：
                     // 对 state 进行递减，直到 state 变成 0；
                     // state 递减为 0 时，返回 true，其余返回 false
                      boolean tryReleaseShared(int releases) {
                    // 自旋保证 CAS 一定可以成功
                     for (;;) {
                        int c = getState();
                        // state 已经是 0 了，直接返回 false
                        if (c == 0)
                         return false;
                        // 对 state 进行递减
                         int nextc = c-1;
                        if (compareAndSetState(c, nextc))
                         return nextc == 0;
                      }
                    从源码中可以看到，只有到 count 递减到 0 时，countDown 才会返回 true。
                 两种示例
                    // await 时有两点需要注意：await 时 state 不会发生变化，2：startSignal 的state初始化是 1，所以所有子线程都是获取不到锁的，都需要到同步队列中去等待，达到先启动的子线程等待后面启动的子线程的结果
                     startSignal.await();
                      doWork();
                      // countDown 每次会使 state 减一，doneSignal 初始化为 9，countDown 前 8 次执行都会返回 false (releaseShared 方法)，执行第 9 次时，state 递减为 0，会 countDown 成功，表示所有子线程都执行完了，会释放 await 在 doneSignal 上的主线程
                     doneSignal.countDown();  

                      // state 初始化为 1 很关键，子线程是不断的 await，await 时 state 是不会变化的，并且发现 state 都是 1，所有线程都获取不到锁
                     // 造成所有线程都到同步队列中去等待，当主线程执行 countDown 时，就会一起把等待的线程给释放掉
                     CountDownLatch startSignal = new CountDownLatch(1);
                    // state 初始化成 9，表示有 9 个子线程执行完成之后，会唤醒主线程
                    CountDownLatch doneSignal = new CountDownLatch(9);


                      // 这行代码唤醒 9 个子线程，开始执行(因为 startSignal 锁的状态是 1，所以调用一次 countDown 方法就可以释放9个等待的子线程)
                       startSignal.countDown();
                     // 这行代码使主线程陷入沉睡，等待 9 个子线程执行完成之后才会继续执行(就是等待子线程执行 doneSignal.countDown())
                    doneSignal.await();        
### Atomic 原子操作类
                 Atomic 打头的原子操作类，在高并发场景下，都是线程安全的
                  // 自减 1，并返回自增之前的值    
                  public final int getAndDecrement() {
                    return unsafe.getAndAddInt(this, valueOffset, -1);
                  }
                 线程安全的操作方法，底层都是使用 unsafe 方法实现，以上几个 unsafe 方法不是使用 Java 实现的，都是线程安全的

                 AtomicInteger 是对 int 类型的值进行自增自减，那如果 Atomic 的对象是个自定义类怎么办呢，Java 也提供了自定义对象的原子操作类，叫做 AtomicReference。AtomicReference 类可操作的对象是个泛型，所以支持自定义类，其底层是没有自增方法的，操作的方法可以作为函数入参传递，源码如下：

                  // 对 x 执行 accumulatorFunction 操作
                    // accumulatorFunction 是个函数，可以自定义想做的事情
                 // 返回老值
                  public final V getAndAccumulate(V x,
                                BinaryOperator<V> accumulatorFunction) {
                      // prev 是老值，next 是新值
                      V prev, next;
                    // 自旋 + CAS 保证一定可以替换老值
                   do {
                          prev = get();
                          // 执行自定义操作
                         next = accumulatorFunction.apply(prev, x);
                       } while (!compareAndSet(prev, next));
                  return prev;
                  }
## 锁面试题
###  AQS 相关面试题        
#### 说说自己对 AQS 的理解？
                    如果和面试官面对面的话，可以边说边画出我们在 AQS 源码解析上中画出的整体架构图，并且可以这么说：
                       AQS 是一个锁框架，它定义了锁的实现机制，并开放出扩展的地方，让子类去实现，比如我们在 lock 的时候，AQS 开放出 state 字段，让子类可以根据 state 字段来决定是否能够获得锁，对于获取不到锁的线程 AQS 会自动进行管理，无需子类锁关心，这就是 lock 时锁的内部机制，封装的很好，又暴露出子类锁需要扩展的地方；
                       AQS 底层是由同步队列 + 条件队列联手组成，同步队列管理着获取不到锁的线程的排队和释放，条件队列是在一定场景下，对同步队列的补充，比如获得锁的线程从空队列中拿数据，肯定是拿不到数据的，这时候条件队列就会管理该线程，使该线程阻塞；
                       AQS 围绕两个队列，提供了四大场景，分别是：获得锁、释放锁、条件队列的阻塞，条件队列的唤醒，分别对应着 AQS 架构图中的四种颜色的线的走向。
                       以上三点都是 AQS 全局方面的描述，接着你可以问问面试官要不要说细一点，可以的话，按照 AQS 源码解析上下两篇，把四大场景都说一下就好了。

                       这样说的好处是很多的：
                         面试的主动权把握在自己手里，而且都是自己掌握的知识点；
                         由全到细的把 AQS 全部说完，会给面试官一种你对 AQS 了如指掌的感觉，再加上全部说完耗时会很久，面试时间又很有限，面试官就不会再问关于 AQS 一些刁钻的问题了，这样 AQS 就可以轻松过关。
#### 多个线程通过锁请求共享资源，获取不到锁的线程怎么办？
                     加锁(排它锁)主要分为以下四步：
                      尝试获得锁，获得锁了直接返回，获取不到锁的走到 2；
                      用 Node 封装当前线程，追加到同步队列的队尾，追加到队尾时，又有两步，如 3 和 4；
                      自旋 + CAS 保证前一个节点的状态置为 signal；
                      阻塞自己，使当前线程进入等待状态。
                      获取不到锁的线程会进行 2、3、4 步，最终会陷入等待状态，这个描述的是排它锁。
#### 排它锁和共享锁的处理机制是一样的么？     
                      不同的是在于第一步，线程获得排它锁的时候，仅仅把自己设置为同步队列的头节点即可，但如果是共享锁的话，还会去唤醒自己的后续节点，一起来获得该锁。

####  共享锁和排它锁的区别？
                      排它锁的意思是同一时刻，只能有一个线程可以获得锁，也只能有一个线程可以释放锁。
                      共享锁可以允许多个线程获得同一个锁，并且可以设置获取锁的线程数量，共享锁之所以能够做到这些，是因为线程一旦获得共享锁，把自己设置成同步队列的头节点后，会自动的去释放头节点后等待获取共享锁的节点，让这些等待节点也一起来获得共享锁，而排它锁就不会这么干。  
                 排它锁和共享锁说的是加锁时的策略，那么锁释放时有排它锁和共享锁的策略么？
                      是的，排它锁和共享锁，主要体现在加锁时，多个线程能否获得同一个锁。
                      但在锁释放时，是没有排它锁和共享锁的概念和策略的，概念仅仅针对锁获取。      
#### 描述下同步队列？
                      同步队列底层的数据结构就是双向的链表，节点叫做 Node，头节点叫做 head，尾节点叫做 tail，节点和节点间的前后指向分别叫做 prev、next，如果是面对面面试的话，可以画一下 AQS 整体架构图中的同步队列。
                      同步队列的作用：阻塞获取不到锁的线程，并在适当时机释放这些线程。
                      实现的大致过程：当多个线程都来请求锁时，某一时刻有且只有一个线程能够获得锁（排它锁），那么剩余获取不到锁的线程，都会到同步队列中去排队并阻塞自己，当有线程主动释放锁时，就会从同步队列中头节点开始释放一个排队的线程，让线程重新去竞争锁   
#### 描述下线程入、出同步队列的时机和过程？
                      (排它锁为例)从 AQS 整体架构图中，可以看出同步队列入队和出队都是有两个箭头指向，所以入队和出队的时机各有两个。
                      同步队列入队时机：
                        多个线程请求锁，获取不到锁的线程需要到同步队列中排队阻塞；
                        条件队列中的节点被唤醒，会从条件队列中转移到同步队列中来。
                      同步队列出队时机：
                        锁释放时，头节点出队；
                        获得锁的线程，进入条件队列时，会释放锁，同步队列头节点开始竞争锁。
                       四个时机的过程可以参考 AQS 源码解析，1 参考 acquire 方法执行过程，2 参考 signal 方法，3 参考 release 方法，4 参考 await 方法。
#### 为什么 AQS 有了同步队列之后，还需要条件队列？
                       ，一般情况下，我们只需要有同步队列就好了，但在上锁后，需要操作队列的场景下，一个同步队列就搞不定了，需要条件队列进行功能补充，比如当队列满时，执行 put 操作的线程会进入条件队列等待，当队列空时，执行 take 操作的线程也会进入条件队列中等待，从一定程度上来看，条件队列是对同步队列的场景功能补充 
#### 描述一下条件队列中的元素入队和出队的时机和过程？
                       入队时机：执行 await 方法时，当前线程会释放锁，并进入到条件队列。
                        出队时机：执行 signal、signalAll 方法时，节点会从条件队列中转移到同步队列中。 

#### 述一下条件队列中的节点转移到同步队列中去的时机和过程
                        时机：当有线程执行 signal、signalAll 方法时，从条件队列的头节点开始，转移到同步队列中去。
                       过程主要是以下几步：
                           找到条件队列的头节点，头节点 next 属性置为 null，从条件队列中移除了；
                           头节点追加到同步队列的队尾；
                           头节点状态（waitStatus）从 CONDITION 修改成 0（初始化状态）；
                           将节点的前一个节点状态置为 SIGNAL。 
#### 线程入条件队列时，为什么需要释放持有的锁？
                       原因很简单，如果当前线程不释放锁，一旦跑去条件队里中阻塞了，后续所有的线程都无法获得锁，正确的场景应该是：当前线程释放锁，到条件队列中去阻塞后，其他线程仍然可以获得当前锁。
### AQS 子类锁面试题
#### 如果我要自定义锁，大概的实现思路是什么样子的？
                     新建内部类继承 AQS，并实现 AQS 的 tryAcquire 和 tryRelease 两个方法，在 tryAcquire 方法里面实现控制能否获取锁，比如当同步器状态 state 是 0 时，即可获得锁，在 tryRelease 方法里面控制能否释放锁，比如将同步器状态递减到 0 时，即可释放锁；
                      对外提供 lock、release 两个方法，lock 表示获得锁的方法，底层调用 AQS 的 acquire 方法，release 表示释放锁的方法，底层调用 AQS 的 release 方法。
                   描述 ReentrantLock 两大特性：可重入性和公平性？底层分别如何实现的？
                      可重入性说的是线程可以对共享资源重复加锁，对应的，释放时也可以重复释放，对于 ReentrantLock 来说，在获得锁的时候，state 会加 1，重复获得锁时，不断的对 state 进行递增即可，比如目前 state 是 4，表示线程已经对共享资源加锁了 4 次，线程每次释放共享资源的锁时，state 就会递减 1，直到递减到 0 时，才算真正释放掉共享资源。

                      公平性和非公平指的是同步队列中的线程得到锁的机制，如果同步队列中的线程按照阻塞的顺序得到锁，我们称之为公平的，反之是非公平的，公平的底层实现是 ReentrantLock 的 tryAcquire 方法（调用的是 AQS 的 hasQueuedPredecessors 方法）里面实现的，要释放同步队列的节点时（或者获得锁时），判断当前线程节点是不是同步队列的头节点的后一个节点，如果是就释放，不是则不能释放，通过这种机制，保证同步队列中的线程得到锁时，是按照从头到尾的顺序的。

                    如果一个线程需要等待一组线程全部执行完之后再继续执行，有什么好的办法么？是如何实现的？
                       CountDownLatch 就提供了这样的机制，比如一组线程有 5 个，只需要在初始化 CountDownLatch 时，给同步器的 state 赋值为 5，主线程执行 CountDownLatch.await ，子线程都执行 CountDownLatch.countDown 即可。
### 各种锁在工作中使用场景和细节  
####                synchronized
                   synchronized 是可重入的排它锁，和 ReentrantLock 锁功能相似，任何使用 synchronized 的地方，几乎都可以使用 ReentrantLock 来代替，两者最大的相似点就是：可重入 + 排它锁，两者的区别主要有这些：
                   ReentrantLock 的功能更加丰富，比如提供了 Condition，可以打断的加锁 API、能满足锁 + 队列的复杂场景等等；
                   ReentrantLock 有公平锁和非公平锁之分，而 synchronized 都是非公平锁；
                   两者的使用姿势也不同，ReentrantLock 需要申明，有加锁和释放锁的 API，而 synchronized 会自动对代码块进行加锁释放锁的操作，synchronized 使用起来更加方便。

                   共享资源初始化
                     在分布式的系统中，我们喜欢把一些死的配置资源在项目启动的时候加锁到 JVM 内存里面去，这样请求在拿这些共享配置资源时，就可直接从内存里面拿，不必每次都从数据库中拿，减少了时间开销。
                     一般这样的共享资源有：死的业务流程配置 + 死的业务规则配置
                     共享资源初始化的步骤一般为：项目启动 -> 触发初始化动作 ->单线程从数据库中捞取数据 -> 组装成我们需要的数据结构 -> 放到 JVM 内存中。
                     @PostConstruct 注解的作用是在 Spring 容器初始化时，再执行该注解打上的方法，也就是说上图说的 init 方法触发的时机，是在 Spring 容器启动的时候。 

                     不是可以直接使用了 ConcurrentHashMap 么，为什么还需要加锁呢？的确 ConcurrentHashMap 是线程安全的，但它只能够保证 Map 内部数据操作时的线程安全，是无法保证多线程情况下，查询数据库并组装数据的整个动作只执行一次的，我们加 synchronized 锁住的是整个操作，保证整个操作只执行一次。
####         CountDownLatch
                  向线程池提交了 30 个任务后，主线程如何等待 30 个任务都执行完成呢？因为主线程需要收集 30 个子任务的执行情况，并汇总返回给前端。
                  CountDownLatch 可以的，CountDownLatch 具有这种功能，让主线程去等待子任务全部执行完成之后才继续执行。
                  此时还有一个关键，我们需要知道子线程执行的结果，所以我们用 Runnable 作为线程任务就不行了，因为 Runnable 是没有返回值的，我们需要选择 Callable 作为任务。
           -- 重写锁的设计结构和细节
                1 需求
                   一般自定义锁的时候，我们都是根据需求来进行定义的，不可能凭空定义出锁来，说到共享锁，大家可能会想到很多场景，比如说对于共享资源的读锁可以是共享的，比如对于数据库链接的共享访问，比如对于 Socket 服务端的链接数是可以共享的，场景有很多，我们选择共享访问数据库链接这个场景来定义一个锁。
                2 详细设计
                    假定(以下设想都为假定)我们的数据库是单机 mysql，只能承受 10 个链接，创建数据库链接时，我们是通过最原始 JDBC 的方式，我们用一个接口把用 JDBC 创建链接的过程进行了封装，这个接口我们命名为：创建链接接口。
                    共享访问数据库链接的整体要求如下：所有请求加在一起的 mysql 链接数，最大不能超过 10（包含 10），一旦超过 10，直接报错。 
                3 
                 定义锁
                     首先我们需要定义一个锁出来，定义时需要有两个元素：
                      锁的定义：同步器 Sync；
                      锁对外提供的加锁和解锁的方法。   
                      // 共享不公平锁
                     public class ShareLock implements Serializable{
                     // 同步器
                     private final Sync sync;
                     // 用于确保不能超过最大值
                     private final int maxCount;

                     唯一需要注意的是，锁需要规定好 API 的规范，主要是两方面：
                       API 需要什么，就是锁在初始化的时候，你需要传哪些参数给我，在 ShareLock 初始化时，需要传最大可共享锁的数目；
                       需要定义自身的能力，即定义每个方法的入参和出参。在 ShareLock 的实现中，加锁和释放锁的入参都没有，是方法里面写死的 1，表示每次方法执行，只能加锁一次或释放锁一次，出参是布尔值，true 表示加锁或释放锁成功，false 表示失败，底层使用的都是 Sync 非公平锁。
                 定义同步器 Sync
                    边界的判断，比如入参是否非法，释放锁时，会不会出现预期的 state 非法等边界问题，对于此类问题我们都需要加以判断，体现出思维的严谨性；
                    加锁和释放锁，需要用 for 自旋 + CAS 的形式，来保证当并发加锁或释放锁时，可以重试成功。写 for 自旋时，我们需要注意在适当的时机要 return，不要造成死循环，CAS 的方法 AQS 已经提供了，不要自己写，我们自己写的 CAS 方法是无法保证原子性的。
                  通过能否获得锁来决定能否得到链接
                     通过 JDBC 建立和 Mysql 的链接；
                     结合锁，来防止请求过大时，Mysql 的总链接数不能超过 10 个。

                     public class MysqlConnection {
                         private final ShareLock lock;
  
                         // maxConnectionSize 表示最大链接数
                        public MysqlConnection(int maxConnectionSize) {
                            lock = new ShareLock(maxConnectionSize);
                     }
                     }

                     // 对外获取 mysql 链接的接口
                      // 这里不用try finally 的结构，获得锁实现底层不会有异常
                     // 即使出现未知异常，也无需释放锁
                     public Connection getLimitConnection() {
                         if (lock.lock()) {
                              return getConnection();
                       }
                        return null;
                        }

                      // 对外释放 mysql 链接的接口
                    public boolean releaseLimitConnection() {
                      return lock.unLock();
                     }
### java 程序什么情况会产生死锁如何定位和修复
               死锁是一种特定的程序状态 在实体之间，由于循环依赖导致彼此一直处于等待状态之中，没有任何个体能够继续前进
               死锁不仅仅是在线程之间会发生，存在资源独占的进程之间同样也会出现死锁
               通常来说  我们大多聚焦于在多线程中的死锁，指两个线程或者多个线程之间互相持有对方需要的锁，而处于阻塞的状态

               定位死锁最常见的方式就是利用jstack等工具获取线程栈，然后定位互相之间的依赖关系，进而找到死锁
               如果明显的死锁 往往jstack就能直接定位，类似jconsole甚至在图形界面进行有限制的死锁检测

               定位死锁：
                 1 使用jps或者系统的ps命令 任务管理器 确定进程id
                 2 调用jstack 获取线程栈  jstack pid
                 找到处于blocked状态的线程，按照试图获取的锁的id查找，很快就定位问题
               
               区分线程状态-》查看等待目标-》对比Monitor等持有状态
               开发自己死锁定位工具 参考ThreadMXBean  


               死锁发生的条件
                 互斥条件
                 互斥条件是长期持有的，在使用结束之前，自己不会释放，也不能被其他线程抢占
                 循环依赖关系，两个或者多个个体之间出现了锁的链条环

               如何避免死锁
                  1避免使用多个锁
                  2将对象和锁之间的关系，用图形化的方式表示分别抽取出来
                    DLSample->锁A->锁B
                   然后根据对象组合，调用的关系对个和组合，考虑可能调用时序 
                   按照可能时序合并，发现可能发生死锁场景
                  3带超时的方法 timed_wait
                   完全可以就不假定该锁就一定会获得，指定超时时间，并未无法获取锁时准备推出逻辑
                    ReentrantLock支持非阻塞的获取锁操作tryLock(timeout,unit) 这是一种插队非公平行为
                  4静态代码分析 findBugs
                    类加载过程中，大量使用自定义类加载时。   
### AQS 原理：
              从原理上，一种同步结构往往是可以利用其他的结构实现的，例如我在专栏第 19 讲中提到过可以使用 Semaphore 实现互斥锁。但是，对某种同步结构的倾向，会导致复杂、晦涩的实现逻辑，所以，他选择了将基础的同步相关操作抽象在 AbstractQueuedSynchronizer 中，利用 AQS 为我们构建同步结构提供了范本。
              AQS 内部数据和方法
                一个 volatile 的整数成员表征状态，同时提供了 setState 和 getState 方法private volatile int state;
                一个先入先出（FIFO）的等待线程队列，以实现多线程间竞争和等待，这是 AQS 机制的核心之一。
                各种基于 CAS 的基础操作方法，以及各种期望具体同步结构去实现的 acquire/release 方法。
                利用 AQS 实现一个同步结构，至少要实现两个基本类型的方法，分别是 acquire 操作，获取资源的独占权；还有就是 release 操作，释放对某个资源的独占
              以 ReentrantLock 为例，它内部通过扩展 AQS 实现了 Sync 类型，以 AQS 的 state 来反映锁的持有情况。  
                private final Sync sync;
                abstract static class Sync extends AbstractQueuedSynchronizer { …}
              下
