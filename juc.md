
# J.U.C
##  并发理论
            可见性、原子性和有序性问题：并发编程Bug的源头
               
               并发程序幕后的故事
                这些年，我们的 CPU、内存、I/O 设备都在不断迭代，不断朝着更快的方向努力。但是，在这个快速发展的过程中，有一个核心矛盾一直存在，就是这三者的速度差异。CPU 和内存的速度差异可以形象地描述为：CPU 是天上一天，内存是地上一年（假设 CPU 执行一条普通指令需要一天，那么 CPU 读写内存得等待一年的时间）。内存和 I/O 设备的速度差异就更大了，内存是天上一天，I/O 设备是地上十年。

                程序里大部分语句都要访问内存，有些还要访问 I/O，根据木桶理论（一只水桶能装多少水取决于它最短的那块木板），程序整体的性能取决于最慢的操作——读写 I/O 设备，也就是说单方面提高 CPU 性能是无效的。为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系结构、操作系统、编译程序都做出了贡献，主要体现为：
                1 CPU 增加了缓存，以均衡与内存的速度差异；
                2操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异；
                3编译程序优化指令执行次序，使得缓存能够得到更加合理地利用

                源头之一：缓存导致的可见性问题
                  在单核时代，所有的线程都是在一颗 CPU 上执行，CPU 缓存与内存的数据一致性容易解决。因为所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来说一定是可见的。
                  一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性。

                  多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存。比如下图中，线程 A 操作的是 CPU-1 上的缓存，而线程 B 操作的是 CPU-2 上的缓存，很明显，这个时候线程 A 对变量 V 的操作对于线程 B 而言就不具备可见性了。这个就属于硬件程序员给软件程序员挖的“坑”
                源头之二：线程切换带来的原子性问题
                   由于 IO 太慢，早期的操作系统就发明了多进程，即便在单核的 CPU 上我们也可以一边听着歌，一边写 Bug，这个就是多进程的功劳。操作系统允许某个进程执行一小段时间，例如 50 毫秒，过了 50 毫秒操作系统就会重新选择一个进程来执行（我们称为“任务切换”），这个 50 毫秒称为“时间片”

                   在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。这里的进程在等待 IO 时之所以会释放 CPU 使用权，是为了让 CPU 在这段等待时间里可以做别的事情，这样一来 CPU 的使用率就上来了；此外，如果这时有另外一个进程也读文件，读文件的操作就会排队，磁盘驱动在完成一个进程的读操作后，发现有排队的任务，就会立即启动下一个读操作，这样 IO 的使用率也上来了。

                   支持多进程分时复用在操作系统的发展史上却具有里程碑意义，Unix 就是因为解决了这个问题而名噪天下的。早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。

                   Java 并发程序都是基于多线程的，自然也会涉及到任务切换，也许你想不到，任务切换竟然也是并发编程里诡异 Bug 的源头之一。任务切换的时机大多数是在时间片结束的时候，我们现在基本都使用高级语言编程，高级语言里一条语句往往需要多条 CPU 指令完成，例如上面代码中的count += 1，至少需要三条 CPU 指令。指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器；指令 2：之后，在寄存器中执行 +1 操作；指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存）


                   操作系统做任务切换，可以发生在任何一条 CPU 指令执行完，是的，是 CPU 指令，而不是高级语言里的一条语句。对于上面的三条指令来说，我们假设 count=0，如果线程 A 在指令 1 执行完后做线程切换，线程 A 和线程 B 按照下图的序列执行，那么我们会发现两个线程都执行了 count+=1 的操作，但是得到的结果不是我们期望的 2，而是 1


                   我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性。CPU 能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符，这是违背我们直觉的地方。因此，很多时候我们需要在高级语言层面保证操作的原子性。
                源头之三：编译优化带来的有序性问题   
                  顾名思义，有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会改变程序中语句的先后顺序，例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”，在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果。不过有时候编译器及解释器的优化可能导致意想不到的 Bug。
                  双重检查问题
                    实际上这个 getInstance() 方法并不完美。问题出在哪里呢？出在 new 操作上，我们以为的 new 操作应该是：
                    分配一块内存 M；
                    在内存 M 上初始化 Singleton 对象；
                    然后 M 的地址赋值给 instance 变量。
                    但是实际上优化后的执行路径却是这样的：
                    分配一块内存 M；
                    将 M 的地址赋值给 instance 变量；
                    最后在内存 M 上初始化 Singleton 对象。
                  我们假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance != null ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。
            Java内存模型：看Java如何解决可见性和有序性问题
              什么是 Java 内存模型？

                导致可见性的原因是缓存，导致有序性的原因是编译优化，那解决可见性、有序性最直接的办法就是禁用缓存和编译优化，但是这样问题虽然解决了，我们程序的性能可就堪忧了。合理的方案应该是按需禁用缓存以及编译优化。那么，如何做到“按需禁用”呢？对于并发程序，何时禁用缓存以及编译优化只有程序员知道，那所谓“按需禁用”其实就是指按照程序员的要求来禁用。所以，为了解决可见性和有序性问题，只需要提供给程序员按需禁用缓存和编译优化的方法即可。
                Java 内存模型是个很复杂的规范，可以从不同的视角来解读，站在我们这些程序员的视角，本质上可以理解为，Java 内存模型规范了 JVM 如何提供按需禁用缓存和编译优化的方法。具体来说，这些方法包括 volatile、synchronized 和 final 三个关键字，以及六项 Happens-Before 规则
              使用 volatile 的困惑
                 volatile 关键字并不是 Java 语言的特产，古老的 C 语言里也有，它最原始的意义就是禁用 CPU 缓存。例如，我们声明一个 volatile 变量 volatile int x = 0，它表达的是：告诉编译器，对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入。这个语义看上去相当明确，但是在实际使用的时候却会带来困惑。

                 Java 内存模型在 1.5 版本对 volatile 语义进行了增强。怎么增强的呢？答案是一项 Happens-Before 规则。
              Happens-Before 规则   
                前面一个操作的结果对后续操作是可见的。
                

                Happens-Before 约束了编译器的优化行为，虽允许编译器优化，但是要求编译器优化后一定遵守 Happens-Before 规则。
                1. 程序的顺序性规则 
                  这条规则是指在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作。
                  程序前面对某个变量的修改一定是对后续操作可见的
                2. volatile 变量规则
                  这条规则是指对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作。
                3 传递性
                   这条规则是指如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C。
                4  管程中锁的规则
                   这条规则是指对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。要理解这个规则，就首先要了解“管程指的是什么”。管程是一种通用的同步原语，在 Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现。管程中的锁在 Java 里是隐式实现的，例如下面的代码，在进入同步块之前，会自动加锁，而在代码块执行完会自动释放锁，加锁以及释放锁都是编译器帮我们实现的。
                5  线程 start() 规则 
                   这条是关于线程启动的。它是指主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作。
                   如果线程 A 调用线程 B 的 start() 方法（即在线程 A 中启动线程 B），那么该 start() 操作 Happens-Before 于线程 B 中的任意操作。

                   Thread B = new Thread(()->{
                      // 主线程调用B.start()之前
                     // 所有对共享变量的修改，此处皆可见
                      // 此例中，var==77
                   });
                   // 此处对共享变量var修改
                  var = 77;
                   // 主线程启动子线程
                  B.start();
                6. 线程 join() 规则
                     这条是关于线程等待的。它是指主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。当然所谓的“看到”，指的是对共享变量的操作

                     换句话说就是，如果在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回。具体可参考下面示例代码。

                     Thread B = new Thread(()->{  // 此处对共享变量var修改  var = 66;});// 例如此处对共享变量修改，// 则这个修改结果对线程B可见// 主线程启动子线程B.start();B.join()// 子线程所有对共享变量的修改// 在主线程调用B.join()之后皆可见// 此例中，var==66 
              被我们忽视的 final    

                final 修饰变量时，初衷是告诉编译器：这个变量生而不变，可以可劲儿优化。     
            互斥锁：解决原子性问题     
               那原子性问题到底该如何解决呢？
                 原子性问题的源头是线程切换，如果能够禁用线程切换那不就能解决这个问题了吗？而操作系统做线程切换是依赖 CPU 中断的，所以禁止 CPU 发生中断就能够禁止线程切换。
                 这里我们以 32 位 CPU 上执行 long 型变量的写操作为例来说明这个问题，long 型变量是 64 位，在 32 位 CPU 上执行写操作会被拆分成两次写操作（写高 32 位和写低 32 位，如下图所示）。

                 在单核 CPU 场景下，同一时刻只有一个线程执行，禁止 CPU 中断，意味着操作系统不会重新调度线程，也就是禁止了线程切换，获得 CPU 使用权的线程就可以不间断地执行，所以两次写操作一定是：要么都被执行，要么都没有被执行，具有原子性。

                 但是在多核场景下，同一时刻，有可能有两个线程同时在执行，一个线程执行在 CPU-1 上，一个线程执行在 CPU-2 上，此时禁止 CPU 中断，只能保证 CPU 上的线程连续执行，并不能保证同一时刻只有一个线程执行，如果这两个线程同时写 long 型变量高 32 位的话，那就有可能出现我们开头提及的诡异 Bug 了。
                 
                 同一时刻只有一个线程执行”这个条件非常重要，我们称之为互斥。如果我们能够保证对共享变量的修改是互斥的，那么，无论是单核 CPU 还是多核 CPU，就都能保证原子性了。

               简易锁模型 

                  我们把一段需要互斥执行的代码称为临界区。线程在进入临界区之前，首先尝试加锁 lock()，如果成功，则进入临界区，此时我们称这个线程持有锁；否则呢就等待，直到持有锁的线程解锁；持有锁的线程执行完临界区的代码后，执行解锁 unlock()

               改进后的锁模型

                 锁和锁要保护的资源是有对应关系的，比如你用你家的锁保护你家的东西，我用我家的锁保护我家的东西。在并发编程世界里，锁和资源也应该有这个关系

                 首先，我们要把临界区要保护的资源标注出来，如图中临界区里增加了一个元素：受保护的资源 R；其次，我们要保护资源 R 就得为它创建一把锁 LR；最后，针对这把锁 LR，我们还需在进出临界区时添上加锁操作和解锁操作。另外，在锁 LR 和受保护资源之间，我特地用一条线做了关联，这个关联关系非常重要

               Java 语言提供的锁技术：synchronized

                  锁是一种通用的技术方案，Java 语言提供的 synchronized     关键字，就是锁的一种实现。synchronized 关键字可以用来修饰方法，也可以用来修饰代码块
                  
                  当修饰静态方法的时候，锁定的是当前类的 Class 对象，在上面的例子中就是 Class X；当修饰非静态方法的时候，锁定的是当前实例对象 this
                   // 修饰静态方法  synchronized(X.class) static void bar() {    // 临界区  }
                   // 修饰非静态方法  synchronized(this) void foo() {    // 临界区  }
                  
                  管程，就是我们这里的 synchronized（至于为什么叫管程，我们后面介绍），我们知道 synchronized 修饰的临界区是互斥的，也就是说同一时刻只有一个线程执行临界区的代码；而所谓“对一个锁解锁  Happens-Before 后续对这个锁的加锁”，指的是前一个线程的解锁操作对后一个线程的加锁操作可见，综合 Happens-Before 的传递性原则，我们就能得出前一个线程在临界区修改的共享变量（该操作在解锁之前），对后续进入临界区（该操作在加锁之后）的线程是可见的。

                  按照这个规则，如果多个线程同时执行 addOne() 方法，可见性是可以保证的，也就说如果有 1000 个线程执行 addOne() 方法，最终结果一定是 value 的值增加了 1000


                  get() 方法和 addOne() 方法都需要访问 value 这个受保护的资源，这个资源用 this 这把锁来保护。线程要进入临界区 get() 和 addOne()，必须先获得 this 这把锁，这样 get() 和 addOne() 也是互斥的。

                  这个模型更像现实世界里面球赛门票的管理，一个座位只允许一个人使用，这个座位就是“受保护资源”，球场的入口就是 Java 类里的方法，而门票就是用来保护资源的“锁”，Java 里的检票工作是由 synchronized 解决的。  

               锁和受保护资源的关系

                   受保护资源和锁之间的关联关系是 N:1 的关系  
                   还拿前面球赛门票的管理来类比，就是一个座位，我们只能用一张票来保护，如果多发了重复的票，那就要打架了。现实世界里，我们可以用多把锁来保护同一个资源，但在并发领域是不行的，并发领域的锁和现实世界的锁不是完全匹配的。不过倒是可以用同一把锁来保护多个资源，这个对应到现实世界就是我们所谓的“包场”了。

                    synchronized long get() {    return value;  }  

                        synchronized static void addOne() {    value += 1;  }

                    如果你仔细观察，就会发现改动后的代码是用两个锁保护一个资源。这个受保护的资源就是静态变量 value，两个锁分别是 this 和 SafeCalc.class。我们可以用下面这幅图来形象描述这个关系。由于临界区 get() 和 addOne() 是用两个锁保护的，因此这两个临界区没有互斥关系，临界区 addOne() 对 value 的修改对临界区 get() 也没有可见性保证，这就导致并发问题了。 

               互斥锁，在并发领域的知名度极高，只要有了并发问题，大家首先容易想到的就是加锁，因为大家都知道，加锁能够保证执行临界区代码的互斥性。这样理解虽然正确，但是却不能够指导你真正用好互斥锁。临界区的代码是操作受保护资源的路径，类似于球场的入口，入口一定要检票，也就是要加锁，但不是随便一把锁都能有效。所以必须深入分析锁定的对象和受保护资源的关系，综合考虑受保护资源的访问路径，多方面考量才能用好互斥锁。synchronized 是 Java 在语言层面提供的互斥原语，其实 Java 里面还有很多其他类型的锁，但作为互斥锁，原理都是相通的：锁，一定有一个要锁定的对象，至于这个锁定的对象要保护的资源以及在哪里加锁 / 解锁，就属于设计层面的事情了     

               如何用一把锁保护多个资源 
               可以用一把锁来保护多个资源，但是不能用多把锁来保护一个资源
               当我们要保护多个资源时，首先要区分这些资源是否存在关联关系
               保护没有关联关系的多个资源
                 用一把锁有个问题，就是性能太差，会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的。而我们用两把锁，取款和修改密码是可以并行的。用不同的锁对受保护资源进行精细化管理，能够提升性能。这种锁还有个名字，叫细粒度锁
               保护有关联关系的多个资源
                 // 转账 synchronized void transfer( Account target, int amt){ if (this.balance > amt) { this.balance -= amt; target.balance += amt; } }

                 在这段代码中，临界区内有两个资源，分别是转出账户的余额 this.balance 和转入账户的余额 target.balance，并且用的是一把锁 this，符合我们前面提到的，多个资源可以用一把锁来保护，这看上去完全正确呀。真的是这样吗？可惜，这个方案仅仅是看似正确，为什么呢？


                 问题就出在 this 这把锁上，this 这把锁可以保护自己的余额 this.balance，却保护不了别人的余额 target.balance，就像你不能用自家的锁来保护别人家的资产，也不能用自己的票来保护别人的座位一样。  

               使用锁的正确姿势  
                  这个办法确实能解决问题，但是有点小瑕疵，它要求在创建 Account 对象的时候必须传入同一个对象，如果创建 Account 对象时，传入的 lock 不是同一个对象，那可就惨了，会出现锁自家门来保护他家资产的荒唐事。在真实的项目场景中，创建 Account 对象的代码很可能分散在多个工程中，传入共享的 lock 真的很难。


                  我们需要更好的方案。还真有，就是用 Account.class 作为共享的锁。Account.class 是所有 Account 对象共享的，而且这个对象是 Java 虚拟机在加载 Account 类的时候创建的，所以我们不用担心它的唯一性。使用 Account.class 作为共享的锁，我们就无需在创建 Account 对象时传入了，代码更简单。


                  “原子性”的本质是什么？其实不是不可分割，不可分割只是外在表现，其本质是多个资源间有一致性的要求，操作的中间状态对外不可见。例如，在 32 位的机器上写 long 型变量有中间状态（只写了 64 位中的 32 位），在银行转账的操作中也有中间状态（账户 A 减少了 100，账户 B 还没来得及发生变化）。所以解决原子性问题，是要保证中间状态对外不可见。  
            一不小心就死锁了，怎么办？   

              // 锁定转出账户    synchronized(this) {                    // 锁定转入账户      synchronized(target) {                   if (this.balance > amt) {          this.balance -= amt;          target.balance += amt;        }      }
              没有免费的午餐
                 细粒度锁。使用细粒度锁可以提高并行度，是性能优化的一个重要手段
                 的确，使用细粒度锁是有代价的，这个代价就是可能会导致死锁
                 死锁的一个比较专业的定义是：一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象。

                 关于这种现象，我们还可以借助资源分配图来可视化锁的占用情况（资源分配图是个有向图，它可以描述资源和线程的状态）。其中，资源用方形节点表示，线程用圆形节点表示；资源中的点指向线程的边表示线程已经获得该资源，线程指向资源的边则表示线程请求资源，但尚未得到。转账发生死锁时的资源分配图就如下图所示，一个“各据山头死等”的尴尬局面。
              如何预防死锁
                  死锁发生的四个条件
                    1互斥，共享资源 X 和 Y 只能被一个线程占用；
                    2占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X；
                    3不可抢占，其他线程不能强行抢占线程 T1 占有的资源；
                    4循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待

                  如何破坏死锁
                    1对于“占用且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了。
                    2对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。
                    3对于“循环等待”这个条件，可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后自然就不存在循环了。


                    1. 破坏占用且等待条件从理论上讲，要破坏这个条件，可以一次性申请所有资源。在现实世界里，就拿前面我们提到的转账操作来讲，它需要的资源有两个，一个是转出账户，另一个是转入账户，当这两个账户同时被申请时，我们该怎么解决这个问题呢？可以增加一个账本管理员，然后只允许账本管理员从文件架上拿账本，也就是说柜员不能直接在文件架上拿账本，必须通过账本管理员才能拿到想要的账本。例如，张三同时申请账本 A 和 B，账本管理员如果发现文件架上只有账本 A，这个时候账本管理员是不会把账本 A 拿下来给张三的，只有账本 A 和 B 都在的时候才会给张三。这样就保证了“一次性申请所有资源”。

                      // 一次性申请转出账户和转入账户，直到成功    while(!actr.apply(this, target))      ；
                    2. 破坏不可抢占条件破坏不可抢占条件看上去很简单，核心是要能够主动释放它占有的资源，这一点 synchronized 是做不到的。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。  

                    3. 破坏循环等待条件破坏这个条件，需要对资源进行排序，然后按序申请资源。这个实现非常简单，我们假设每个账户都有不同的属性 id，这个 id 可以作为排序字段，申请的时候，我们可以按照从小到大的顺序来申请。比如下面代码中，①~⑥处的代码对转出账户（this）和转入账户（target）排序，然后按照序号从小到大的顺序锁定账户。这样就不存在“循环”等待了。
                     if (this.id > target.id) { ③      left = target;           ④      right = this;            ⑤    }                          ⑥    // 锁定序号小的账户    synchronized(left){      // 锁定序号大的账户 
            用“等待-通知”机制优化循环等待
               在破坏占用且等待条件的时候，如果转出账本和转入账本不满足同时在文件架上这个条件，就用死循环的方式来循环等待，核心代码如下：
                // 一次性申请转出账户和转入账户，直到成功while(!actr.apply(this, target))  ；
               如果 apply() 操作耗时非常短，而且并发冲突量也不大时，这个方案还挺不错的，因为这种场景下，循环上几次或者几十次就能一次性获取转出账户和转入账户了。但是如果 apply() 操作耗时长，或者并发冲突量大的时候，循环等待这种方案就不适用了，因为在这种场景下，可能要循环上万次才能获取到锁，太消耗 CPU 了。

               其实在这种场景下，最好的方案应该是：如果线程要求的条件（转出账本和转入账本同在文件架上）不满足，则线程阻塞自己，进入等待状态；当线程要求的条件（转出账本和转入账本同在文件架上）满足后，通知等待的线程重新执行。其中，使用线程阻塞的方式就能避免循环等待消耗 CPU 的问题。

               完美的就医流程
                 1患者先去挂号，然后到就诊门口分诊，等待叫号；
                 2当叫到自己的号时，患者就可以找大夫就诊了；
                 3就诊过程中，大夫可能会让患者去做检查，同时叫下一位患者；
                 4当患者做完检查后，拿检测报告重新分诊，等待叫号；
                 5当大夫再次叫到自己的号时，患者再去找大夫就诊。
                 一个完整的等待 - 通知机制：线程首先获取互斥锁，当线程要求的条件不满足时，释放互斥锁，进入等待状态；当要求的条件满足时，通知等待的线程，重新获取互斥锁。
               用 synchronized 实现等待 - 通知机制
                  在 Java 语言里，等待 - 通知机制可以有多种实现方式，比如 Java 语言内置的 synchronized 配合 wait()、notify()、notifyAll() 这三个方法就能轻松实现

                  如何用 synchronized 实现互斥锁，你应该已经很熟悉了。在下面这个图里，左边有一个等待队列，同一时刻，只允许一个线程进入 synchronized 保护的临界区（这个临界区可以看作大夫的诊室），当有一个线程进入临界区后，其他线程就只能进入图中左边的等待队列里等待（相当于患者分诊等待）。这个等待队列和互斥锁是一对一的关系，每个互斥锁都有自己独立的等待队列。

                  在并发程序中，当一个线程进入临界区后，由于某些条件不满足，需要进入等待状态，Java 对象的 wait() 方法就能够满足这种需求。如上图所示，当调用 wait() 方法后，当前线程就会被阻塞，并且进入到右边的等待队列中，这个等待队列也是互斥锁的等待队列。 线程在进入等待队列的同时，会释放持有的互斥锁，线程释放锁后，其他线程就有机会获得锁，并进入临界区了。

                  那线程要求的条件满足时，该怎么通知这个等待的线程呢？很简单，就是 Java 对象的 notify() 和 notifyAll() 方法。我在下面这个图里为你大致描述了这个过程，当条件满足时调用 notify()，会通知等待队列（互斥锁的等待队列）中的线程，告诉它条件曾经满足过。

                  为什么说是曾经满足过呢？因为 notify() 只能保证在通知时间点，条件是满足的。而被通知线程的执行时间点和通知的时间点基本上不会重合，所以当线程执行的时候，很可能条件已经不满足了（保不齐有其他线程插队）。这一点你需要格外注意。


                   上面我们一直强调 wait()、notify()、notifyAll() 方法操作的等待队列是互斥锁的等待队列，所以如果 synchronized 锁定的是 this，那么对应的一定是 this.wait()、this.notify()、this.notifyAll()；如果 synchronized 锁定的是 target，那么对应的一定是 target.wait()、target.notify()、target.notifyAll() 。而且 wait()、notify()、notifyAll() 这三个方法能够被调用的前提是已经获取了相应的互斥锁，所以我们会发现 wait()、notify()、notifyAll() 都是在 synchronized{}内部被调用的。如果在 synchronized{}外部调用，或者锁定的 this，而用 target.wait() 调用的话，JVM 会抛出一个运行时异常：java.lang.IllegalMonitorStateException。 
               小试牛刀：一个更好地资源分配器
                  1互斥锁：上一篇文章我们提到 Allocator 需要是单例的，所以我们可以用 this 作为互斥锁。
                  2线程要求的条件：转出账户和转入账户都没有被分配过。
                  3何时等待：线程要求的条件不满足就等待。
                  4何时通知：当有线程释放账户时就通知。   

                    // 经典写法    while(als.contains(from) ||         als.contains(to)){      try{        wait();


                     // 归还资源  synchronized void free(    Object from, Object to){    als.remove(from);    als.remove(to);    notifyAll();  

                   尽量使用 notifyAll()
                      notify() 是会随机地通知等待队列中的一个线程，而 notifyAll() 会通知等待队列中的所有线程。从感觉上来讲，应该是 notify() 更好一些，因为即便通知所有线程，也只有一个线程能够进入临界区。但那所谓的感觉往往都蕴藏着风险，实际上使用 notify() 也很有风险，它的风险在于可能导致某些线程永远不会被通知到  
            安全性、活跃性以及性能问题          
              
              安全性问题
    
               那什么是线程安全呢？其实本质上就是正确性，而正确性的含义就是程序按照我们期望的执行，不要让我们感到意外。
               理论上线程安全的程序，就要避免出现原子性问题、可见性问题和有序性问题。
               存在共享数据并且该数据会发生变化，通俗地讲就是有多个线程会同时读写同一数据。
               那如果能够做到不共享数据或者数据状态不发生变化，不就能够保证线程的安全性了嘛。有不少技术方案都是基于这个理论的，例如线程本地存储（Thread Local Storage，TLS）、不变模式等等，

               竞态条件（Race Condition）。所谓竞态条件，指的是程序的执行结果依赖线程执行的顺序。例如上面的例子，如果两个线程完全同时执行，那么结果是 1；如果两个线程是前后执行，那么结果就是 2。在并发环境里，线程的执行顺序是不确定的，如果程序存在竞态条件问题，那就意味着程序执行的结果是不确定的，而执行结果不确定这可是个大 Bug。


               竞态条件。在并发场景中，程序的执行依赖于某个状态变量，也就是类似于下面这样：
               if (状态变量 满足 执行条件) {  执行操作}
               那面对数据竞争和竞态条件问题，又该如何保证线程的安全性呢？其实这两类问题，都可以用互斥这个技术方案，而实现互斥的方案有很多，CPU 提供了相关的互斥指令，操作系统、编程语言也会提供相关的 API。从逻辑上来看，我们可以统一归为：锁  

              活跃性问题 

               所谓活跃性问题，指的是某个操作无法执行下去。我们常见的“死锁”就是一种典型的活跃性问题，当然除了死锁外，还有两种情况，分别是“活锁”和“饥饿”  

               有时线程虽然没有发生阻塞，但仍然会存在执行不下去的情况，这就是所谓的“活锁”
               解决“活锁”的方案很简单，谦让时，尝试等待一个随机的时间就可以了。Raft 这样知名的分布式一致性算法中

               那“饥饿”该怎么去理解呢？所谓“饥饿”指的是线程因无法访问所需资源而无法执行下去的情况。“不患寡，而患不均”，如果线程优先级“不均”，在 CPU 繁忙的情况下，优先级低的线程得到执行的机会很小，就可能发生线程“饥饿”；持有锁的线程，如果执行的时间过长，也可能导致“饥饿”问题。

               解决“饥饿”问题的方案很简单，有三种方案：一是保证资源充足，二是公平地分配资源，三就是避免持有锁  的线程长时间执行。这三个方案中，方案一和方案三的适用场景比较有限，因为很多场景下，资源的稀缺性是没办法解决的，持有锁的线程执行的时间也很难缩短。倒是方案二的适用场景相对来说更多一些

               在并发编程里，主要是使用公平锁。所谓公平锁，是一种先来后到的方案，线程的等待是有顺序的，排在等待 队列前面的线程会优先获得资源。   

              性能问题  
                第一，既然使用锁会带来性能问题，那最好的方案自然就是使用无锁的算法和数据结构了。在这方面有很多相关的技术，例如线程本地存储 (Thread Local Storage, TLS)、写入时复制 (Copy-on-write)、乐观锁等；Java 并发包里面的原子类也是一种无锁的数据结构；Disruptor 则是一个无锁的内存队列，性能都非常好……第二，减少锁持有的时间。互斥锁本质上是将并行的程序串行化，所以要增加并行度，一定要减少持有锁的时间。这个方案具体的实现技术也有很多，例如使用细粒度的锁，一个典型的例子就是 Java 并发包里的 ConcurrentHashMap，它使用了所谓分段锁的技术（这个技术后面我们会详细介绍）；还可以使用读写锁，也就是读是无锁的，只有写的时候才会互斥。

                性能方面的度量指标有很多，我觉得有三个指标非常重要，就是：
                1吞吐量、延迟和并发量。吞吐量：指的是单位时间内能处理的请求数量。吞吐量越高，说明性能越好。
                2延迟：指的是从发出请求到收到响应的时间。延迟越小，说明性能越好。
                3并发量：指的是能同时处理的请求数量，一般来说随着并发量的增加、延迟也会增加。所以延迟这个指标，一般都会是基于并发量来说的。例如并发量是 1000 的时候，延迟是 50 毫秒。

              并发编程是一个复杂的技术领域，微观上涉及到原子性问题、可见性问题和有序性问题，宏观则表现为安全性、活跃性以及性能问题。我们在设计并发程序的时候，主要是从宏观出发，也就是要重点关注它的安全性、活跃性以及性能。安全性方面要注意数据竞争和竞态条件，活跃性方面需要注意死锁、活锁、饥饿等问题，性能方面我们虽然介绍了两个方案，但是遇到具体问题，你还是要具体分析，根据特定的场景选择合适的数据结构和算法。    
            管程：并发编程的万能钥匙
               Java 采用的是管程技术，synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法都是管程的组成部分。而管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程。但是管程更容易使用，所以 Java 选择了管程。
              
               管程，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发。翻译为 Java 领域的语言，就是管理类的成员变量和成员方法，让这个类是线程安全的。
                
               MESA 模型
                 在管程的发展史上，先后出现过三种不同的管程模型，分别是：Hasen 模型、Hoare 模型和 MESA 模型。其中，现在广泛应用的是 MESA 模型，并且 Java 管程的实现参考的也是 MESA 模型。所以今天我们重点介绍一下 MESA 模型。
                
                 在并发编程领域，有两大核心问题：一个是互斥，即同一时刻只允许一个线程访问共享资源；另一个是同步，即线程之间如何通信、协作。这两大问题，管程都是能够解决的。

                 管程如何让解决互斥 

                   管程解决互斥问题的思路很简单，就是将共享变量及其对共享变量的操作统一封装起来。在下图中，管程 X 将共享变量 queue 这个队列和相关的操作入队 enq()、出队 deq() 都封装起来了；线程 A 和线程 B 如果想访问共享变量 queue，只能通过调用管程提供的 enq()、deq() 方法来实现；enq()、deq() 保证互斥性，只允许一个线程进入管程。不知你有没有发现，管程模型和面向对象高度契合的。估计这也是 Java 选择管程的原因吧。而我在前面章节介绍的互斥锁用法，其背后的模型其实就是它。

                 管程如何解决同步

                    在管程模型里，共享变量和对共享变量的操作是被封装起来的，图中最外层的框就代表封装的意思。框的 上面只有一个入口，并且在入口旁边还有一个入口等待队列。当多个线程同时试图进入管程内部时，只允许一个线程进入，其他线程则在入口等待队列中等待。这个过程类似就医流程的分诊，只允许一个患者就诊，其他患者都在门口等待。

                    管程里还引入了条件变量的概念，而且每个条件变量都对应有一个等待队列，如下图，条件变量 A 和条件变量 B 分别都有自己的等待队列。


                    假设有个线程 T1  执行出队操作，不过需要注意的是执行出队操作，有个前提条件，就是队列不能是空的  ，而队列不空这个前提条件就是管程里的条件变量。 如果线程 T1 进入管程后恰好发现队列是空的，那怎么办呢？等待啊，去哪里等呢？就去条件变量对应的等待队列里面等。此时线程 T1 就去“队列不空”这个条件变量的等待队列中等待。这个过程类似于大夫发现你要去验个血，于是给你开了个验血的单子，你呢就去验血的队伍里排队。线程 T1 进入条件变量的等待队列后，是允许其他线程进入管程的。这和你去验血的时候，医生可以给其他患者诊治，道理都是一样的。再假设之后另外一个线程 T2 执行入队操作，入队操作执行成功之后，“队列不空”这个条件对于线程 T1 来说已经满足了，此时线程 T2 要通知 T1，告诉它需要的条件已经满足了。当线程 T1 得到通知后，会从等待队列里面出来，但是出来之后不是马上执行，而是重新进入到入口等待队列里面。这个过程类似你验血完，回来找大夫，需要重新分诊。

                 我们用了 Java 并发包里面的 Lock 和 Condition，如果你看着吃力，也没关系，后面我们还会详细介绍，这个例子只是先让你明白条件变量及其等待队列是怎么回事。需要注意的是：await() 和前面我们提到的 wait() 语义是一样的；signal() 和前面我们提到的 notify() 语义是一样的

                 wait() 的正确姿势

                   Hasen 模型、Hoare 模型和 MESA 模型的一个核心区别就是当条件满足后，如何通知相关线程。管程要求同一时刻只允许一个线程执行，那当线程 T2 的操作使线程 T1 等待的条件满足时，T1 和 T2 究竟谁可以执行呢？
                   1Hasen 模型里面，要求 notify() 放在代码的最后，这样 T2 通知完 T1 后，T2 就结束了，然后 T1 再执行，这样就能保证同一时刻只有一个线程执行。
                   2Hoare 模型里面，T2 通知完 T1 后，T2 阻塞，T1 马上执行；等 T1 执行完，再唤醒 T2，也能保证同一时刻只有一个线程执行。但是相比 Hasen 模型，T2 多了一次阻塞唤醒操作。
                   3MESA 管程里面，T2 通知完 T1 后，T2 还是会接着执行，T1 并不立即执行，仅仅是从条件变量的等待队列进到入口等待队列里面。这样做的好处是 notify() 不用放到代码的最后，T2 也没有多余的阻塞唤醒操作。但是也有个副作用，就是当 T1 再次执行的时候，可能曾经满足的条件，现在已经不满足了，所以需要以循环方式检验条件变量。  

                 notify() 何时可以使用 

                   还有一个需要注意的地方，就是 notify() 和 notifyAll() 的使用，前面章节，我曾经介绍过，除非经过深思熟虑，否则尽量使用 notifyAll()。那什么时候可以使用 notify() 呢？需要满足以下三个条件：
                   1所有等待线程拥有相同的等待条件；
                   2所有等待线程被唤醒后，执行相同的操作；
                   3只需要唤醒一个线程  
            java 线程
              Java线程的生命周期
  

                Java 语言里的线程本质上就是操作系统的线程，它们是一一对应的。 
                通用的线程生命周期 

               初始状态、可运行状态、运行状态、休眠状态和终止状态。

               1初始状态，指的是线程已经被创建，但是还不允许分配 CPU 执行。这个状态属于编程语言特有的，不过这里所谓的被创建，仅仅是在编程语言层面被创建，而在操作系统层面，真正的线程还没有创建。
               2可运行状态，指的是线程可以分配 CPU 执行。在这种状态下，真正的操作系统线程已经被成功创建了，所以可以分配 CPU 执行。
               3当有空闲的 CPU 时，操作系统会将其分配给一个处于可运行状态的线程，被分配到 CPU 的线程的状态就转换成了运行状态。
               4运行状态的线程如果调用一个阻塞的 API（例如以阻塞方式读文件）或者等待某个事件（例如条件变量），那么线程的状态就会转换到休眠状态，同时释放 CPU 使用权，休眠状态的线程永远没有机会获得 CPU 使用权。当等待的事件出现了，线程就会从休眠状态转换到可运行状态。
               5线程执行完或者出现异常就会进入终止状态，终止状态的线程不会切换到其他任何状态，进入终止状态也就意味着线程的生命周期结束了。   
               这五种状态在不同编程语言里会有简化合并。例如，C 语言的 POSIX Threads 规范，就把初始状态和可运行状态合并了；Java 语言里则把可运行状态和运行状态合并了，这两个状态在操作系统调度层面有用，而 JVM 层面不关心这两个状态，因为 JVM 把线程调度交给操作系统处理了 
              
                Java 中线程的生命周期

                Java 语言中线程共有六种状态，分别是：NEW（初始化状态）RUNNABLE（可运行 / 运行状态）BLOCKED（ 阻塞状态）WAITING（无时限等待）TIMED_WAITING（有时限等待）TERMINATED（终止状态）

                但其实在操作系统层面，Java 线程中的 BLOCKED、WAITING、TIMED_WAITING 是一种状态，即前面我们提到的休眠状态。也就是说只要 Java 线程处于这三种状态之一，那么这个线程就永远没有 CPU 的使用权


                BLOCKED、WAITING、TIMED_WAITING 可以理解为线程导致休眠状态的三种原因。那具体是哪些情形会导致线程从 RUNNABLE 状态转换到这三种状态呢？而这三种状态又是何时转换回 RUNNABLE 的呢？以及 NEW、TERMINATED 和 RUNNABLE 状态是如何转换的？

                1. RUNNABLE 与 BLOCKED 的状态转换

                   只有一种场景会触发这种转换，就是线程等待 synchronized 的隐式锁。synchronized 修饰的方法、代码块同一时刻只允许一个线程执行，其他线程只能等待，这种情况下，等待的线程就会从 RUNNABLE 转换到 BLOCKED 状态。而当等待的线程获得 synchronized 隐式锁时，就又会从 BLOCKED 转换到 RUNNABLE 状态。如果你熟悉操作系统线程的生命周期的话，可能会有个疑问：线程调用阻塞式 API 时，是否会转换到 BLOCKED 状态呢？在操作系统层面，线程是会转换到休眠状态的，但是在 JVM 层面，Java 线程的状态不会发生变化，也就是说 Java 线程的状态会依然保持 RUNNABLE 状态。JVM 层面并不关心操作系统调度相关的状态，因为在 JVM 看来，等待 CPU 使用权（操作系统层面此时处于可执行状态）与等待 I/O（操作系统层面此时处于休眠状态）没有区别，都是在等待某个资源，所以都归入了 RUNNABLE 状态。而我们平时所谓的 Java 在调用阻塞式 API 时，线程会阻塞，指的是操作系统线程的状态，并不是 Java 线程的状态。

                2. RUNNABLE 与 WAITING 的状态转换
                  第一种场景，获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法。其中，wait() 方法我们在上一篇讲解管程的时候已经深入介绍过了，这里就不再赘述。
                  第二种场景，调用无参数的 Thread.join() 方法。其中的 join() 是一种线程同步方法，例如有一个线程对象 thread A，当调用 A.join() 的时候，执行这条语句的线程会等待 thread A 执行完，而等待中的这个线程，其状态会从 RUNNABLE 转换到 WAITING。当线程 thread A 执行完，原来等待它的线程又会从 WAITING 状态转换到 RUNNABLE。
                  第三种场景，调用 LockSupport.park() 方法。其中的 LockSupport 对象，也许你有点陌生，其实 Java 并发包中的锁，都是基于它实现的。调用 LockSupport.park() 方法，当前线程会阻塞，线程的状态会从 RUNNABLE 转换到 WAITING。调用 LockSupport.unpark(Thread thread) 可唤醒目标线程，目标线程的状态又会从 WAITING 状态转换到 RUNNABLE

                3. RUNNABLE 与 TIMED_WAITING 的状态转换

                  有五种场景会触发这种转换：
                  调用带超时参数的 Thread.sleep(long millis) 方法；
                   获得 synchronized 隐式锁的线程，调用带超时参数的 Object.wait(long timeout) 方法；
                    调用带超时参数的 Thread.join(long millis) 方法；
                   调用带超时参数的 LockSupport.parkNanos(Object blocker, long deadline) 方法；
                   调用带超时参数的 LockSupport.parkUntil(long deadline) 方法。

                4. 从 NEW 到 RUNNABLE 状态
                  Java 刚创建出来的 Thread 对象就是 NEW 状态，而创建 Thread 对象主要有两种方法。一种是继承 Thread 对象，重写 run() 方法

                  另一种是实现 Runnable 接口，重写 run() 方法，并将该实现类作为创建 Thread 对象的参数。

                   NEW 状态的线程，不会被操作系统调度，因此不会执行。Java 线程要执行，就必须转换到 RUNNABLE 状态。从 NEW 状态转换到 RUNNABLE 状态很简单，只要调用线程对象的 start() 方法就可以了，

               
                5. 从 RUNNABLE 到 TERMINATED

                   状态线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的时候异常抛出，也会导致线程终止。有时候我们需要强制中断 run() 方法的执行，例如 run() 方法访问一个很慢的网络，我们等不下去了，想终止怎么办呢？Java 的 Thread 类里面倒是有个 stop() 方法，不过已经标记为 @Deprecated，所以不建议使用了。正确的姿势其实是调用 interrupt() 方法。

                那 stop() 和 interrupt() 方法的主要区别是什么呢？

                   stop() 方法会真的杀死线程，不给线程喘息的机会，如果线程持有 ReentrantLock 锁，被 stop()  的线程并不会自动调用 ReentrantLock 的 unlock() 去释放锁，那其他线程就再也没机会获得 ReentrantLock 锁，这实在是太危险了。所以该方法就不建议使用了，类似的方法还有 suspend() 和 resume() 方法，这两个方法同样也都不建议使用了，所以这里也就不多介绍了。而 interrupt() 方法就温柔多了，interrupt() 方法仅仅是通知线程，线程有机会执行一些后续操作，同时也可以无视这个通知。被 interrupt 的线程，是怎么收到通知的呢？一种是异常，另一种是主动检测。当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的 interrupt() 方法，会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常。上面我们提到转换到 WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法，我们看这些方法的签名，发现都会 throws InterruptedException 这个异常。这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法
                   当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在 java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 的 java.nio.channels.Selector 会立即返回。上面这两种情况属于被中断的线程通过异常的方式获得了通知。还有一种是主动检测，如果线程处于 RUNNABLE 状态，并且没有阻塞在某个 I/O 操作上，例如中断计算圆周率的线程 A，这时就得依赖线程 A 主动检测中断状态了。如果其他线程调用线程 A 的 interrupt() 方法，那么线程 A 可以通过 isInterrupted() 方法，检测是不是自己被中断了 
              创建多少线程才是合适的  
                为什么要使用多线程？
    
                  使用多线程，本质上就是提升程序性能。不过此刻谈到的性能，可能在你脑海里还是比较笼统的，基本上就是快、快、快，这种无法度量的感性认识很不科学，所以在提升性能之前，首要问题是：如何度量性能。度量性能的指标有很多，但是有两个指标是最核心的，它们就是延迟和吞吐量。延迟指的是发出请求到收到响应这个过程的时间；延迟越短，意味着程序执行得越快，性能也就越好。 吞吐量指的是在单位时间内能处理请求的数量；吞吐量越大，意味着程序能处理的请求越多，性能也就越好。这两个指标内部有一定的联系（同等条件下，延迟越短，吞吐量越大），但是由于它们隶属不同的维度（一个是时间维度，一个是空间维度），并不能互相转换。我们所谓提升性能，从度量的角度，主要是降低延迟，提高吞吐量。这也是我们使用多线程的主要目的。      
                多线程的应用场景
                  
                  要想“降低延迟，提高吞吐量”，对应的方法呢，基本上有两个方向，一个方向是优化算法，另一个方向是将硬件的性能发挥到极致。前者属于算法范畴，后者则是和并发编程息息相关了。那计算机主要有哪些硬件呢？主要是两类：一个是 I/O，一个是 CPU。简言之，在并发编程领域，提升性能本质上就是提升硬件的利用率，再具体点来说，就是提升 I/O 的利用率和 CPU 的利用率。

                  估计这个时候你会有个疑问，操作系统不是已经解决了硬件的利用率问题了吗？的确是这样，例如操作系统已经解决了磁盘和网卡的利用率问题，利用中断机制还能避免 CPU 轮询 I/O 状态，也提升了 CPU 的利用率。但是操作系统解决硬件利用率问题的对象往往是单一的硬件设备，而我们的并发程序，往往需要 CPU 和 I/O 设备相互配合工作，也就是说，我们需要解决 CPU 和 I/O 设备综合利用率的问题。关于这个综合利用率的问题，操作系统虽然没有办法完美解决，但是却给我们提供了方案，那就是：多线程。

                  ，如果只有一个线程，执行 CPU 计算的时候，I/O 设备空闲；执行 I/O 操作的时候，CPU 空闲，所以 CPU 的利用率和 I/O 设备的利用率都是 50%。

                  如果有两个线程，如下图所示，当线程 A 执行 CPU 计算的时候，线程 B 执行 I/O 操作；当线程 A 执行 I/O 操作的时候，线程 B 执行 CPU 计算，这样 CPU 的利用率和 I/O 设备的利用率就都达到了 100%

                  如果 CPU 和 I/O 设备的利用率都很低，那么可以尝试通过增加线程来提高吞吐量。

                  在单核时代，多线程主要就是用来平衡 CPU 和 I/O 设备的。如果程序只有 CPU 计算，而没有 I/O 操作的话，多线程不但不会提升性能，还会使性能变得更差，原因是增加了线程切换的成本。但是在多核时代，这种纯计算型的程序也可以利用多线程来提升性能。为什么呢？因为利用多核可以降低响应时间。
                创建多少线程合适？  
                   创建多少线程合适，要看多线程具体的应用场景。我们的程序一般都是 CPU 计算和 I/O 操作交叉执行的，由于 I/O 设备的速度相对于 CPU 来说都很慢，所以大部分情况下，I/O 操作执行的时间相对于 CPU 计算来说都非常长，这种场景我们一般都称为 I/O 密集型计算；和 I/O 密集型计算相对的就是 CPU 密集型计算了，CPU 密集型计算大部分场景下都是纯 CPU 计算。I/O 密集型程序和 CPU 密集型程序，计算最佳线程数的方法是不同的。

                   对于 CPU 密集型计算，多线程本质上是提升多核 CPU 的利用率，所以对于一个 4 核的 CPU，每个核一个线程，理论上创建 4 个线程就可以了，再多创建线程也只是增加线程切换的成本。所以，对于 CPU 密集型的计算场景，理论上“线程的数量 =CPU 核数”就是最合适的。不过在工程上，线程的数量一般会设置为“CPU 核数 +1”，这样的话，当线程因为偶尔的内存页失效或其他原因导致阻塞时，这个额外的线程可以顶上，从而保证 CPU 的利用率

                   对于 I/O 密集型的计算场景，比如前面我们的例子中，如果 CPU 计算和 I/O 操作的耗时是 1:1，那么 2 个线程是最合适的。如果 CPU 计算和 I/O 操作的耗时是 1:2，那多少个线程合适呢？是 3 个线程，如下图所示：CPU 在 A、B、C 三个线程之间切换，对于线程 A，当 CPU 从 B、C 切换回来时，线程 A 正好执行完 I/O 操作。这样 CPU 和 I/O 设备的利用率都达到了 100%。

                   通过上面这个例子，我们会发现，对于 I/O 密集型计算场景，最佳的线程数是与程序中 CPU 计算和 I/O 操作的耗时比相关的，我们可以总结出这样一个公式：最佳线程数 =1 +（I/O 耗时 / CPU 耗时）我们令 R=I/O 耗时 / CPU 耗时，综合上图，可以这样理解：当线程 A 执行 IO 操作时，另外 R 个线程正好执行完各自的 CPU 计算。这样 CPU 的利用率就达到了 100%。不过上面这个公式是针对单核 CPU 的，至于多核 CPU，也很简单，只需要等比扩大就可以了，计算公式如下：最佳线程数 =CPU 核数 * [ 1 +（I/O 耗时 / CPU 耗时）]   
                为什么局部变量是线程安全的？  

                   方法是如何被执行的

                     CPU 去哪里找到调用方法的参数和返回地址？”如果你熟悉 CPU 的工作原理，你应该会立刻想到：通过 CPU 的堆栈寄存器。CPU 支持一种栈结构，栈你一定很熟悉了，就像手枪的弹夹，先入后出。因为这个栈是和方法调用相关的，因此经常被称为调用栈。

                     有三个方法 A、B、C，他们的调用关系是 A->B->C（A 调用 B，B 调用 C），在运行时，会构建出下 面这样的调用栈。每个方法在调用栈里都有自己的独立空间，称为栈帧，每个栈帧里都有对应方法需要的参数和返回地址。当调用方法时，会创建新的栈帧，并压入调用栈；当方法返回时，对应的栈帧就会被自动弹出。也就是说，栈帧和方法是同生共死的。


                     利用栈结构来支持方法调用这个方案非常普遍，以至于 CPU 里内置了栈寄存器。虽然各家编程语言定义的方法千奇百怪，但是方法的内部执行原理却是出奇的一致：都是靠栈结构解决的。Java 语言虽然是靠虚拟机解释执行的，但是方法的调用也是利用栈结构解决的      

                   局部变量存哪里？

                     局部变量的作用域是方法内部，也就是说当方法执行完，局部变量就没用了，局部变量应该和方法同生共死。此时你应该会想到调用栈的栈帧，调用栈的栈帧就是和方法同生共死的，所以局部变量放到调用栈里那儿是相当的合理。事实上，的确是这样的，局部变量就是放到了调用栈里。于是调用栈的结构就变成了下图这样

                     局部变量是和方法同生共死的，一个变量如果想跨越方法的边界，就必须创建在堆里。    

                   调用栈与线程

                     两个线程可以同时用不同的参数调用相同的方法，那调用栈和线程之间是什么关系呢？答案是：每个线程都有自己独立的调用栈。

                     因为如果不是这样，那两个线程就互相干扰了。如下面这幅图所示，线程 A、B、C 每个线程都有自己独立的调用栈。

                     现在，让我们回过头来再看篇首的问题：Java 方法里面的局部变量是否存在并发问题？现在你应该很清楚了，一点问题都没有。因为每个线程都有自己的调用栈，局部变量保存在线程各自的调用栈里面，不会共享，所以自然也就没有并发问题。再次重申一遍：没有共享，就没有伤害。    

                   线程封闭
                     线程封闭，比较官方的解释是：仅在单线程内访问数据。由于不存在共享，所以即便不同步也不会有并发问题，性能杠杠的。  

                     例如从数据库连接池里获取的连接 Connection，在 JDBC 规范里并没有要求这个 Connection 必须是线程安全的。数据库连接池通过线程封闭技术，保证一个 Connection 一旦被一个线程获取之后，在这个线程关闭 Connection 之前的这段时间里，不会再分配给其他线程，从而保证了 Connection 不会有并发问题。   
            如何用面向对象思想写好并发程序？
              在 Java 语言里，面向对象思想能够让并发编程变得更简单。
              可以从封装共享变量、识别共享变量间的约束条件和制定并发访问策略这三个方面下手。
              一、封装共享变量
                编程领域里面对于共享变量的访问路径就类似于球场的入口，必须严格控制。好在有了面向对象思想，对共享变量的访问路径可以轻松把控。

                面向对象思想里面有一个很重要的特性是封装，封装的通俗解释就是将属性和实现细节封装在对象内部，外界对象只能通过目标对象提供的公共方法来间接访问这些内部属性，这和门票管理模型匹配度相当的高，球场里的座位就是对象属性，球场入口就是对象的公共方法。我们把共享变量作为对象的属性，那对于共享变量的访问路径就是对象的公共方法，所有入口都要安排检票程序就相当于我们前面提到的并发访问策略。

                利用面向对象思想写并发程序的思路，其实就这么简单：将共享变量作为对象属性封装在内部，对所有公共方法制定并发访问策略

                利用面向对象思想写并发程序的思路，其实就这么简单：将共享变量作为对象属性封装在内部，对所有公共方法制定并发访问策略。就拿很多统计程序都要用到计数器来说，下面的计数器程序共享变量只有一个，就是 value，我们把它作为 Counter 类的属性，并且将两个公共方法 get() 和 addOne() 声明为同步方法，这样 Counter 类就成为一个线程安全的类了。

                很多共享变量的值是不会变的，例如信用卡账户的卡号、姓名、身份证。对于这些不会发生变化的共享变量，建议你用 final 关键字来修饰。这样既能避免并发问题，也能很明了地表明你的设计意图，让后面接手你程序的兄弟知道，你已经考虑过这些共享变量的并发安全问题了。
              二、识别共享变量间的约束条件  
                  在没有识别出库存下限要小于库存上限这个约束条件之前，我们制定的并发访问策略是利用原子类，但是这个策略，完全不能保证库存下限要小于库存上限这个约束条件。所以说，在设计阶段，我们一定要识别出所有共享变量之间的约束条件，如果约束条件识别不足，很可能导致制定的并发访问策略南辕北辙。
                  共享变量之间的约束条件，反映在代码里，基本上都会有 if 语句，所以，一定要特别注意竞态条件。
              三、制定并发访问策略  
                 1避免共享：避免共享的技术主要是利于线程本地存储以及为每个任务分配独立的线程。
                 2不变模式：这个在 Java 领域应用的很少，但在其他领域却有着广泛的应用，例如 Actor 模式、CSP 模式以及函数式编程的基础都是不变模式。
                 3管程及其他同步工具：Java 领域万能的解决方案是管程，但是对于很多特定场景，使用 Java 并发包提供的读写锁、并发容器等同步工具会更好  

                 三个原则
        -- 并发工具 
            --  Lock和Condition

                在并发编程领域，有两大核心问题：一个是互斥，即同一时刻只允许一个线程访问共享资源；另一个是同步，即线程之间如何通信、协作。这两大问题，管程都是能够解决的。Java SDK 并发包通过 Lock 和 Condition 两个接口来实现管程，其中 Lock 用于解决互斥问题，Condition 用于解决同步问题。

                再造管程的理由
                  我们前面在介绍死锁问题的时候，提出了一个破坏不可抢占条件方案，但是这个方案 synchronized 没有办法解决。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。但我们希望的是：
                  对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。

                  如果我们重新设计一把互斥锁去解决这个问题，那该怎么设计呢？我觉得有三种方案。
                  1 能够响应中断。synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。这样就破坏了不可抢占条件了。
                  2支持超时。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。
                  3非阻塞地获取锁。如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。      


                  这三种方案可以全面弥补 synchronized 的问题。到这里相信你应该也能理解了，这三个方案就是“重复造轮子”的主要原因，体现在 API 上，就是 Lock 接口的三个方法。详情如下：

                  // 支持中断的API
                  void lockInterruptibly()   throws InterruptedException;
                  // 支持超时的API
                  boolean tryLock(long time, TimeUnit unit)   throws InterruptedException;
                  // 支持非阻塞获取锁的
                  APIboolean tryLock(); 
                如何保证可见性  
                  你已经知道 Java 里多线程的可见性是通过 Happens-Before 规则保证的，而 synchronized 之所以能够保证可见性，也是因为有一条 synchronized 相关的规则：synchronized 的解锁 Happens-Before 于后续对这个锁的加锁。
                  那 Java SDK 里面 Lock 靠什么保证可见性呢？例如在下面的代码中，线程 T1 对 value 进行了 +=1 操作，那后续的线程 T2 能够看到 value 的正确结果吗？
                  它是利用了 volatile 相关的 Happens-Before 规则
                  。Java SDK 里面的 ReentrantLock，内部持有一个 volatile 的成员变量    state，获取锁的时候，会读写 state 的值；解锁的时候，也会读写 state 的值（简化后的代码如下面所示）。也就是说，在执行 value+=1 之前，程序先读写了一次 volatile 变量 state，在执行 value+=1 之后，又读写了一次 volatile 变量 state。根据相关的 Happens-Before 规则：
                  1顺序性规则：对于线程 T1，value+=1 Happens-Before 释放锁的操作 unlock()；
                  2volatile 变量规则：由于 state = 1 会先读取 state，所以线程 T1 的 unlock() 操作 Happens-Before 线程 T2 的 lock() 操作；
                  3传递性规则：线程 T1 的 value+=1  Happens-Before 线程 T2 的 lock() 操作。 
                什么是可重入锁  

                   所谓可重入锁，顾名思义，指的是线程可以重复获取同一把锁。例如下面代码中，当线程 T1 执行到 ① 处时，已经获取到了锁 rtl ，当在 ① 处调用 get() 方法时，会在 ② 再次对锁 rtl 执行加锁操作。此时，如果锁 rtl 是可重入的，那么线程 T1 可以再次加锁成功；如果锁 rtl 是不可重入的，那么线程 T1 此时会被阻塞。     

                   除了可重入锁，可能你还听说过可重入函数，可重入函数怎么理解呢？指的是线程可以重复调用？显然不是，所谓可重入函数，指的是多个线程可以同时调用该函数，每个线程都能得到正确结果；同时在一个线程内支持线程切换，无论被切换多少次，结果都是正确的。多线程可以同时执行，还支持线程切换，这意味着什么呢？线程安全啊。所以，可重入函数是线程安全的。 
                公平锁与非公平锁  
                
                  在使用 ReentrantLock 的时候，你会发现 ReentrantLock 这个类有两个构造函数，一个是无参构造函数，一个是传入 fair 参数的构造函数。fair 参数代表的是锁的公平策略，如果传入 true 就表示需要构造一个公平锁，反之则表示要构造一个非公平锁。  

                  我们介绍过入口等待队列，锁都对应着一个等待队列，如果一个线程没有获得锁，就会进入等待队列，当有线程释放锁的时候，就需要从等待队列中唤醒一个等待的线程。如果是公平锁，唤醒的策略就是谁等待的时间长，就唤醒谁，很公平；如果是非公平锁，则不提供这个公平保证，有可能等待时间短的线程反而先被唤醒。  
                用锁的最佳实践   
                   1永远只在更新对象的成员变量时加锁
                   2永远只在访问可变的成员变量时加锁
                   3永远不在调用其他对象的方法时加锁
                
                们提到过 Java 语言内置的管程里只有一个条件变量，而 Lock&Condition 实现的管程是支持多个条件变量的，这是二者的一个重要区别。在很多并发场景下，支持多个条件变量能够让我们的并发程序可读性更好，实现起来也更容易。例如，实现一个阻塞队列，就需要两个条件变量。

                那如何利用两个条件变量快速实现阻塞队列呢？

                   一个阻塞队列，需要两个条件变量，一个是队列不空（空队列不允许出队），另一个是队列不满（队列已满不允许入队）

                   Lock 和 Condition 实现的管程，线程等待和通知需要调用 await()、signal()、signalAll()，它们的语义和 wait()、notify()、notifyAll() 是相同的。但是不一样的是，Lock&Condition 实现的管程里只能使用前面的 await()、signal()、signalAll()，而后面的 wait()、notify()、notifyAll() 只有在 synchronized 实现的管程里才能使用。如果一不小心在 Lock&Condition 实现的管程里调用了 wait()、notify()、notifyAll()，那程序可就彻底玩儿完了。   
                同步和异步 

                  通俗点来讲就是调用方是否需要等待结果，如果需要等待结果，就是同步；如果不需要等待结果，就是异步。
                  同步，是 Java 代码默认的处理方式。如果你想让你的程序支持异步，可以通过下面两种方式来实现：1调用方创建一个子线程，在子线程中执行方法调用，这种调用我们称为异步调用；
                  2方法实现的时候，创建一个新的线程执行主要逻辑，主线程直接 return，这种方法我们一般称为异步方法  
                Dubbo 源码分析  
                  其实在编程领域，异步的场景还是挺多的，比如 TCP 协议本身就是异步的，我们工作中经常用到的 RPC 调用，在 TCP 协议层面，发送完 RPC 请求后，线程是不会等待 RPC 的响应结果的。
                  其实很简单，一定是有人帮你做了异步转同步的事情。例如目前知名的 RPC 框架 Dubbo 就给我们做了异步转同步的事情，

                  当 RPC 返回结果之前，阻塞调用线程，让调用线程等待；当 RPC 返回结果后，唤醒调用线程，让调用线程重新执行。不知道你有没有似曾相识的感觉，这不就是经典的等待 - 通知机制吗？

                  调用线程通过调用 get() 方法等待 RPC 返回结果，这个方法里面，你看到的都是熟悉的“面孔”：调用 lock() 获取锁，在 finally 里面调用 unlock() 释放锁；获取锁后，通过经典的在循环中调用 await() 方法来实现等待。当 RPC 结果返回时，会调用 doReceived() 方法，这个方法里面，调用 lock() 获取锁，在 finally 里面调用 unlock() 释放锁，获取锁后通过调用 signal() 来通知调用线程，结果已经返回，不用继续等待了。  
            --  Semaphore：如何快速实现一个限流器     
                 在编程世界里，线程能不能执行，也要看信号量是不是允许。
                 信号量模型 
                   一个计数器，一个等待队列，三个方法
                   在信号量模型里，计数器和等待队列对外是透明的，所以只能通过信号量模型提供的三个方法来访问它们，这三个方法分别是：init()、down() 和 up()。你可以结合下图来形象化地理解。

                   这三个方法详细的语义具体如下所示。
                   init()：设置计数器的初始值。
                   own()：计数器的值减 1；如果此时计数器的值小于 0，则当前线程将被阻塞，否则当前线程可以继续执行。
                   up()：计数器的值加 1；如果此时计数器的值小于或者等于 0，则唤醒等待队列中的一个线程，并将其从等待队列中移除。
                   这里提到的 init()、down() 和 up() 三个方法都是原子性的，并且这个原子性是由信号量模型的实现方保证的。在 Java SDK 里面，信号量模型是由 java.util.concurrent.Semaphore 实现的，Semaphore 这个类能够保证这三个方法都是原子操作。
   
                       void down(){    this.count--;    if(this.count<0){      //将当前线程插入等待队列      //阻塞当前线程    }  }

                       void up(){    this.count++;    if(this.count<=0) {      //移除等待队列中的某个线程T      //唤醒线程T    }  }

                   在 Java SDK 并发包里，down() 和 up() 对应的则是 acquire() 和 release()。 
                 如何使用信号量 
                    只需要在进入临界区之前执行一下 down() 操作，退出临界区之前执行一下 up() 操作就可以了。下面是 Java 代码的示例，acquire() 就是信号量里的 down() 操作，release() 就是信号量里的 up() 操作  
                     //用信号量保证互斥 
                     static void addOne() 
                     { s.acquire(); 
                      try { count+=1; }
                       finally 
                       { s.release();
                        }}
                    下面我们再来分析一下，信号量是如何保证互斥的。假设两个线程 T1 和 T2 同时访问 addOne() 方法，当它们同时调用 acquire() 的时候，由于 acquire() 是一个原子操作，所以只能有一个线程（假设 T1）把信号量里的计数器减为 0，另外一个线程（T2）则是将计数器减为 -1。对于线程 T1，信号量里面的计数器的值是 0，大于等于 0，所以线程 T1 会继续执行；对于线程 T2，信号量里面的计数器的值是 -1，小于 0，按照信号量模型里对 down() 操作的描述，线程 T2 将被阻塞。所以此时只有线程 T1 会进入临界区执行count+=1；。 
                    
                    当线程 T1 执行 release() 操作，也就是 up() 操作的时候，信号量里计数器的值是 -1，加 1 之后的值是 0，小于等于 0，按照信号量模型里对 up() 操作的描述，此时等待队列中的 T2 将会被唤醒。于是 T2 在 T1 执行完临界区代码之后才获得了进入临界区执行的机会，从而保证了互斥性。   
                 快速实现一个限流器   
                   其实实现一个互斥锁，仅仅是 Semaphore 的部分功能，Semaphore 还有一个功能是 Lock 不容易实现的，那就是：Semaphore 可以允许多个线程访问一个临界区。

                   现实中还有这种需求？有的。比较常见的需求就是我们工作中遇到的各种池化资源，例如连接池、对象池、线程池等等。其中，你可能最熟悉数据库连接池，在同一时刻，一定是允许多个线程同时使用连接池的，当然，每个连接在被释放前，是不允许其他线程使用的。

                   我在工作中也遇到了一个对象池的需求。所谓对象池呢，指的是一次性创建出 N 个对象，之后所有的线程重复利用这 N 个对象，当然对象在被释放前，也是不允许其他线程使用的。对象池，可以用 List 保存实例对象，这个很简单。但关键是限流器的设计，这里的限流，指的是不允许多于 N 个线程同时进入临界区。那如何快速实现一个这样的限流器呢？这种场景，我立刻就想到了信号量的解决方案

                   信号量的计数器，在上面的例子中，我们设置成了 1，这个 1 表示只允许一个线程进入临界区，但如果我们把计数器的值设置成对象池里对象的个数 N，就能完美解决对象池的限流问题了
                    // 利用对象池的对象，调用func  R exec(Function<T,R> func) {    T t = null;    sem.acquire();    try {      t = pool.remove(0);      return func.apply(t);    } finally {      pool.add(t);      sem.release();    }  }}
                   假设对象池的大小是 10，信号量的计数器初始化为 10，那么前 10 个线程调用 acquire() 方法，都能继续执行，相当于通过了信号灯，而其他线程则会阻塞在 acquire() 方法上。对于通过信号灯的线程，我们为每个线程分配了一个对象 t（这个分配工作是通过 pool.remove(0) 实现的），分配完之后会执行一个回调函数 func，而函数的参数正是前面分配的对象 t ；执行完回调函数之后，它们就会释放对象（这个释放工作是通过 pool.add(t) 实现的），同时调用 release() 方法来更新信号量的计数器。如果此时信号量里计数器的值小于等于 0，那么说明有线程在等待，此时会自动唤醒等待的线程。   
        -- 锁
           -- 锁：Synchronized、ReentrantLock
      
                  Synchronized java内建的同步机制 提供互斥的语义和可见性 当一个线程获取当前锁时，其他线程只能等待
                   Synchronized 底层实现 是由一对monitorenter .monitorexit指令是想的 monitor对象是同步的基本实现
                   java6之前 monitor的实现完全是依靠操作系统内部的互斥锁。因为要进行用户态到内核态的切换，同步操作
                   是一个无差别的重量级操作

                   三种不同的monitor实现
                    重入、偏向
                    偏斜锁 轻量级锁  重量级锁
                   锁的升级降级 就是jvm优化sychronized运行机制 当jvm检测到不同的竞争状况时 会自动切换到适合的锁实现，这种切换就是锁的升级和降级
                    当没有竞争出现时 默认会使用偏斜锁 Jvm会利用cas操作，在对象头上的mark word不分设置线程Id,以表示这个对象偏向于当前线程，并不涉及正真的互斥锁，这样做假设是基于很多应用场景，大部分对象生命周期都最多被一个线程锁定，使用偏斜锁可以降低无竞争开销

                    如果有另外的线程视图锁定某个已经被偏斜过的对象，jvm需要撤销偏斜锁，并切换到轻量级锁的实现，轻量级锁依赖cas操作mark word来试图获取锁如果成功，使用普通的轻量级锁，否则进一步升级为重量级锁

                    锁降级是会发生的 jvm进入安全点会检查是否有闲置的monitor 试图进行降价。

                    sychronized是jvm内部的intrinsic lock,轻量级 重量级 偏斜锁核心代码不在核心类库中，在 jvm代码中
                     偏斜锁并不适合所有的应用场景 revoke操作是比较重的行为
                     偏斜锁会延缓JIT预热的进程 很多场景关闭偏斜锁 -XX:-UseBiasedLocking

                     biasedLocking 定义偏斜锁相关操作 revoke_and-rebias 是获取偏斜锁的相关操作
                     revoke_as_safepoint则定义了当检测到安全点的处理逻辑
                     fast_exit  slow_exit时对应的锁释放逻辑
                      ReadWriteLock->ReentranReadWriteLock
                      StampedLock 不支持再入语义 不是以持有锁的线程为单位

                      lock
                        ReentranLock
                        ReentranReadWriteLock.ReadLock
                        ReentranReadWriteLock.WriteLock
                      要么不占 要么独占，实际应用场景中，有的时候不需要大量竞争的写操作，而是以并发读取操作为主
                      
                      java并发包提供的读写锁等扩展锁的能力 基于的原理是多个读锁是不需要互斥的，因为读操作并不会改写数据，所以并不存在相互干扰。而写操作则会导致并发一致性的问题，所以在线程之间 读写线程之间 需要精心设计的互斥逻辑
                      读写锁开销较大

                      后期引入了stampedLock 在提供类似读写锁的同时。还支持优化读模式，该模式假设大多数情况并不会和写操作冲突
                      其逻辑是先试着读，然后通过validate方法确认是否进入了写模式，如果没有进入，则成功避免开销；如果进入则尝试获取读锁

                  ReentrantLock 再入锁  通过调用代码lock获取  提供公平性 或者条件定义，编码中必须显示调用unlock

                  线程安全
                    是一个多线程环境下正确性的概念 保证多线程环境下共享的 可修改的状态的正确性 状态可以看做是数据
                    如果状态不是共享的 或者不可修改的也就不存在线程安全问题
                  两个办法
                    封装 将内部对象状态隐藏 保护起来
                    不可变  
                  几个基本特性
                    原子性  相关操作中途不会被其他线程干扰 通过同步机制实现
                    可见性   一个线程修改了某个共享变量 其状态立即能够被其他线程知晓 将线程本地状态反应到主内存上 volatiel 负责保证可见性
                    有序性   保证线程内串行语义 避免指令重排


                  ReentrantLock 表示当一个线程视图获取一个它已经获取的锁时 这个获取动作就会自动成功 锁的持有是以线程为单位的而不是基于调用次数
                   再入锁可以设置公平性 
                         ReentrantLock fairLock = new ReentrantLock(true);
                   当公平性为true时 倾向于选择等待时间最久的线程  公平性是为了减少线程饥饿
                   公平性会造成性能下降 为了保证锁释放 每一个lock 都会对应一个unlock
                     带超时的获取锁尝试
                     判断是否有线程或者某个特定线程 在排队等待获取锁
                     可以响应中断
                   条件变量  
                     Condition 是将wait notify notifyall等操作装换为相应的对象 将负责晦涩的同步操作转变为只管可控的对象行为
                     条件变量应用最典型的场景就是ArrayBlockingQueue
                      通过再入锁获取条件变量
                       lock.lockUbterruptibly() 拿到可中断的锁
                      take 中 notempty.await 阻塞
                      enqueue 中 notEmpty.signal() 通知条件满足
           -- ReentrantLock 
               ReentrantLock 中文我们习惯叫做可重入互斥锁，可重入的意思是同一个线程可以对同一个共享资源重复的加锁或释放锁，互斥就是 AQS 中的排它锁的意思，只允许一个线程获得锁
               类注释  
                  可重入互斥锁，和 synchronized 锁具有同样的功能语义，但更有扩展性；
                  构造器接受 fairness 的参数，fairness 是 ture 时，保证获得锁时的顺序，false 不保证；
                  公平锁的吞吐量较低，获得锁的公平性不能代表线程调度的公平性；
                  tryLock() 无参方法没有遵循公平性，是非公平的（lock 和 unlock 都有公平和非公平，而 tryLock 只有公平锁，所以单独拿出来说一说）。 
                  ReentrantLock 的公平和非公平，是针对获得锁来说的，如果是公平的，可以保证同步队列中的线程从头到尾的顺序依次获得锁，非公平的就无法保证，在释放锁的过程中，我们是没有公平和非公平的说法的。  
                类结构
                   ReentrantLock 类本身是不继承 AQS 的，实现了 Lock 接口   
                    public class ReentrantLock implements Lock, java.io.Serializable {}
                   Lock 接口定义了各种加锁，释放锁的方法，接口有如下几个：
                    // 获得锁方法，获取不到锁的线程会到同步队列中阻塞排队
                    void lock();
                   // 获取可中断的锁
                   void lockInterruptibly() throws InterruptedException;
                   / / 尝试获得锁，如果锁空闲，立马返回 true，否则返回 false
                    boolean tryLock();
                   // 带有超时等待时间的锁，如果超时时间到了，仍然没有获得锁，返回 false
                   boolean tryLock(long time, TimeUnit unit) throws InterruptedException;
                   // 释放锁
                   void unlock();
                   // 得到新的 Condition
                   Condition newCondition(); 
                   ReentrantLock 就负责实现这些接口，我们使用时，直接面对的也是这些方法，这些方法的底层实现都是交给 Sync 内部类去实现的，Sync 类的定义如下：
                   abstract static class Sync extends AbstractQueuedSynchronizer {}  

                   Sync 继承了 AbstractQueuedSynchronizer ，所以 Sync 就具有了锁的框架，根据 AQS 的框架，Sync 只需要实现 AQS 预留的几个方法即可，但 Sync 也只是实现了部分方法，还有一些交给子类 NonfairSync 和 FairSync 去实现了，NonfairSync 是非公平锁，FairSync 是公平锁，定义如下：
                   // 同步器 Sync 的两个子类锁
                   static final class FairSync extends Sync {}
                   static final class NonfairSync extends Sync {}
                构造器
                   无参构造器默认构造是非公平的锁，有参构造器可以选择。
                    从构造器中可以看出，公平锁是依靠 FairSync 实现的，非公平锁是依靠 NonfairSync 实现的。
                Sync 同步器  
                  nonfairTryAcquire 
                     // 同步器的状态是 0，表示同步器的锁没有人持有
                     if (c == 0) {
                     // 当前线程持有锁
                     if (compareAndSetState(0, acquires)) {
                     // 标记当前持有锁的线程是谁
                     setExclusiveOwnerThread(current);
                       return true;
                     }
                     }
                     // 如果当前线程已经持有锁了，同一个线程可以对同一个资源重复加锁，代码实现的是  可重入锁
                     else if (current == getExclusiveOwnerThread()) {
                     // 当前线程持有锁的数量 + acquires
                     int nextc = c + acquires;
                     // int 是有最大值的，<0 表示持有锁的数量超过了 int 的最大值
                     if (nextc < 0) // overflow
                        throw new Error("Maximum lock count exceeded");
                     setState(nextc);
                     return true;
                     三点注意
                     通过判断 AQS 的 state 的状态来决定是否可以获得锁，0 表示锁是空闲的；
                     else if 的代码体现了可重入加锁，同一个线程对共享资源重入加锁，底层实现就是把 state + 1，并且可重入的次数是有限制的，为 Integer 的最大值；
                     这个方法是非公平的，所以只有非公平锁才会用到，公平锁是另外的实现。
                     无参的 tryLock 方法调用的就是此方法，tryLock 的方法源码如下：
                      public boolean tryLock() {
                      // 入参数是 1 表示尝试获得一次锁
                      return sync.nonfairTryAcquire(1);
                      } 
                  tryRelease
                   // 释放锁方法，非公平和公平锁都使用
                   protected final boolean tryRelease(int releases) {
                     // 当前同步器的状态减去释放的个数，releases 一般为 1
                     int c = getState() - releases;
                     // 当前线程根本都不持有锁，报错
                    if (Thread.currentThread() != getExclusiveOwnerThread())
                     // 如果 c 为 0，表示当前线程持有的锁都释放了
                     if (c == 0) {
                         free = true;
                        setExclusiveOwnerThread(null);
                     }
                     // 如果 c 不为 0，那么就是可重入锁，并且锁没有释放完，用 state 减去 releases 即可，无需做其他操作
                      setState(c);
                     tryRelease 方法是公平锁和非公平锁都公用的，在锁释放的时候，是没有公平和非公平的说法的。
                      从代码中可以看到，锁最终被释放的标椎是 state 的状态为 0，在重入加锁的情况下，需要重入解锁相应的次数后，才能最终把锁释放，比如线程 A 对共享资源 B 重入加锁 5 次，那么释放锁的话，也需要释放 5 次之后，才算真正的释放该共享资源了。 
                FairSync 公平锁
                  FairSync 公平锁只实现了 lock 和 tryAcquire 两个方法，lock 方法非常简单
                  // acquire 是 AQS 的方法，表示先尝试获得锁，失败之后进入同步队列阻塞等待
                  final void lock() {
                       acquire(1);
                    }
                    // hasQueuedPredecessors 是实现公平的关键
                  // 会判断当前线程是不是属于同步队列的头节点的下一个节点(头节点是释放锁的节点)
                  // 如果是(返回false)，符合先进先出的原则，可以获得锁
                  // 如果不是(返回true)，则继续等待
                   if (!hasQueuedPredecessors() && 
                NonfairSync 非公平锁  
                  // 加锁
                  final void lock() {
                  // cas 给 state 赋值
                  if (compareAndSetState(0, 1))
                  // cas 赋值成功，代表拿到当前锁，记录拿到锁的线程
                  setExclusiveOwnerThread(Thread.currentThread());
                  else
                  // acquire 是抽象类AQS的方法,
                  // 会再次尝试获得锁，失败会进入到同步队列中
                  acquire(1); 
                如何串起来
                  lock加锁 
                    公平locl-> fairsync.lock-> aqs.acquire->fairSyncTryAcquire
                    非公平locl-> Nonfairsync.lock-> aqs.acquire->NonfairSyncTryAcquire 
                   tryLock 
                     / 无参构造器
                     public boolean tryLock() {
                       return sync.nonfairTryAcquire(1);
                     }
                     // timeout 为超时的时间，在时间内，仍没有得到锁，会返回 false
                     public boolean tryLock(long timeout, TimeUnit unit)
                      throws InterruptedException {
                        return sync.tryAcquireNanos(1, unit.toNanos(timeout));
                     }
                   unlock释放锁
                     unlock 释放锁的方法，底层调用的是 Sync 同步器的 release 方法，release 是 AQS 的方法，分成两步：
                      尝试释放锁，如果释放失败，直接返回 false；
                       释放成功，从同步队列的头节点的下一个节点开始唤醒，让其去竞争锁。
                     // 释放锁
                     public void unlock() {
                           sync.release(1);
                     }  
                  Condition
                  ReentrantLock 对 Condition 并没有改造，直接使用 AQS 的 ConditionObject 即可   
           -- CountDownLatch、Atomic 等其它源码解析
               CountDownLatch
                 CountDownLatch 中文有的叫做计数器，也有翻译为计数锁，其最大的作用不是为了加锁，而是通过计数达到等待的功能，主要有两种形式的等待：
                 让一组线程在全部启动完成之后，再一起执行（先启动的线程需要阻塞等待后启动的线程，直到一组线程全部都启动完成后，再一起执行）；
                 主线程等待另外一组线程都执行完成之后，再继续执行

                 ountDownLatch 有两个比较重要的 API，分别是 await 和 countDown，管理着线程能否获得锁和锁的释放（也可以称为对 state 的计数增加和减少）。
                
                 await 
                   await 我们可以叫做等待，也可以叫做加锁，有两种不同入参的方法
                   无参 await 底层使用的是 acquireSharedInterruptibly 方法，有参的使用的是 tryAcquireSharedNanos 方法，这两个方法都是 AQS 的方法，底层实现很相似，主要分成两步：
                   使用子类的 tryAcquireShared 方法尝试获得锁，如果获取了锁直接返回，获取不到锁走  2；
                   获取不到锁，用 Node 封装一下当前线程，追加到同步队列的尾部，等待在合适的时机去获得锁。

                   第二步是 AQS 已经实现了，第一步 tryAcquireShared 方法是交给 Sync 实现的，源码如下：
                   // 如果当前同步器的状态是 0 的话，表示可获得锁
                   protected int tryAcquireShared(int acquires) {
                      return (getState() == 0) ? 1 : -1;
                   }
                   
                   获得锁的代码也很简单，直接根据同步器的 state 字段来进行判断，但还是有两点需要注意一下：

                   CountDownLatch 的 state 并不是 AQS 的默认值 0，而是可以赋值的，是在  CountDownLatch 初始化的时候赋值的，代码如下：
                   // 初始化,count 代表 state 的初始化值
                   public CountDownLatch(int count) {
                   if (count < 0) throw new IllegalArgumentException("count < 0");
                     // new Sync 底层代码是 state = count;
                     this.sync = new Sync(count);
                   }  
                   这里的初始化的 count 和一般的锁意义不太一样，count  表示我们希望等待的线程数，在两种不同的等待场景中，count 有不同的含义：
                   让一组线程在全部启动完成之后，再一起执行的等待场景下， count 代表一组线程的个数；
                   主线程等待另外一组线程都执行完成之后，再继续执行的等待场景下，count  代表一组线程的个数。
                   所以我们可以把 count 看做我们希望等待的一组线程的个数，可能我们是等待一组线程全 部启动完成，可能我们是等待一组线程全部执行完成  
                 countDown
                   countDown 中文翻译为倒计时，每调用一次，都会使 state 减一，底层调用的方法如下：
                   public void countDown() {
                     sync.releaseShared(1);
                   }  
                   releaseShared 是 AQS 定义的方法，方法主要分成两步：
                   尝试释放锁（tryReleaseShared），锁释放失败直接返回，释放成功走 2；
                   释放当前节点的后置等待节点。
                   第二步 AQS 已经实现了，第一步是 Sync 实现的，我们一起来看下 tryReleaseShared 方法的实现源码：
                     // 对 state 进行递减，直到 state 变成 0；
                     // state 递减为 0 时，返回 true，其余返回 false
                      boolean tryReleaseShared(int releases) {
                    // 自旋保证 CAS 一定可以成功
                     for (;;) {
                        int c = getState();
                        // state 已经是 0 了，直接返回 false
                        if (c == 0)
                         return false;
                        // 对 state 进行递减
                         int nextc = c-1;
                        if (compareAndSetState(c, nextc))
                         return nextc == 0;
                      }
                    从源码中可以看到，只有到 count 递减到 0 时，countDown 才会返回 true。
                 两种示例
                    // await 时有两点需要注意：await 时 state 不会发生变化，2：startSignal 的state初始化是 1，所以所有子线程都是获取不到锁的，都需要到同步队列中去等待，达到先启动的子线程等待后面启动的子线程的结果
                     startSignal.await();
                      doWork();
                      // countDown 每次会使 state 减一，doneSignal 初始化为 9，countDown 前 8 次执行都会返回 false (releaseShared 方法)，执行第 9 次时，state 递减为 0，会 countDown 成功，表示所有子线程都执行完了，会释放 await 在 doneSignal 上的主线程
                     doneSignal.countDown();  

                      // state 初始化为 1 很关键，子线程是不断的 await，await 时 state 是不会变化的，并且发现 state 都是 1，所有线程都获取不到锁
                     // 造成所有线程都到同步队列中去等待，当主线程执行 countDown 时，就会一起把等待的线程给释放掉
                     CountDownLatch startSignal = new CountDownLatch(1);
                    // state 初始化成 9，表示有 9 个子线程执行完成之后，会唤醒主线程
                    CountDownLatch doneSignal = new CountDownLatch(9);


                      // 这行代码唤醒 9 个子线程，开始执行(因为 startSignal 锁的状态是 1，所以调用一次 countDown 方法就可以释放9个等待的子线程)
                       startSignal.countDown();
                     // 这行代码使主线程陷入沉睡，等待 9 个子线程执行完成之后才会继续执行(就是等待子线程执行 doneSignal.countDown())
                    doneSignal.await();        
               Atomic 原子操作类
                 Atomic 打头的原子操作类，在高并发场景下，都是线程安全的
                  // 自减 1，并返回自增之前的值    
                  public final int getAndDecrement() {
                    return unsafe.getAndAddInt(this, valueOffset, -1);
                  }
                 线程安全的操作方法，底层都是使用 unsafe 方法实现，以上几个 unsafe 方法不是使用 Java 实现的，都是线程安全的

                 AtomicInteger 是对 int 类型的值进行自增自减，那如果 Atomic 的对象是个自定义类怎么办呢，Java 也提供了自定义对象的原子操作类，叫做 AtomicReference。AtomicReference 类可操作的对象是个泛型，所以支持自定义类，其底层是没有自增方法的，操作的方法可以作为函数入参传递，源码如下：

                  // 对 x 执行 accumulatorFunction 操作
                    // accumulatorFunction 是个函数，可以自定义想做的事情
                 // 返回老值
                  public final V getAndAccumulate(V x,
                                BinaryOperator<V> accumulatorFunction) {
                      // prev 是老值，next 是新值
                      V prev, next;
                    // 自旋 + CAS 保证一定可以替换老值
                   do {
                          prev = get();
                          // 执行自定义操作
                         next = accumulatorFunction.apply(prev, x);
                       } while (!compareAndSet(prev, next));
                  return prev;
                  }
           -- 锁面试题
               AQS 相关面试题        
                 说说自己对 AQS 的理解？
                    如果和面试官面对面的话，可以边说边画出我们在 AQS 源码解析上中画出的整体架构图，并且可以这么说：
                       AQS 是一个锁框架，它定义了锁的实现机制，并开放出扩展的地方，让子类去实现，比如我们在 lock 的时候，AQS 开放出 state 字段，让子类可以根据 state 字段来决定是否能够获得锁，对于获取不到锁的线程 AQS 会自动进行管理，无需子类锁关心，这就是 lock 时锁的内部机制，封装的很好，又暴露出子类锁需要扩展的地方；
                       AQS 底层是由同步队列 + 条件队列联手组成，同步队列管理着获取不到锁的线程的排队和释放，条件队列是在一定场景下，对同步队列的补充，比如获得锁的线程从空队列中拿数据，肯定是拿不到数据的，这时候条件队列就会管理该线程，使该线程阻塞；
                       AQS 围绕两个队列，提供了四大场景，分别是：获得锁、释放锁、条件队列的阻塞，条件队列的唤醒，分别对应着 AQS 架构图中的四种颜色的线的走向。
                       以上三点都是 AQS 全局方面的描述，接着你可以问问面试官要不要说细一点，可以的话，按照 AQS 源码解析上下两篇，把四大场景都说一下就好了。

                       这样说的好处是很多的：
                         面试的主动权把握在自己手里，而且都是自己掌握的知识点；
                         由全到细的把 AQS 全部说完，会给面试官一种你对 AQS 了如指掌的感觉，再加上全部说完耗时会很久，面试时间又很有限，面试官就不会再问关于 AQS 一些刁钻的问题了，这样 AQS 就可以轻松过关。
                 多个线程通过锁请求共享资源，获取不到锁的线程怎么办？
                     加锁(排它锁)主要分为以下四步：
                      尝试获得锁，获得锁了直接返回，获取不到锁的走到 2；
                      用 Node 封装当前线程，追加到同步队列的队尾，追加到队尾时，又有两步，如 3 和 4；
                      自旋 + CAS 保证前一个节点的状态置为 signal；
                      阻塞自己，使当前线程进入等待状态。
                      获取不到锁的线程会进行 2、3、4 步，最终会陷入等待状态，这个描述的是排它锁。
                 排它锁和共享锁的处理机制是一样的么？     
                      不同的是在于第一步，线程获得排它锁的时候，仅仅把自己设置为同步队列的头节点即可，但如果是共享锁的话，还会去唤醒自己的后续节点，一起来获得该锁。

                 共享锁和排它锁的区别？
                      排它锁的意思是同一时刻，只能有一个线程可以获得锁，也只能有一个线程可以释放锁。
                      共享锁可以允许多个线程获得同一个锁，并且可以设置获取锁的线程数量，共享锁之所以能够做到这些，是因为线程一旦获得共享锁，把自己设置成同步队列的头节点后，会自动的去释放头节点后等待获取共享锁的节点，让这些等待节点也一起来获得共享锁，而排它锁就不会这么干。  
                 排它锁和共享锁说的是加锁时的策略，那么锁释放时有排它锁和共享锁的策略么？
                      是的，排它锁和共享锁，主要体现在加锁时，多个线程能否获得同一个锁。
                      但在锁释放时，是没有排它锁和共享锁的概念和策略的，概念仅仅针对锁获取。      
                 描述下同步队列？
                      同步队列底层的数据结构就是双向的链表，节点叫做 Node，头节点叫做 head，尾节点叫做 tail，节点和节点间的前后指向分别叫做 prev、next，如果是面对面面试的话，可以画一下 AQS 整体架构图中的同步队列。
                      同步队列的作用：阻塞获取不到锁的线程，并在适当时机释放这些线程。
                      实现的大致过程：当多个线程都来请求锁时，某一时刻有且只有一个线程能够获得锁（排它锁），那么剩余获取不到锁的线程，都会到同步队列中去排队并阻塞自己，当有线程主动释放锁时，就会从同步队列中头节点开始释放一个排队的线程，让线程重新去竞争锁   
                 描述下线程入、出同步队列的时机和过程？
                      (排它锁为例)从 AQS 整体架构图中，可以看出同步队列入队和出队都是有两个箭头指向，所以入队和出队的时机各有两个。
                      同步队列入队时机：
                        多个线程请求锁，获取不到锁的线程需要到同步队列中排队阻塞；
                        条件队列中的节点被唤醒，会从条件队列中转移到同步队列中来。
                      同步队列出队时机：
                        锁释放时，头节点出队；
                        获得锁的线程，进入条件队列时，会释放锁，同步队列头节点开始竞争锁。
                       四个时机的过程可以参考 AQS 源码解析，1 参考 acquire 方法执行过程，2 参考 signal 方法，3 参考 release 方法，4 参考 await 方法。
                  为什么 AQS 有了同步队列之后，还需要条件队列？
                       ，一般情况下，我们只需要有同步队列就好了，但在上锁后，需要操作队列的场景下，一个同步队列就搞不定了，需要条件队列进行功能补充，比如当队列满时，执行 put 操作的线程会进入条件队列等待，当队列空时，执行 take 操作的线程也会进入条件队列中等待，从一定程度上来看，条件队列是对同步队列的场景功能补充 
                  描述一下条件队列中的元素入队和出队的时机和过程？
                       入队时机：执行 await 方法时，当前线程会释放锁，并进入到条件队列。
                        出队时机：执行 signal、signalAll 方法时，节点会从条件队列中转移到同步队列中。 

                  述一下条件队列中的节点转移到同步队列中去的时机和过程
                        时机：当有线程执行 signal、signalAll 方法时，从条件队列的头节点开始，转移到同步队列中去。
                       过程主要是以下几步：
                           找到条件队列的头节点，头节点 next 属性置为 null，从条件队列中移除了；
                           头节点追加到同步队列的队尾；
                           头节点状态（waitStatus）从 CONDITION 修改成 0（初始化状态）；
                           将节点的前一个节点状态置为 SIGNAL。 
                  线程入条件队列时，为什么需要释放持有的锁？
                       原因很简单，如果当前线程不释放锁，一旦跑去条件队里中阻塞了，后续所有的线程都无法获得锁，正确的场景应该是：当前线程释放锁，到条件队列中去阻塞后，其他线程仍然可以获得当前锁。
               AQS 子类锁面试题
                   如果我要自定义锁，大概的实现思路是什么样子的？
                     新建内部类继承 AQS，并实现 AQS 的 tryAcquire 和 tryRelease 两个方法，在 tryAcquire 方法里面实现控制能否获取锁，比如当同步器状态 state 是 0 时，即可获得锁，在 tryRelease 方法里面控制能否释放锁，比如将同步器状态递减到 0 时，即可释放锁；
                      对外提供 lock、release 两个方法，lock 表示获得锁的方法，底层调用 AQS 的 acquire 方法，release 表示释放锁的方法，底层调用 AQS 的 release 方法。
                   描述 ReentrantLock 两大特性：可重入性和公平性？底层分别如何实现的？
                      可重入性说的是线程可以对共享资源重复加锁，对应的，释放时也可以重复释放，对于 ReentrantLock 来说，在获得锁的时候，state 会加 1，重复获得锁时，不断的对 state 进行递增即可，比如目前 state 是 4，表示线程已经对共享资源加锁了 4 次，线程每次释放共享资源的锁时，state 就会递减 1，直到递减到 0 时，才算真正释放掉共享资源。

                      公平性和非公平指的是同步队列中的线程得到锁的机制，如果同步队列中的线程按照阻塞的顺序得到锁，我们称之为公平的，反之是非公平的，公平的底层实现是 ReentrantLock 的 tryAcquire 方法（调用的是 AQS 的 hasQueuedPredecessors 方法）里面实现的，要释放同步队列的节点时（或者获得锁时），判断当前线程节点是不是同步队列的头节点的后一个节点，如果是就释放，不是则不能释放，通过这种机制，保证同步队列中的线程得到锁时，是按照从头到尾的顺序的。

                    如果一个线程需要等待一组线程全部执行完之后再继续执行，有什么好的办法么？是如何实现的？
                       CountDownLatch 就提供了这样的机制，比如一组线程有 5 个，只需要在初始化 CountDownLatch 时，给同步器的 state 赋值为 5，主线程执行 CountDownLatch.await ，子线程都执行 CountDownLatch.countDown 即可。
           -- 各种锁在工作中使用场景和细节  
                synchronized
                   synchronized 是可重入的排它锁，和 ReentrantLock 锁功能相似，任何使用 synchronized 的地方，几乎都可以使用 ReentrantLock 来代替，两者最大的相似点就是：可重入 + 排它锁，两者的区别主要有这些：
                   ReentrantLock 的功能更加丰富，比如提供了 Condition，可以打断的加锁 API、能满足锁 + 队列的复杂场景等等；
                   ReentrantLock 有公平锁和非公平锁之分，而 synchronized 都是非公平锁；
                   两者的使用姿势也不同，ReentrantLock 需要申明，有加锁和释放锁的 API，而 synchronized 会自动对代码块进行加锁释放锁的操作，synchronized 使用起来更加方便。

                   共享资源初始化
                     在分布式的系统中，我们喜欢把一些死的配置资源在项目启动的时候加锁到 JVM 内存里面去，这样请求在拿这些共享配置资源时，就可直接从内存里面拿，不必每次都从数据库中拿，减少了时间开销。
                     一般这样的共享资源有：死的业务流程配置 + 死的业务规则配置
                     共享资源初始化的步骤一般为：项目启动 -> 触发初始化动作 ->单线程从数据库中捞取数据 -> 组装成我们需要的数据结构 -> 放到 JVM 内存中。
                     @PostConstruct 注解的作用是在 Spring 容器初始化时，再执行该注解打上的方法，也就是说上图说的 init 方法触发的时机，是在 Spring 容器启动的时候。 

                     不是可以直接使用了 ConcurrentHashMap 么，为什么还需要加锁呢？的确 ConcurrentHashMap 是线程安全的，但它只能够保证 Map 内部数据操作时的线程安全，是无法保证多线程情况下，查询数据库并组装数据的整个动作只执行一次的，我们加 synchronized 锁住的是整个操作，保证整个操作只执行一次。
                CountDownLatch
                  向线程池提交了 30 个任务后，主线程如何等待 30 个任务都执行完成呢？因为主线程需要收集 30 个子任务的执行情况，并汇总返回给前端。
                  CountDownLatch 可以的，CountDownLatch 具有这种功能，让主线程去等待子任务全部执行完成之后才继续执行。
                  此时还有一个关键，我们需要知道子线程执行的结果，所以我们用 Runnable 作为线程任务就不行了，因为 Runnable 是没有返回值的，我们需要选择 Callable 作为任务。
           -- 重写锁的设计结构和细节
                1 需求
                   一般自定义锁的时候，我们都是根据需求来进行定义的，不可能凭空定义出锁来，说到共享锁，大家可能会想到很多场景，比如说对于共享资源的读锁可以是共享的，比如对于数据库链接的共享访问，比如对于 Socket 服务端的链接数是可以共享的，场景有很多，我们选择共享访问数据库链接这个场景来定义一个锁。
                2 详细设计
                    假定(以下设想都为假定)我们的数据库是单机 mysql，只能承受 10 个链接，创建数据库链接时，我们是通过最原始 JDBC 的方式，我们用一个接口把用 JDBC 创建链接的过程进行了封装，这个接口我们命名为：创建链接接口。
                    共享访问数据库链接的整体要求如下：所有请求加在一起的 mysql 链接数，最大不能超过 10（包含 10），一旦超过 10，直接报错。 
                3 
                 定义锁
                     首先我们需要定义一个锁出来，定义时需要有两个元素：
                      锁的定义：同步器 Sync；
                      锁对外提供的加锁和解锁的方法。   
                      // 共享不公平锁
                     public class ShareLock implements Serializable{
                     // 同步器
                     private final Sync sync;
                     // 用于确保不能超过最大值
                     private final int maxCount;

                     唯一需要注意的是，锁需要规定好 API 的规范，主要是两方面：
                       API 需要什么，就是锁在初始化的时候，你需要传哪些参数给我，在 ShareLock 初始化时，需要传最大可共享锁的数目；
                       需要定义自身的能力，即定义每个方法的入参和出参。在 ShareLock 的实现中，加锁和释放锁的入参都没有，是方法里面写死的 1，表示每次方法执行，只能加锁一次或释放锁一次，出参是布尔值，true 表示加锁或释放锁成功，false 表示失败，底层使用的都是 Sync 非公平锁。
                 定义同步器 Sync
                    边界的判断，比如入参是否非法，释放锁时，会不会出现预期的 state 非法等边界问题，对于此类问题我们都需要加以判断，体现出思维的严谨性；
                    加锁和释放锁，需要用 for 自旋 + CAS 的形式，来保证当并发加锁或释放锁时，可以重试成功。写 for 自旋时，我们需要注意在适当的时机要 return，不要造成死循环，CAS 的方法 AQS 已经提供了，不要自己写，我们自己写的 CAS 方法是无法保证原子性的。
                  通过能否获得锁来决定能否得到链接
                     通过 JDBC 建立和 Mysql 的链接；
                     结合锁，来防止请求过大时，Mysql 的总链接数不能超过 10 个。

                     public class MysqlConnection {
                         private final ShareLock lock;
  
                         // maxConnectionSize 表示最大链接数
                        public MysqlConnection(int maxConnectionSize) {
                            lock = new ShareLock(maxConnectionSize);
                     }
                     }

                     // 对外获取 mysql 链接的接口
                      // 这里不用try finally 的结构，获得锁实现底层不会有异常
                     // 即使出现未知异常，也无需释放锁
                     public Connection getLimitConnection() {
                         if (lock.lock()) {
                              return getConnection();
                       }
                        return null;
                        }

                      // 对外释放 mysql 链接的接口
                    public boolean releaseLimitConnection() {
                      return lock.unLock();
                     }
           -- java 程序什么情况会产生死锁如何定位和修复
               死锁是一种特定的程序状态 在实体之间，由于循环依赖导致彼此一直处于等待状态之中，没有任何个体能够继续前进
               死锁不仅仅是在线程之间会发生，存在资源独占的进程之间同样也会出现死锁
               通常来说  我们大多聚焦于在多线程中的死锁，指两个线程或者多个线程之间互相持有对方需要的锁，而处于阻塞的状态

               定位死锁最常见的方式就是利用jstack等工具获取线程栈，然后定位互相之间的依赖关系，进而找到死锁
               如果明显的死锁 往往jstack就能直接定位，类似jconsole甚至在图形界面进行有限制的死锁检测

               定位死锁：
                 1 使用jps或者系统的ps命令 任务管理器 确定进程id
                 2 调用jstack 获取线程栈  jstack pid
                 找到处于blocked状态的线程，按照试图获取的锁的id查找，很快就定位问题
               
               区分线程状态-》查看等待目标-》对比Monitor等持有状态
               开发自己死锁定位工具 参考ThreadMXBean  


               死锁发生的条件
                 互斥条件
                 互斥条件是长期持有的，在使用结束之前，自己不会释放，也不能被其他线程抢占
                 循环依赖关系，两个或者多个个体之间出现了锁的链条环

               如何避免死锁
                  1避免使用多个锁
                  2将对象和锁之间的关系，用图形化的方式表示分别抽取出来
                    DLSample->锁A->锁B
                   然后根据对象组合，调用的关系对个和组合，考虑可能调用时序 
                   按照可能时序合并，发现可能发生死锁场景
                  3带超时的方法 timed_wait
                   完全可以就不假定该锁就一定会获得，指定超时时间，并未无法获取锁时准备推出逻辑
                    ReentrantLock支持非阻塞的获取锁操作tryLock(timeout,unit) 这是一种插队非公平行为
                  4静态代码分析 findBugs
                    类加载过程中，大量使用自定义类加载时。   
           -- AQS 原理：
              从原理上，一种同步结构往往是可以利用其他的结构实现的，例如我在专栏第 19 讲中提到过可以使用 Semaphore 实现互斥锁。但是，对某种同步结构的倾向，会导致复杂、晦涩的实现逻辑，所以，他选择了将基础的同步相关操作抽象在 AbstractQueuedSynchronizer 中，利用 AQS 为我们构建同步结构提供了范本。
              AQS 内部数据和方法
                一个 volatile 的整数成员表征状态，同时提供了 setState 和 getState 方法private volatile int state;
                一个先入先出（FIFO）的等待线程队列，以实现多线程间竞争和等待，这是 AQS 机制的核心之一。
                各种基于 CAS 的基础操作方法，以及各种期望具体同步结构去实现的 acquire/release 方法。
                利用 AQS 实现一个同步结构，至少要实现两个基本类型的方法，分别是 acquire 操作，获取资源的独占权；还有就是 release 操作，释放对某个资源的独占
              以 ReentrantLock 为例，它内部通过扩展 AQS 实现了 Sync 类型，以 AQS 的 state 来反映锁的持有情况。  
                private final Sync sync;
                abstract static class Sync extends AbstractQueuedSynchronizer { …}
              下面是 ReentrantLock 对应 acquire 和 release 操作，如果是 CountDownLatch 则可以看作是 await()/countDown()，具体实现也有区别。  
               public void lock() {
                 sync.acquire(1);
               }
               public void unlock() {
                 sync.release(1);
                }
                排除掉一些细节，整体地分析 acquire 方法逻辑，其直接实现是在 AQS 内部，调用了 tryAcquire 和  acquireQueued，这是两个需要搞清楚的基本部分 
                首先，我们来看看 tryAcquire。在 ReentrantLock 中，tryAcquire 逻辑实现在 NonfairSync 和 FairSync 中，分别提供了进一步的非公平或公平性方法，而 AQS 内部 tryAcquire 仅仅是个接近未实现的方法（直接抛异常），这是留个实现者自己定义的操作。
                以非公平的 tryAcquire 为例，其内部实现了如何配合状态与 CAS 获取锁，注意，对比公平版本的 tryAcquire，它在锁无人占有时，并不检查是否有其他等待者，这里体现了非公平的语义
                int c = getState();// 获取当前AQS内部状态量    if (c == 0) { // 0表示无人占有，则直接用CAS修改状态位，      if (compareAndSetState(0, acquires)) {// 不检查排队情况，直接争抢          setExclusiveOwnerThread(current);  //并设置当前线程独占锁          return true;      }    } else if (current == getExclusiveOwnerThread()) { //即使状态不是0，也可能当前线程是锁持有者，因为这是再入锁      int nextc = c + acquires; 
                接下来我再来分析 acquireQueued，如果前面的 tryAcquire 失败，代表着锁争抢失败，进入排队竞争阶段。这里就是我们所说的，利用 FIFO 队列，实现线程间对锁的竞争的部分，算是是 AQS 的核心逻辑。
                当前线程会被包装成为一个排他模式的节点（EXCLUSIVE），通过 addWaiter  方法添加到队列中。acquireQueued 的逻辑，简要来说，就是如果当前节点的前面是头节点，则试图获取锁，一切顺利则成为新的头节点；否则，有必要则等待

                 for (;;) {// 循环          final Node p = node.predecessor();// 获取前一个节点          if (p == head && tryAcquire(arg)) { // 如果前一个节点是头结点，表示当前节点合适去tryAcquire              setHead(node); // acquire成功，则设置新的头节点              p.next = null; // 将前面节点对当前节点的引用清空              return interrupted;          }
                到这里线程试图获取锁的过程基本展现出来了，tryAcquire 是按照特定场景需要开发者去实现的部分，而线程间竞争则是 AQS 通过 Waiter 队列与 acquireQueued 提供的，在 release 方法中，同样会对队列进行对应操作。

             - 独占 & 共享
             - state & CHL队列 
               AbstractQueuedSynchronizer 中文翻译叫做同步器，简称 AQS，是各种各样锁的基础，比如说 ReentrantLock、CountDownLatch 等等，这些我们经常用的锁底层实现都是 AQS
              整体架构
                AQS 中队列只有两个：同步队列 + 条件队列，底层数据结构两者都是链表；
                图中有四种颜色的线代表四种不同的场景，1、2、3 序号代表看的顺序
                AQS 本身就是一套锁的框架，它定义了获得锁和释放锁的代码结构，所以如果要新建锁，只要继承 AQS，并实现相应方法即可
               类注释
                提供了一种框架，自定义了先进先出的同步队列，让获取不到锁的线程能进入同步队列中排队；
                同步器有个状态字段，我们可以通过状态字段来判断能否得到锁，此时设计的关键在于依赖安全的 atomic value 来表示状态（虽然注释是这个意思，但实际上是通过把状态声明为 volatile，在锁里面修改状态值来保证线程安全的）；
                子类可以通过给状态 CAS 赋值来决定能否拿到锁，可以定义那些状态可以获得锁，哪些状态表示获取不到锁（比如定义状态值是 0 可以获得锁，状态值是 1 就获取不到锁）；
                子类可以新建非 public 的内部类，用内部类来继承 AQS，从而实现锁的功能；
                AQS 提供了排它模式和共享模式两种锁模式。排它模式下：只有一个线程可以获得锁，共享模式可以让多个线程获得锁，子类 ReadWriteLock 实现了两种模式；
                内部类 ConditionObject 可以被用作 Condition，我们通过 new ConditionObject () 即可得到条件队列；
                AQS 实现了锁、排队、锁队列等框架，至于如何获得锁、释放锁的代码并没有实现，比如 tryAcquire、tryRelease、tryAcquireShared、tryReleaseShared、isHeldExclusively 这些方法，AQS 中默认抛 UnsupportedOperationException 异常，都是需要子类去实现的；
                AQS 继承 AbstractOwnableSynchronizer 是为了方便跟踪获得锁的线程，可以帮助监控和诊断工具识别是哪些线程持有了锁；
                AQS 同步队列和条件队列，获取不到锁的节点在入队时是先进先出，但被唤醒时，可能并不会按照先进先出的顺序执行。  
               类定义：
                 public abstract class AbstractQueuedSynchronizer
                 extends AbstractOwnableSynchronizer
                  implements java.io.Serializable { 
                  AQS 是个抽象类，就是给各种锁子类继承用的，AQS 定义了很多如何获得锁，如何释放锁的抽象方法，目的就是为了让子类去实现；
                  继承了 AbstractOwnableSynchronizer，AbstractOwnableSynchronizer 的作用就是为了知道当前是那个线程获得了锁，方便监控用的 
               基本属性
                  同步器简单属性、同步队列属性、条件队列属性、公用 Node
                  简单属性
                    // 同步器的状态，子类会根据状态字段进行判断是否可以获得锁
                    // 比如 CAS 成功给 state 赋值 1 算得到锁，赋值失败为得不到锁， CAS 成功给 state 赋值 0 算释放锁，赋值失败为释放失败
                   // 可重入锁，每次获得锁 +1，每次释放锁 -1
                    private volatile int state;

                    // 自旋超时阀值，单位纳秒
                    // 当设置等待时间时才会用到这个属性
                    static final long spinForTimeoutThreshold = 1000L;
                   最重要的就是 state 属性，是 int 属性的，所有继承 AQS 的锁都是通过这个字段来判断能不能获得锁，能不能释放锁。
                  同步队列属性
                    首先我们介绍以下同步队列：当多个线程都来请求锁时，某一时刻有且只有一个线程能够获得锁（排它锁），那么剩余获取不到锁的线程，都会到同步队列中去排队并阻塞自己，当有线程主动释放锁时，就会从同步队列头开始释放一个排队的线程，让线程重新去竞争锁。

                     所以同步队列的主要作用阻塞获取不到锁的线程，并在适当时机释放这些线程。

                      同步队列底层数据结构是个双向链表，我们从源码中可以看到链表的头尾
                    // 同步队列的头。
                     private transient volatile Node head; 
                     源码中的 Node 是同步队列中的元素，但 Node 被同步队列和条件队列公用，所以我们在说完条件队列之后再说 Node。 
                  条件队列的属性
                      条件队列和同步队列的功能一样，管理获取不到锁的线程，底层数据结构也是链表队列，但条件队列不直接和锁打交道，但常常和锁配合使用，是一定的场景下，对锁功能的一种补充。
                      // 条件队列，从属性上可以看出是链表结构
                      public class ConditionObject implements Condition, java.io.Serializable {
                         private static final long serialVersionUID = 1173984872572414699L;
                         // 条件队列中第一个 node
                         private transient Node firstWaiter;
                         // 条件队列中最后一个 node
                          private transient Node lastWaiter;
                        }  
                       ConditionObject 我们就称为条件队列，我们需要使用时，直接 new ConditionObject () 即可。
                        ConditionObject 是实现 Condition 接口的，Condition 接口相当于 Object 的各种监控方法，比如 Object#wait ()、Object#notify、Object#notifyAll 这些方法，我 
                  Node 
                      Node 非常重要，即是同步队列的节点，又是条件队列的节点，在入队的时候，我们用 Node 把线程包装一下，然后把 Node 放入两个队列中，我们看下 Node 的数据结构
                        /**
                       * 同步队列单独的属性
                          */
                          //node 是共享模式
                        static final Node SHARED = new Node();
                         //node 是排它模式
                        static final Node EXCLUSIVE = null;

                         // 当前节点的前节点
                       // 节点 acquire 成功后就会变成head
                       // head 节点不能被 cancelled
                        volatile Node prev;

                       // 当前节点的下一个节点
                        volatile Node next;

                         /**
                       * 两个队列共享的属性
                           */
                           // 表示当前节点的状态，通过节点的状态来控制节点的行为
                          // 普通同步节点，就是 0 ，条件节点是 CONDITION -2
                        volatile int waitStatus;

                      // waitStatus 的状态有以下几种
                       // 被取消
                         static final int CANCELLED =  1;

                        // SIGNAL 状态的意义：同步队列中的节点在自旋获取锁的时候，如果前一个节点的状态是 SIGNAL，那么自己就可以阻塞休息了，否则自己一直自旋尝试获得锁
                        static final int SIGNAL    = -1;

                       // 表示当前 node 正在条件队列中，当有节点从同步队列转移到条件队列时，状态就会被更改成 CONDITION
                        static final int CONDITION = -2;

                      // 无条件传播,共享模式下，该状态的进程处于可运行状态
                       static final int PROPAGATE = -3;

                       从 Node 的结构中，我们需要重点关注 waitStatus 字段，Node 的很多操作都是围绕着 waitStatus 字段进行的。

                       Node 的 pre、next 属性是同步队列中的链表前后指向字段，nextWaiter 是条件队列中下一个节点的指向字段，但在同步队列中，nextWaiter 只是一个标识符，表示当前节点是共享还是排它模式。
                  共享锁和排他锁的区别
                      排它锁的意思是同一时刻，只能有一个线程可以获得锁，也只能有一个线程可以释放锁。
                      共享锁可以允许多个线程获得同一个锁，并且可以设置获取锁的线程数量。      
               Condition
                类注释
                 刚才我们看条件队列 ConditionObject 时，发现其是实现 Condition 接口的，现在我们一起来看下 Condition 接口，其类注释上是这么写的：
                 当 lock 代替 synchronized 来加锁时，Condition 就可以用来代替 Object 中相应的监控方法了，比如 Object#wait ()、Object#notify、Object#notifyAll 这些方法；
                 提供了一种线程协作方式：一个线程被暂停执行，直到被其它线程唤醒；
                 Condition 实例是绑定在锁上的，通过 Lock#newCondition 方法可以产生该实例；
                 除了特殊说明外，任意空值作为方法的入参，都会抛出空指针；
                  Condition 提供了明确的语义和行为，这点和 Object 监控方法不同。
                其他方法  
                 定义出一些方法，这些方法奠定了条件队列的基础，方法主要有：
                 void await() throws InterruptedException;  
                 这个方法的主要作用是：使当前线程一直等待，直到被 signalled 或被打断。
                 当以下四种情况发生时，条件队列中的线程将被唤醒
                  有线程使用了 signal 方法，正好唤醒了条件队列中的当前线程；
                  有线程使用了 signalAll 方法；
                  其它线程打断了当前线程，并且当前线程支持被打断；
                  被虚假唤醒 (即使没有满足以上 3 个条件，wait 也是可能被偶尔唤醒)。
                 线程从条件队列中苏醒时，必须重新获得锁，才能真正被唤
                 超时方法
                  // 返回的 long 值表示剩余的给定等待时间，如果返回的时间小于等于 0 ，说明等待时间过了
                 / 选择纳秒是为了避免计算剩余等待时间时的截断误差
                  long awaitNanos(long nanosTimeout) throws InterruptedException;
                 // 虽然入参可以是任意单位的时间，但底层仍然转化成纳秒
                  boolean await(long time, TimeUnit unit) throws InterruptedException;
                  唤醒方法
                   了等待方法，还是唤醒线程的两个方法，如下：
                   // 唤醒条件队列中的一个线程，在被唤醒前必须先获得锁
                   void signal();
                   // 唤醒条件队列中的所有线程
                   void signalAll();
              同步器状态
                在同步器中，我们有两个状态，一个叫做 state，一个叫做 waitStatus，两者是完全不同的概念：
                 state 是锁的状态，是 int 类型，子类继承 AQS 时，都是要根据 state 字段来判断有无得到锁，比如当前同步器状态是 0，表示可以获得锁，当前同步器状态是 1，表示锁已经被其他线程持有，当前线程无法获得锁；
                 waitStatus 是节点（Node）的状态，种类很多，一共有初始化 (0)、CANCELLED (1)、SIGNAL (-1)、CONDITION (-2)、PROPAGATE (-3)，各个状态的含义可以见上文。
              获取锁
                获取锁最直观的感受就是使用 Lock.lock () 方法来获得锁，最终目的是想让线程获得对资源的访问权。
                Lock 一般是 AQS 的子类，lock 方法根据情况一般会选择调用 AQS 的 acquire 或 tryAcquire 方法。
                acquire 方法 AQS 已经实现了，tryAcquire 方法是等待子类去实现，acquire 方法制定了获取锁的框架，先尝试使用 tryAcquire 方法获取锁，获取不到时，再入同步队列中等待锁。tryAcquire 方法 AQS 中直接抛出一个异常，表明需要子类去实现，子类可以根据同步器的 state 状态来决定是否能够获得锁，接下来我们详细看下 acquire 的源码解析。
               acquire 也分两种，一种是排它锁，一种是共享锁 
               acquire 排它锁 
                 // 排它模式下，尝试获得锁
                 public final void acquire(int arg) {
                    // tryAcquire 方法是需要实现类去实现的，实现思路一般都是 cas 给 state 赋值来决定是否能获得锁
                   if (!tryAcquire(arg) &&
                    // addWaiter 入参代表是排他模式
                     acquireQueued(addWaiter(Node.EXCLUSIVE), arg))
                     selfInterrupt();
                 }
                 以上代码作用
                   尝试执行一次 tryAcquire，如果成功直接返回，失败走 2；
                   线程尝试进入同步队列，首先调用 addWaiter 方法，把当前线程放到同步队列的队尾；
                   接着调用 acquireQueued 方法，两个作用，1：阻塞当前节点，2：节点被唤醒时，使其能够获得锁；
                   如果 2、3 失败了，打断线程。

                  addWaiter 
                    // 方法主要目的：node 追加到同步队列的队尾
                    // 入参 mode 表示 Node 的模式（排它模式还是共享模式）
                    // 出参是新增的 node
                    // 新 node.pre = 队尾
                    / / 队尾.next = 新 node
                    // 这里的逻辑和 enq 一致，enq 的逻辑仅仅多了队尾是空，初始化的逻辑
                    先简单的尝试放一下，成功立马返回，如果不行，再 while 循环
                       if (pred != null) {
                     node.prev = pred;
                     if (compareAndSetTail(pred, node)) {
                     pred.next = node;
                     return node;
                     }
                     }
                   acquireQueued
                     下一步就是要阻塞当前线程了，是 acquireQueued 方法来实现的
                     / 主要做两件事情：
                     // 1：通过不断的自旋尝试使自己前一个节点的状态变成 signal，然后阻塞自己。
                     // 2：获得锁的线程执行完成之后，释放锁时，会把阻塞的 node 唤醒,node 唤醒之后再次自旋，尝试获得锁
                    // 返回 false 表示获得锁成功，返回 true 表示失败

                    // 有两种情况会走到 p == head：
                    // 1:node 之前没有获得锁，进入 acquireQueued 方法时，才发现他的前置节点就是头节点，于是尝试获得一次锁；
                    // 2:node 之前一直在阻塞沉睡，然后被唤醒，此时唤醒 node 的节点正是其前一个节点，也能走到 if
                   // 如果自己 tryAcquire 成功，就立马把自己设置成 head，把上一个节点移除
                    // 如果 tryAcquire 失败，尝试进入同步队列
                     if (p == head && tryAcquire(arg)) {
                     // 获得锁，设置成 head 节点
                      setHead(node);
                     //p被回收
                     p.next = null; // help GC
                      failed = false;
                     return interrupted;
                    }
                     // shouldParkAfterFailedAcquire 把 node 的前一个节点状态置为 SIGNAL
                   // 只要前一个节点状态是 SIGNAL了，那么自己就可以阻塞(park)了
                    // parkAndCheckInterrupt 阻塞当前线程
                     if (shouldParkAfterFailedAcquire(p, node) &&
                     // 线程是在这个方法里面阻塞的，醒来的时候仍然在无限 for 循环里面，就能再次自旋尝试获得锁
                     parkAndCheckInterrupt())
                       interrupted = true;
                   }

                   shouldParkAfterFailedAcquire，这个方法的主要目的就是把前一个节点的状态置为 SIGNAL，只要前一个节点的状态是 SIGNAL，当前节点就可以阻塞了（parkAndCheckInterrupt 就是使节点阻塞的方法）

                   // 当前线程可以安心阻塞的标准，就是前一个节点线程状态是 SIGNAL 了。
                  // 入参 pred 是前一个节点，node 是当前节点。

                  / 关键操作：
                  // 1：确认前一个节点是否有效，无效的话，一直往前找到状态不是取消的节点。
                  // 2: 把前一个节点状态置为 SIGNAL。
                  // 1、2 两步操作，有可能一次就成功，有可能需要外部循环多次才能成功（外面是个无限的 for 循环），但最后一定是可以成功的
                  private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) {
                     // 如果当前节点状态已经被取消了。
                   if (ws > 0) {
        
                    // 找到前一个状态不是取消的节点，因为把当前 node 挂在有效节点身上
                  // 因为节点状态是取消的话，是无效的，是不能作为 node 的前置节点的，所以必须找到 node 的有效节点才行
                   do {
                      node.prev = pred = pred.prev;
                    } while (pred.waitStatus > 0);
                   pred.next = node;
                   // 否则直接把节点状态置 为SIGNAL
                   } else {
        
                      compareAndSetWaitStatus(pred, ws, Node.SIGNAL);
                    }

                   三步走：
                      使用 tryAcquire  方法尝试获得锁，获得锁直接返回，获取不到锁的走 2；
                      把当前线程组装成节点（Node），追加到同步队列的尾部（addWaiter）；
                      自旋，使同步队列中当前节点的前置节点状态为 signal 后，然后阻塞自己。 
               共享锁
                 排它锁使用的是 tryAcquire 方法，共享锁使用的是 tryAcquireShared 方法       
                  在于节点获得排它锁时，仅仅把自己设置为同步队列的头节点即可（setHead 方法），但如果是共享锁的话，还会去唤醒自己的后续节点，一起来获得该锁（setHeadAndPropagate 方法）
                  // 主要做两件事情
                  // 1:把当前节点设置成头节点
                  // 2:看看后续节点有无正在等待，并且也是共享模式的，有的话唤醒这些节点
                    // propagate > 0 表示已经有节点获得共享锁了
                  if (propagate > 0 || h == null || h.waitStatus < 0 ||
                       (h = head) == null || h.waitStatus < 0) {
                       Node s = node.next;
                     //共享模式，还唤醒头节点的后置节点
                      if (s == null || s.isShared())
                     doReleaseShared();
                   }
                    // 如果队列状态是 SIGNAL ，说明后续节点都需要唤醒
                 if (ws == Node.SIGNAL) {
                 // CAS 保证只有一个节点可以运行唤醒的操作
                 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0))
                    continue;            // loop to recheck cases
                 // 进行唤醒操作
                 unparkSuccessor(h); 

                 这个就是共享锁独特的地方，当一个线程获得锁后，它就会去唤醒排在它后面的其它节点，让其它节点也能够获得锁
              释放锁
                释放锁的触发时机就是我们常用的 Lock.unLock () 方法，目的就是让线程释放对资源的访问权
                释放锁也是分为两类，一类是排它锁的释放，一类是共享锁的释放
               释放排它锁 release
                 排它锁的释放就比较简单了，从队头开始，找它的下一个节点，如果下一个节点是空的，就会从尾开始，一直找到状态不是取消的节点，然后释放该节点  
                  // tryRelease 交给实现类去实现，一般就是用当前同步器状态减去 arg，如果返回 true 说明成功释放锁。
                if (tryRelease(arg)) {
                    Node h = head;
                   // 头节点不为空，并且非初始化状态
                  if (h != null && h.waitStatus != 0)
                  // 从头开始唤醒等待锁的节点
                   unparkSuccessor(h);
                  return true;
                }
                // 很有意思的方法，当线程释放锁成功后，从 node 开始唤醒同步队列中的节点
                // 通过唤醒机制,保证线程不会一直在同步队列中阻塞等待
                 private void unparkSuccessor(Node node) {
                 // 拿出 node 节点的后面一个节点
                 Node s = node.next;
                 // s 为空，表示 node 的后一个节点为空
                  // s.waitStatus 大于0，代表 s 节点已经被取消了
                 // 遇到以上这两种情况，就从队尾开始，向前遍历，找到第一个 waitStatus 字段不是被取消的

                 // 这里从尾迭代，而不是从头开始迭代是有原因的。
                 // 主要是因为节点被阻塞的时候，是在 acquireQueued 方法里面被阻塞的，唤醒时也一定会在 acquireQueued 方法里面被唤醒，唤醒之后的条件是，判断当前节点的前置节点是否是头节点，这里是判断当前节点的前置节点，所以这里必须使用从尾到头的迭代顺序才行，目的就是为了过滤掉无效的前置节点，不然节点被唤醒时，发现其前置节点还是无效节点，就又会陷入阻塞。
                 for (Node t = tail; t != null && t != node; t = t.prev)
                 // t.waitStatus <= 0 说明 t 没有被取消，肯定还在等待被唤醒
                 if (t.waitStatus <= 0)
                   s = t;
                   // 唤醒以上代码找到的线程
                 if (s != null)
                  LockSupport.unpark(s.thread);
                释放共享锁  releaseShared
                   tryReleaseShared 尝试释放当前共享锁，失败返回 false，成功走 2；
                   唤醒当前节点的后续阻塞节点，这个方法我们之前看过了，线程在获得共享锁的时候，就会去唤醒其后面的节点，方法名称为：doReleaseShared。
              条件队列重要方法 
                 主要是因为并不是所有场景一个同步队列就可以搞定的，在遇到锁 + 队列结合的场景时，就需要 Lock + Condition 配合才行，先使用 Lock 来决定哪些线程可以获得锁，哪些线程需要到同步队列里面排队阻塞；获得锁的多个线程在碰到队列满或者空的时候，可以使用 Condition 来管理这些线程，让这些线程阻塞等待，然后在合适的时机后，被正常唤醒。

                 同步队列 + 条件队列联手使用的场景，最多被使用到锁 + 队列的场景中
                 入队列等待 await
                   获得锁的线程，如果在碰到队列满或空的时候，就会阻塞住，这个阻塞就是用条件队列实现的，这个动作我们叫做入条件队列，方法名称为 await
                   // 加入到条件队列的队尾
                   Node node = addConditionWaiter();
                   标记位置A 
                   // 加入条件队列后，会释放 lock 时申请的资源，唤醒同步队列队列头的节点
                   // 自己马上就要阻塞了，必须马上释放之前 lock 的资源，不然自己不被唤醒的话，别的线程永远得不到该共享资源了 
                   int savedState = fullyRelease(node);
                     // 确认node不在同步队列上，再阻塞，如果 node 在同步队列上，是不能够上锁的
                  // 目前想到的只有两种可能：
                  // 1:node 刚被加入到条件队列中，立马就被其他线程 signal 转移到同步队列中去了
                   // 2:线程之前在条件队列中沉睡，被唤醒后加入到同步队列中去
                   标记位置B
                     // 其他线程通过 signal 已经把 node 从条件队列中转移到同步队列中的数据结构中去了
                     // 所以这里节点苏醒了，直接尝试 acquireQueued
                  上述代码标记位置 A 处，节点在准备进入条件队列之前，一定会先释放当前持有的锁，不然自己进去条件队列了，其余的线程都无法获得锁了；
                  上述代码标记位置 B 处，此时节点是被 Condition.signal 或者 signalAll 方法唤醒的，此时节点已经成功的被转移到同步队列中去了（整体架构图中蓝色流程），所以可以直接执行 acquireQueued 方法；
                  Node 在条件队列中的命名，源码喜欢用 Waiter 来命名，所以我们在条件队列中看到 Waiter，其实就是 Node。
                  addConditionWaiter
                   // 增加新的 waiter 到队列中，返回新添加的 waiter
                  // 如果尾节点状态不是 CONDITION 状态，删除条件队列中所有状态不是 CONDITION  的节点
                  // 如果队列为空，新增节点作为队列头节点，否则追加到尾节点上
                  unlinkCancelledWaiters
                   // 会检查尾部的 waiter 是不是已经不是CONDITION状态了
                   // trail 表示上一个状态,这个字段作用非常大，可以把状态都是 CONDITION 的 node 串联起来，即使 node 之间有其他节点都可以
                 单个唤醒signal
                   signal 方法是唤醒的意思，比如之前队列满了，有了一些线程因为 take 操作而被阻塞进条件队列中，突然队列中的元素被线程 A 消费了，线程 A 就会调用 signal 方法，唤醒之前阻塞的线程，会从条件队列的头节点开始唤醒  
                   // doSignal 方法会把条件队列中的节点转移到同步队列中去
                    doSignal(first);
                    // 返回 true 表示转移成功， false 失败
                   // 大概思路：
                  // 1. node 追加到同步队列的队尾
                  // 2. 将 node 的前一个节点状态置为 SIGNAL，成功直接返回，失败直接唤醒
                  // 可以看出来 node 的状态此时是 0 了
                  final boolean transferForSignal(Node node) 
                  唤醒条件队列中的节点，实际上就是把条件队列中的节点转移到同步队列中，并把其前置节点状态置为 SIGNAL。
                 全部唤醒 signalAll
                   其本质就是 for 循环调用 transferForSignal 方法，将条件队列中的节点循环转移到同步队列中去。
          
           -- AtomicInteger 底层实现原理是什么？如何在自己的产品代码中应用 CAS 操作？
               AtomicIntger 是对 int 类型的一个封装，提供原子性的访问和更新操作，其原子性操作的实现是基于 CAS（compare-and-swap）技术。
               所谓 CAS，表征的是一些列操作的集合，获取当前数值，进行一些运算，利用 CAS 指令试图进行更新。如果当前数值未变，代表没有其他线程进行并发修改，则成功更新。否则，可能出现不同的选择，要么进行重试，要么就返回一个成功或者失败的结果
               从 AtomicInteger 的内部属性可以看出，它依赖于 Unsafe 提供的一些底层能力，进行底层操作；以 volatile 的 value 字段，记录数值，以保证可见性。

               类似 compareAndSet 这种返回 boolean 类型的函数，因为其返回值表现的就是成功与否，所以不需要重试。public final boolean compareAndSet(int expectedValue, int newValue)

               CAS 是 Java 并发中所谓 lock-free 机制的基础。
               扩展
                 场景 
                 在数据库产品中，为保证索引的一致性，一个常见的选择是，保证只有一个线程能够排他性地修改一个索引分区，如何在数据库抽象层面实现呢？

                 目前 Java 提供了两种公共 API，可以实现这种 CAS 操作，比如使用 java.util.concurrent.atomic.AtomicLongFieldUpdater，它是基于反射机制创建，我们需要保证类型和字段名称正确。

                 AtomicLongFieldUpdater lockFieldUpdater = AtomicLongFieldUpdater.newUpdater(AtomicBTreePartition.class, "lock");private void acquireLock(){ long t = Thread.currentThread().getId(); while (!lockFieldUpdater.compareAndSet(this, 0L, t)){ // 等待一会儿，数据库操作可能比较慢 … }}

                 Atomic 包提供了最常用的原子性数据类型，甚至是引用、数组等相关原子类型和更新操作工具，是很多线程安全程序的首选。
                 绍使用原子数据类型和 Atomic*FieldUpdater，创建更加紧凑的计数器实现，以替代 AtomicLong。优化永远是针对特定需求、特定目的，我这里的侧重点是介绍可能的思路，具体还是要看需求。如果仅仅创建一两个对象，其实完全没有必要进行前面的优化，但是如果对象成千上万或者更多，就要考虑紧凑性的影响了。而 atomic 包提供的LongAdder，在高度竞争环境下，可能就是比 AtomicLong 更佳的选择，尽管它的本质是空间换时间。

                 如果是 Java 9 以后，我们完全可以采用另外一种方式实现，也就是 Variable Handle API，这是源自于JEP 193，提供了各种粒度的原子或者有序性的操作等

                 private static final VarHandle HANDLE = MethodHandles.lookup().findStaticVarHandle
                 (AtomicBTreePartition.class, "lock");

                 private void acquireLock(){
                 long t = Thread.currentThread().getId();
                      while (!HANDLE.compareAndSet(this, 0L, t)){
                     // 等待一会儿，数据库操作可能比较慢
        
                   }
               cas副作用
                  CAS的缺点，自旋、ABA问题    
                   CAS 也并不是没有副作用，试想，其常用的失败重试机制，隐含着一个假设，即竞争情况是短暂的。大多数应用场景中，确实大部分重试只会发生一次就获得了成功，但是总是有意外情况，所以在有需要的时候，还是要考虑限制自旋的次数，以免过度消耗 CPU。另外一个就是著名的ABA问题，这是通常只在 lock-free 算法下暴露的问题。我前面说过 CAS 是在更新时比较前值，如果对方只是恰好相同，例如期间发生了 A -> B -> A 的更新，仅仅判断数值是 A，可能导致不合理的修改操作。针对这种情况，Java 提供了 AtomicStampedReference 工具类，通过为引用建立类似版本号（stamp）的方式，来保证 CAS 的正确性 
           -- 原子类：无锁工具类的典范
               在这个例子中，add10K() 这个方法不是线程安全的，问题就出在变量 count 的可见性和 count+=1 的原子性上。可见性问题可以用 volatile 来解决，而原子性问题我们前面一直都是采用的互斥锁方案。  

               其实对于简单的原子性问题，还有一种无锁方案。Java SDK 并发包将这种无锁方案封装提炼之后，实现了一系列的原子类。不过，在深入介绍原子类的实现之前，我们先看看如何利用原子类解决累加器问题   

               在下面的代码中，我们将原来的 long 型变量 count 替换为了原子类 AtomicLong，原来的 count +=1 替换成了 count.getAndIncrement()，

               无锁方案相对互斥锁方案，最大的好处就是性能。互斥锁方案为了保证互斥性，需要执行加锁、解锁操作，而加锁、解锁操作本身就消耗性能；同时拿不到锁的线程还会进入阻塞状态，进而触发线程切换，线程切换对性能的消耗也很大。 相比之下，无锁方案则完全没有加锁、解锁的性能消耗，同时还能保证互斥性，既解决了问题，又没有带来新的问题，可谓绝佳方案。

               无锁方案的实现原理 
                  CPU 为了解决并发问题，提供了 CAS 指令（CAS，全称是 Compare And Swap，即“比较并交换”）。CAS 指令包含 3 个参数：共享变量的内存地址 A、用于比较的值 B 和共享变量的新值 C；并且只有当内存中地址 A 处的值等于 B 时，才能将内存中地址 A 处的值更新为新值 C。作为一条 CPU 指令，CAS 指令本身是能够保证原子性的。

                  对于前面提到的累加器的例子，count += 1 的一个核心问题是：基于内存中 count 的当前值 A 计算出来的 count+=1 为 A+1，在将 A+1 写入内存的时候，很可能此时内存中 count 已经被其他线程更新过了，这样就会导致错误地覆盖其他线程写入的值（也就是说，只有当内存中 count 的值等于期望值 A 时，才能将内存中 count 的值更新为计算结果 A+1，这不就是 CAS 的语义吗！

                  使用 CAS 来解决并发问题，一般都会伴随着自旋，而所谓自旋，其实就是循环尝试。例如，实现一个线程安全的count += 1操作，“CAS+ 自旋”的实现方案如下所示，首先计算 newValue = count+1，如果 cas(count,newValue) 返回的值不等于 count，则意味着线程在执行完代码①处之后，执行代码②处之前，count 的值被其他线程更新过。那此时该怎么处理呢？可以采用自旋方案，就像下面代码中展示的，可以重新读 count 最新的值来计算 newValue 并尝试再次更新，直到成功。

                  CAS 这种无锁方案，完全没有加锁、解锁操作，即便两个线程完全同时执行 addOne() 方法，也不会有线程被阻塞，所以相对于互斥锁方案来说，性能好了很多。


                  但是在 CAS 方案中，有一个问题可能会常被你忽略，那就是 ABA 的问题。什么是 ABA 问题呢
                  前面我们提到“如果 cas(count,newValue) 返回的值不等于count，意味着线程在执行完代码①处之后，执行代码②处之前，count 的值被其他线程更新过”，那如果 cas(count,newValue) 返回的值等于count，是否就能够认为 count 的值没有被其他线程更新过呢？显然不是的，假设 count 原本是 A，线程 T1 在执行完代码①处之后，执行代码②处之前，有可能 count 被线程 T2 更新成了 B，之后又被 T3 更新回了 A，这样线程 T1 虽然看到的一直是 A，但是其实已经被其他线程更新过了，这就是 ABA 问题。

                  可能大多数情况下我们并不关心 ABA 问题，例如数值的原子递增，但也不能所有情况下都不关心，例如原子化的更新对象很可能就需要关心 ABA 问题，因为两个 A 虽然相等，但是第二个 A 的属性可能已经发生变化了。所以在使用 CAS 方案的时候，一定要先 check 一下。  

               看 Java 如何实现原子化的 count += 1
                  在 Java 1.8 版本中，getAndIncrement() 方法会转调 unsafe.getAndAddLong() 方法。这里 this 和 valueOffset 两个参数可以唯一确定共享变量的内存地址。  

                  unsafe.getAndAddLong() 方法的源码如下，该方法首先会在内存中读取共享变量的值，之后循环调用 compareAndSwapLong() 方法来尝试设置共享变量的值，直到成功为止。compareAndSwapLong() 是一个 native 方法，只有当内存中共享变量的值等于 expected 时，才会将共享变量的值更新为 x，并且返回 true；否则返回 fasle。compareAndSwapLong 的语义和 CAS   指令的语义的差别仅仅是返回值不同而已。 

                  getAndAddLong() 方法的实现，基本上就是 CAS 使用的经典范例。所以请你再次体会下面这段抽象后的代码片段，它在很多无锁程序中经常出现。Java 提供的原子类里面 CAS 一般被实现为 compareAndSet()，compareAndSet() 的语义和 CAS 指令的语义的差别仅仅是返回值不同而已，compareAndSet() 里面如果更新成功，则会返回 true，否则返回 false。  

               原子类概览 
                  原子化的基本数据类型、原子化的对象引用类型、原子化数组、原子化对象属性更新器和原子化的累加器。
                  1. 原子化的基本数据类型
                  2. 原子化的对象引用类型
                     相关实现有 AtomicReference、AtomicStampedReference 和 AtomicMarkableReference，利用它们可以实现对象引用的原子化更新。
                     不过需要注意的是，对象引用的更新需要重点关注 ABA 问题，AtomicStampedReference 和 AtomicMarkableReference 这两个原子类可以解决 ABA 问题。 
                     解决 ABA 问题的思路其实很简单，增加一个版本号维度就可以了，介绍的乐观锁机制很类似，每次执行 CAS 操作，附加再更新一个版本号，只要保证版本号是递增的，那么即便 A 变成 B 之后再变回 A，版本号也不会变回来（版本号递增的）。
                  3. 原子化数组   

                     相关实现有 AtomicIntegerArray、AtomicLongArray 和 AtomicReferenceArray，利用这些原子类，我们可以原子化地更新数组里面的每一个元素。这些类提供的方法和原子化的基本数据类型的区别仅仅是：每个方法多了一个数组的索引参数  
                  4. 原子化对象属性更新器 
                      
                     相关实现有 AtomicIntegerFieldUpdater、AtomicLongFieldUpdater 和 AtomicReferenceFieldUpdater，利用它们可以原子化地更新对象的属性，这三个方法都是利用反射机制实现的，创建更新器的方法如下     
                     
                     public static <U>AtomicXXXFieldUpdater<U> newUpdater(Class<U> tclass,   String fieldName)
   
                     需要注意的是，对象属性必须是 volatile 类型的，只有这样才能保证可见性；如果对象属性不是 volatile 类型的，newUpdater() 方法会抛出 IllegalArgumentException 这个运行时异常。

                     你会发现 newUpdater() 的方法参数只有类的信息，没有对象的引用，而更新对象的属性，一定需要对象的引用，那这个参数是在哪里传入的呢？是在原子操作的方法参数中传入的。例如 compareAndSet() 这个原子操作，相比原子化的基本数据类型多了一个对象引用 obj。原子化对象属性更新器相关的方法，相比原子化的基本数据类型仅仅是多了对象引用参数  
                  5. 原子化的累加器
                    DoubleAccumulator、DoubleAdder、LongAccumulator 和 LongAdder，这四个类仅仅用来执行累加操作，相比原子化的基本数据类型，速度更快，但是不支持 compareAndSet() 方法。如果你仅仅需要累加操作，使用原子化的累加器性能会更好   



           -- RWLock、Condition、LockSupport、StampedLock
           -- ReadWriteLock：如何快速实现一个完备的缓存 
              那 Java SDK 并发包里为什么还有很多其他的工具类呢？原因很简单：分场景优化性能，提升易用性。
              读多写少场景。实际工作中，为了优化性能，我们经常会使用缓存，例如缓存元数据、缓存基础数据等，这就是一种典型的读多写少应用场景。缓存之所以能提升性能，一个重要的条件就是缓存的数据一定是读多写少的，例如元数据和基础数据基本上不会发生变化（写少），但是使用它们的地方却很多（读多）
              针对读多写少这种并发场景，Java SDK 并发包提供了读写锁——ReadWriteLock，非常容易使用，并且性能很好。
              那什么是读写锁呢？  
                 三个基本原则  
                  1允许多个线程同时读共享变量；
                  2只允许一个线程写共享变量；
                  3如果一个写线程正在执行写操作，此时禁止读线程读共享变量。
                 读写锁与互斥锁的一个重要区别就是读写锁允许多个线程同时读共享变量，而互斥锁是不允许的，这是读写锁在读多写少场景下性能优于互斥锁的关键。但读写锁的写操作是互斥的，当一个线程在写共享变量的时候，是不允许其他线程执行写操作和读操作。   
              快速实现一个缓存
                我们声明了一个 Cache<K, V> 类，其中类型参数 K 代表缓存里 key 的类型，V 代表缓存里 value 的类型。缓存的数据保存在 Cache 类内部的 HashMap 里面，HashMap 不是线程安全的，这里我们使用读写锁 ReadWriteLock 来保证其线程安全。ReadWriteLock 是一个接口，它的实现类是   ReentrantReadWriteLock，通过名字你应该就能判断出来，它是支持可重入的。下面我们通过 rwl 创建了一把读锁和一把写锁。
                如果你曾经使用过缓存的话，你应该知道使用缓存首先要解决缓存数据的初始化问题。缓存数据的初始化，可以采用一次性加载的方式，也可以使用按需加载的方式。

                如果源头数据的数据量不大，就可以采用一次性加载的方式，这种方式最简单（可参考下图），只需在应用启动的时候把源头数据查询出来，依次调用类似上面示例代码中的 put() 方法就可以了
               
                如果源头数据量非常大，那么就需要按需加载了，按需加载也叫懒加载，指的是只有当应用查询缓存，并且数据不在缓存里的时候，才触发加载源头相关数据进缓存的操作。下面你可以结合文中示意图看看如何利用 ReadWriteLock 来实现缓存的按需加载      
              实现缓存的按需加载

                 原因是在高并发的场景下，有可能会有多线程竞争写锁。假设缓存是空的，没有缓存任何东西，如果此时有三个线程 T1、T2 和 T3 同时调用 get() 方法，并且参数 key 也是相同的。那么它们会同时执行到代码⑤处，但此时只有一个线程能够获得写锁，假设是线程 T1，线程 T1 获取写锁之后查询数据库并更新缓存，最终释放写锁。此时线程 T2 和 T3 会再有一个线程能够获取写锁，假设是 T2，如果不采用再次验证的方式，此时 T2 会再次查询数据库。T2 释放写锁之后，T3 也会再次查询一次数据库。而实际上线程 T1 已经把缓存的值设置好了，T2、T3 完全没有必要再次查询数据库。所以，再次验证的方式，能够避免高并发场景下重复查询数据的问题。  
              读写锁的升级与降级 

                 这样看上去好像是没有问题的，先是获取读锁，然后再升级为写锁，对此还有个专业的名字，叫锁的升级。可惜 ReadWriteLock 并不支持这种升级。在上面的代码示例中，读锁还没有释放，此时获取写锁，会导致写锁永久等待，最终导致相关线程都被阻塞，永远也没有机会被唤醒。锁的升级是不允许的，这个你一定要注意。  

                   // 获取读锁    r.lock();    if (!cacheValid) {      // 释放读锁，因为不允许读锁的升级      r.unlock();      // 获取写锁      w.lock();      try {        // 再次检查状态          if (!cacheValid) {          data = ...          cacheValid = true;        }        // 释放写锁前，降级为读锁        // 降级是可以的        r.lock(); ① 

                   那就是只有写锁支持条件变量，读锁是不支持条件变量的，读锁调用 newCondition() 会抛出 UnsupportedOperationException 异常。  
           -- StampedLock：有没有比读写锁更快的锁？    
               Java 在 1.8 这个版本里，提供了一种叫 StampedLock 的锁，它的性能就比读写锁还要好。
               StampedLock 支持的三种锁模式
                 ReadWriteLock 支持两种模式：一种是读锁，一种是写锁。
                 而 StampedLock 支持三种模式，分别是：写锁、悲观读锁和乐观读。 
                 其中，写锁、悲观读锁的语义和 ReadWriteLock 的写锁、读锁的语义非常类似，允许多个线程同时获取悲观读锁，但是只允许一个线程获取写锁，写锁和悲观读锁是互斥的。不同的是：StampedLock 里的写锁和悲观读锁加锁成功之后，都会返回一个 stamp；然后解锁的时候，需要传入这个 stamp  

                 // 获取/释放悲观读锁示意代码long stamp = sl.readLock();try {  //省略业务相关代码} finally {  sl.unlockRead(stamp);}     

                 StampedLock 的性能之所以比 ReadWriteLock 还要好，其关键是 StampedLock 支持乐观读的方式。ReadWriteLock 支持多个线程同时读，但是当多个线程同时读的时候，所有的写操作会被阻塞；而 StampedLock 提供的乐观读，是允许一个线程获取写锁的，也就是说不是所有的写操作都被阻塞。

                 乐观读这个操作是无锁的，所以相比较 ReadWriteLock 的读锁，乐观读的性能更好一些。

                 在 distanceFromOrigin() 这个方法中，首先通过调用 tryOptimisticRead() 获取了一个 stamp，这里的 tryOptimisticRead() 就是我们前面提到的乐观读。之后将共享变量 x 和 y 读入方法的局部变量中，不过需要注意的是，由于 tryOptimisticRead() 是无锁的，所以共享变量 x 和 y 读入方法局部变量时，x 和 y 有可能被其他线程修改了。因此最后读完之后，还需要再次验证一下是否存在写操作，这个验证操作是通过调用 validate(stamp) 来实现的。

                 //则sl.validate返回false    if (!sl.validate(stamp)){      // 升级为悲观读锁      stamp = sl.readLock();      try {        curX = x;        curY = y;      } finally {        //释放悲观读锁        sl.unlockRead(stamp);      }    }

                 如果执行乐观读操作的期间，存在写操作，会把乐观读升级为悲观读锁。这个做法挺合理的，否则你就需要在一个循环里反复执行乐观读，直到执行乐观读操作的期间没有写操作（只有这样才能保证 x 和 y 的正确性和一致性），而循环读会浪费大量的 CPU。升级为悲观读锁，代码简练且不易出错，建议你在具体实践时也采用这样的方法。 
               进一步理解乐观读 
                  如果你曾经用过数据库的乐观锁，可能会发现 StampedLock 的乐观读和数据库的乐观锁有异曲同工之妙。的确是这样的，就拿我个人来说，我是先接触的数据库里的乐观锁，然后才接触的 StampedLock，我就觉得我前期数据库里乐观锁的学习对于后面理解 StampedLock 的乐观读有很大帮助，所以这里有必要再介绍一下数据库里的乐观锁 

                  你会发现数据库里的乐观锁，查询的时候需要把 version 字段查出来，更新的时候要利用 version 字段做验证。这个 version 字段就类似于 StampedLock 里面的 stamp。这样对比着看，相信你会更容易理解 StampedLock 里乐观读的用法 
               StampedLock 使用注意事项 
                 对于读多写少的场景 StampedLock 性能很好，简单的应用场景基本上可以替代 ReadWriteLock，但是 StampedLock 的功能仅仅是 ReadWriteLock 的子集，在使用的时候，还是有几个地方需要注意一下。

                 StampedLock 在命名上并没有增加 Reentrant，想必你已经猜测到 StampedLock 应该是不可重入的。事实上，的确是这样的，StampedLock 不支持重入。这个是在使用中必须要特别注意的。

                 另外，StampedLock 的悲观读锁、写锁都不支持条件变量，这个也需要你注意。还有一点需要特别注意，那就是：如果线程阻塞在 StampedLock 的 readLock() 或者 writeLock() 上时，此时调用该阻塞线程的 interrupt() 方法，会导致 CPU 飙升。例如下面的代码中，线程 T1 获取写锁之后将自己阻塞，线程 T2 尝试获取悲观读锁，也会阻塞；如果此时调用线程 T2 的 interrupt() 方法来中断线程 T2 的话，你会发现线程 T2 所在 CPU 会飙升到 100%。

                 使用 StampedLock 一定不要调用中断操作，如果需要支持中断功能，一定使用可中断的悲观读锁 readLockInterruptibly() 和写锁 writeLockInterruptibly() 
           -- CountDownLatch和CyclicBarrier：如何让多线程步调一致      
              
              利用并行优化对账系统 

                在下面的代码中，我们创建了两个线程 T1 和 T2，并行执行查询未对账订单 getPOrders() 和查询派送单 getDOrders() 这两个操作。在主线程中执行对账操作 check() 和差异写入 save() 两个操作。不过需要注意的是：主线程需要等待线程 T1 和 T2 执行完才能执行 check() 和 save() 这两个操作，为此我们通过调用 T1.join() 和 T2.join() 来实现等待，当 T1 和 T2 线程退出时，调用 T1.join() 和 T2.join() 的主线程就会从阻塞态被唤醒，从而执行之后的 check() 和 save()。  
                在下面的代码中，我们创建了两个线程 T1 和 T2，并行执行查询未对账订单 getPOrders() 和查询派送单 getDOrders() 这两个操作。在主线程中执行对账操作 check() 和差异写入 save() 两个操作。不过需要注意的是：主线程需要等待线程 T1 和 T2 执行完才能执行 check() 和 save() 这两个操作，为此我们通过调用 T1.join() 和 T2.join() 来实现等待，当 T1 和 T2 线程退出时，调用 T1.join() 和 T2.join() 的主线程就会从阻塞态被唤醒，从而执行之后的 check() 和 save()。    
              用 CountDownLatch 实现线程等待   

                经过上面的优化之后，基本上可以跟老板汇报收工了，但还是有点美中不足，相信你也发现了，while 循环里面每次都会创建新的线程，而创建线程可是个耗时的操作。所以最好是创建出来的线程能够循环利用，估计这时你已经想到线程池了，是的，线程池就能解决这个问题。

                而下面的代码就是用线程池优化后的：我们首先创建了一个固定大小为 2 的线程池，之后在 while 循环里重复利用。一切看上去都很顺利，但是有个问题好像无解了，那就是主线程如何知道 getPOrders() 和 getDOrders() 这两个操作什么时候执行完。前面主线程通过调用线程 T1 和 T2 的 join() 方法来等待线程 T1 和 T2 退出，但是在线程池的方案里，线程根本就不会退出，所以 join() 方法已经失效了

                你可以开动脑筋想出很多办法，最直接的办法是弄一个计数器，初始值设置成 2，当执行完pos = getPOrders();这个操作之后将计数器减 1，执行完dos = getDOrders();之后也将计数器减 1，在主线程里，等待计数器等于 0；当计数器等于 0 时，说明这两个查询操作执行完了。等待计数器等于 0 其实就是一个条件变量，用管程实现起来也很简单。
  
                因为 Java 并发包里已经提供了实现类似功能的工具类：CountDownLatch，我们直接使用就可以了。下面的代码示例中，在 while 循环里面，我们首先创建了一个 CountDownLatch，计数器的初始值等于 2，之后在pos = getPOrders();和dos = getDOrders();两条语句的后面对计数器执行减 1 操作，这个对计数器减 1 的操作是通过调用 latch.countDown(); 来实现的。在主线程中，我们通过调用 latch.await() 来实现对计数器等于 0 的等待

                 // 计数器初始化为2  CountDownLatch latch =     new CountDownLatch(2);  // 查询未对账订单  executor.execute(()-> {    pos = getPOrders();    latch.countDown();  });  // 查询派送单  executor.execute(()-> {    dos = getDOrders();    latch.countDown();  });

                 // 等待两个查询操作结束  latch.await();   
              进一步优化性能
              用 CyclicBarrier 实现线程同步
                一个是线程 T1 和 T2 要做到步调一致，另一个是要能够通知到线程 T3。你依然可以利用一个计数器来解决这两个难点，计数器初始化为 2，线程 T1 和 T2 生产完一条数据都将计数器减 1，如果计数器大于 0 则线程 T1 或者 T2 等待。如果计数器等于 0，则通知线程 T3，并唤醒等待的线程 T1 或者 T2，与此同时，将计数器重置为 2，这样线程 T1 和线程 T2 生产下一条数据的时候就可以继续使用这个计数器了。
                 
                 CyclicBarrier。在下面的代码中，我们首先创建了一个计数器初始值为 2 的 CyclicBarrier，你需要注意的是创建 CyclicBarrier 的时候，我们还传入了一个回调函数，当计数器减到 0 的时候，会调用这个回调函数。  

                 线程 T1 负责查询订单，当查出一条时，调用 barrier.await() 来将计数器减 1，同时等待计数器变成 0；线程 T2 负责查询派送单，当查出一条时，也调用 barrier.await() 来将计数器减 1，同时等待计数器变成 0；当 T1 和 T2 都调用 barrier.await() 的时候，计数器会减到 0，此时 T1 和 T2 就可以执行下一条语句了，同时会调用 barrier 的回调函数来执行对账操作。

                 非常值得一提的是，CyclicBarrier 的计数器有自动重置的功能，当减到 0 的时候，会自动重置你设置的初始值。这个功能用起来实在是太方便了。


                 final CyclicBarrier barrier =  new CyclicBarrier(2, ()->{    executor.execute(()->check());  });

                 区别和联系
                   CountDownLatch 主要用来解决一个线程等待多个线程的场景，可以类比旅游团团长要等待所有的游客到齐才能去下一个景点；而 CyclicBarrier 是一组线程之间互相等待，更像是几个驴友之间不离不弃。除此之外 CountDownLatch 的计数器是不能循环利用的，也就是说一旦计数器减到 0，再有线程调用 await()，该线程会直接通过。但 CyclicBarrier 的计数器是可以循环利用的，而且具备自动重置的功能，一旦计数器减到 0 会自动重置到你设置的初始值。除此之外，CyclicBarrier 还可以设置回调函数，可以说是功能丰富。
        -- 线程
          --线程源码解析
             类注释
               Thread
                 每个线程都有优先级，高优先级的线程可能会优先执行；
                 父线程创建子线程后，优先级、是否是守护线程等属性父子线程是一致的；
                 JVM 启动时，通常都启动 MAIN 非守护线程，以下任意一个情况发生时，线程就会停止：
                 退出方法被调用，并且安全机制允许这么做（比如调用 Thread.interrupt 方法）；
                 所有非守护线程都消亡，或者从运行的方法正常返回，或者运行的方法抛出了异常；
                 每个线程都有名字，多个线程可能具有相同的名字，Thread 有的构造器如果没有指定名字，会自动生成一个名字
             线程的基本概念
                线程的状态


                  NEW 表示线程创建成功，但没有运行，在 new Thread 之后，没有 start 之前，线程的状态都是 NEW；
                 当我们运行 strat 方法，子线程被创建成功之后，子线程的状态变成 RUNNABLE，RUNNABLE 表示线程正在运行中；
                 子线程运行完成、被打断、被中止，状态都会从 RUNNABLE 变成 TERMINATED，TERMINATED 表示线程已经运行结束了；
                 如果线程正好在等待获得 monitor lock 锁，比如在等待进入 synchronized 修饰的代码块或方法时，会从 RUNNABLE 变成 BLOCKED，BLOCKED 表示阻塞的意思；
                  WAITING 和 TIMED_WAITING 类似，都表示在遇到 Object#wait、Thread#join、LockSupport#park 这些方法时，线程就会等待另一个线程执行完特定的动作之后，才能结束等待，只不过 TIMED_WAITING 是带有等待时间的（可以看下面的 join 方法的 demo）。
                优先级  
                  优先级代表线程执行的机会的大小，优先级高的可能先执行，低的可能后执行，在 Java 源码中，优先级从低到高分别是 1 到 10，线程默认 new 出来的优先级都是 5
                   // 普通优先级，也是默认的
                   public final static int NORM_PRIORITY = 5;
                守护线程
                   我们默认创建的线程都是非守护线程。创建守护线程时，需要将 Thread 的 daemon 属性设置成 true，守护线程的优先级很低，当 JVM 退出时，是不关心有无守护线程的，即使还有很多守护线程，JVM 仍然会退出，我们在工作中，可能会写一些工具做一些监控的工作，这时我们都是用守护子线程去做，这样即使监控抛出异常，但因为是子线程，所以也不会影响到业务主线程，因为是守护线程，所以 JVM 也无需关注监控是否正在运行，该退出就退出，所以对业务不会产生任何影响。
                ClassLoader   
                   ClassLoader 我们可以简单理解成类加载器，就是把类从文件、二进制数组、URL 等位置加载成可运行 Class。
             线程两种初始化方式
               继承 Thread，成为 Thread 的子类（调用 start 方法即可，会自动调用到 run 方法的）
                start底层源码
                   // 这里会创建一个新的线程，执行完成之后，新的线程已经在运行了，既 target 的内容已经在运行了
                  start0();
               实现 Runnable 接口，作为 Thread 的入参  
                 // 开一个子线程去执行
                 thread.start();
                // 不会新起线程，是在当前主线程上继续运行（// 简单的运行，不会新起线程，target 是 Runnable）
                 thread.run(); 
             线程初始化
               // 无参构造器，线程名字自动生成   Thread() {
                 init(null, null, "Thread-" + nextThreadNum(), 0);
                }  
                // g 代表线程组，线程组可以对组内的线程进行批量的操作，比如批量的打断 interrupt
                // target 是我们要运行的对象
                 // name 我们可以自己传，如果不传默认是 "Thread-" + nextThreadNum()，nextThreadNum 方法返回的是自增的数字
                // stackSize 可以设置堆栈的大小
                 private void init(ThreadGroup g, Runnable target, String name,
                  long stackSize, AccessControlContext acc) 
                    // 当前线程作为父线程
                    Thread parent = currentThread();
                    this.group = g;
                    // 子线程会继承父线程的守护属性
                    this.daemon = parent.isDaemon();
                     // 子线程继承父线程的优先级属性
                      this.priority = parent.getPriority();
                    // 当父线程的 inheritableThreadLocals 的属性值不为空时
                     // 会把 inheritableThreadLocals 里面的值全部传递给子线程
                    if (parent.inheritableThreadLocals != null)
                       this.inheritableThreadLocals =
                  ThreadLocal.createInheritedMap(parent.inheritableThreadLocals);  
             线程其他操作
               join join 的意思就是当前线程等待另一个线程执行完成之后，才能继续操作
                  // 开一个子线程去执行
                 thread.start();
                 // 当前主线程等待子线程执行完成之后再执行
                 thread.join();
                 执行的结果，就是主线程在执行 thread.join (); 代码后会停住，会等待子线程沉睡 30 秒后再执行，这里的 join 的作用就是让主线程等待子线程执行完成
                 主线程一直等待子线程沉睡 30s 后才继续执行，在等待期间，主线程的状态也是 TIMED_WAITING。
               yield 是个 native 方法
                  意思是当前线程做出让步，放弃当前 cpu，让 cpu 重新选择线程，避免线程过度使用 cpu，我们在写 while 死循环的时候，预计短时间内 while 死循环可以结束的话，可以在循环里面使用 yield 方法，防止 cpu 一直被 while 死循环霸占。
                 有点需要说明的是，让步不是绝不执行，重新竞争时，cpu 也有可能重新选中自己 
               sleep  
                   sleep 也是 native 方法，可以接受毫秒的一个入参，也可以接受毫秒和纳秒的两个入参，意思是当前线程会沉睡多久，沉睡时不会释放锁资源，所以沉睡时，其它线程是无法得到锁的。
               interrupt interrupt 中文是打断的意思，意思是可以打断中止正在运行的线程
                   Object#wait ()、Thread#join ()、Thread#sleep (long) 这些方法运行后，线程的状态是 WAITING 或 TIMED_WAITING，这时候打断这些线程，就会抛出 InterruptedException 异常，使线程的状态直接到 TERMINATED；
                    如果 I/O 操作被阻塞了，我们主动打断当前线程，连接会被关闭，并抛出 ClosedByInterruptException 异常；
                    // 开一个子线程去执行
                    thread.start();
                     Thread.sleep(1000L);
                    log.info("主线程等待 1s 后，发现子线程还没有运行成功，打断子线程");
                      thread.interrupt();
          --Future、ExecutorService 源码解析 
             整体架构
               future&runnable->runnableFuture
               callable&Execuotrs->RunnableAdapter
               我们往线程池里面提交一个有返回值的线程，代码如下：
                  // 首先我们创建了一个线程池
                   ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 3, 0L, TimeUnit.MILLISECONDS,
                                                     new LinkedBlockingQueue<>());
                   // futureTask 我们叫做线程任务，构造器的入参是 Callable
                   FutureTask futureTask = new FutureTask(new Callable<String> 
                  // 把任务提交到线程池中，线程池会分配线程帮我们执行任务
                  executor.submit(futureTask); 
                  // 得到任务执行的结果
                  String result = (String) futureTask.get();

                  以上代码作用
                    Callable 定义我们需要做的事情，是可以有返回值的；
                    FutureTask 我们叫做任务，入参是 Callable，是对 Callable 的包装，方便线程池的使用；
                     
                     最后通过 FutureTask.get() 得到子线程的计算结果。
             Callable
               Callable 是一个接口，约定了线程要做的事情，和 Runnable 一样，不过这个线程任务是有返回值的
               public interface Callable<V> {
                 V call() throws Exception;
                返回值是一个泛型，可以定义成任何类型，但我们使用的时候，都不会直接使用 Callable，而是会结合 FutureTask 一起使用
             FutureTask
               FutureTask 我们可以当做是线程运行的具体任务，从上图中，我们可以看到 FutureTask 实现了 RunnableFuture 接口，源码如下
               public class FutureTask<V> implements RunnableFuture<V> {
                而 RunnableFuture 又实现了 Runnable, Future 两个接口，接下来我们先看 Future，再看 RunnableFuture，最后看 FutureTask。
             Future 
               我们刚才说 Callable 是可以返回子线程执行结果的，在获取结果的时候，就需要用到 Future 接口了。
               Future 接口注释上写了这些：
                 定义了异步计算的接口，提供了计算是否完成的 check、等待完成和取回等多种方法；
                 如果想得到结果可以使用 get 方法，此方法(无参方法)会一直阻塞到异步任务计算完成；
                 取消可以使用 cancel 方法，但一旦任务计算完成，就无法被取消了。
                 定义的一些接口
                    // 如果任务已经成功了，或已经取消了，是无法再取消的，会直接返回取消成功(true)
                    // 如果任务还没有开始进行时，发起取消，是可以取消成功的。
                    // 如果取消时，任务已经在运行了，mayInterruptIfRunning 为 true    的话，就可以打断运行中的线程
                    // mayInterruptIfRunning 为 false，表示不能打断直接返回
                    boolean cancel(boolean mayInterruptIfRunning);

                     // 返回线程是否已经被取消了，true 表示已经被取消了
                     // 如果线程已经运行结束了，isCancelled 和 isDone 返回的都是 true
                      boolean isCancelled();
                    // 线程是否已经运行结束了
                     boolean isDone(); 
                      // 等待结果返回
                     // 如果任务被取消了，抛 CancellationException 异常
                     // 如果等待过程中被打断了，抛 InterruptedException 异常
                     V get() throws InterruptedException, ExecutionException;
             RunnableFuture
               RunnableFuture 也是一个接口，
               RunnableFuture 接口的最大目的，是让 Future 可以对 Runnable 进行管理，可以取消 Runnable，查看 Runnable 是否完成等等
             统一 Callable 和 Runnable (FutureTask) 
               所以 FutureTask 出现了，FutureTask 实现了 RunnableFuture 接口，又集合了 Callable（Callable 是 FutureTask 的属性），还提供了两者一系列的转化方法，这样 FutureTask 就统一了 Callable 和 Runnable
                public class FutureTask<V> implements RunnableFuture<V>
               从类定义上可以看出来 FutureTask 实现了 RunnableFuture 接口，也就是说间接实现了 Runnnable 接口（RunnableFuture 实现了 Runnnable 接口），就是说 FutureTask 本身就是个 Runnnable，同时 FutureTask 也实现了 Future，也就是说 FutureTask 具备对任务进行管理的功能（Future 具备对任务进行管理的功能）
             FutureTask 的属性  
               // 任务状态
               private volatile int state;
               private static final int NEW          = 0;//线程任务创建
               private static final int COMPLETING   = 1;//任务执行中
               private static final int NORMAL       = 2;//任务执行结束
               private static final int EXCEPTIONAL  = 3;//任务异常
               private static final int CANCELLED    = 4;//任务取消成功
               private static final int INTERRUPTING = 5;//任务正在被打断中
               private static final int INTERRUPTED  = 6;//任务被打断成功

               // 组合了 Callable 
               private Callable<V> callable;
               / / 异步线程返回的结果
               private Object outcome; 
               // 当前任务所运行的线程
               private volatile Thread runner;
               // 记录调用 get 方法时被等待的线程
               private volatile WaitNode waiters;
             FutureTask 的构造器
               // 使用 Runnable 初始化，并传入 result 作为返回结果。
               // Runnable 是没有返回值的，所以 result 一般没有用，置为 null 就好了
                  public FutureTask(Runnable runnable, V result) {
                // Executors.callable 方法把 runnable 适配成 RunnableAdapter，RunnableAdapter 实现了 callable，所以也就是把 runnable 直接适配成了 callable。
                this.callable = Executors.callable(runnable, result);
               this.state = NEW;       // ensure visibility of callable
               }

               我们注意到入参是 Runnable 的构造器，会使用 Executors.callable 方法来把 Runnnable 转化成 Callable，Runnnable 和 Callable 两者都是接口，两者之间是无法进行转化的，所以 Java 新建了一个转化类：RunnableAdapter 来进行转化

               首先 RunnableAdapter 实现了 Callable，所以 RunnableAdapter 就是 Callable；
               其次 Runnable 是 RunnableAdapter 的一个属性，在构造 RunnableAdapter 的时候会传进来，并且在 call 方法里面调用 Runnable 的 run 方法。

               我们要把 Runnable 适配成 Callable，首先要实现 Callable 的接口，接着在 Callable 的 call 方法里面调用被适配对象（Runnable）的方法。
             FutureTask 对 Future 接口方法的实现 
               get 源码实现
                 // 当前任务已经执行完了，返回
                 if (s > COMPLETING) {
                  // 如果正在执行，当前线程让出 cpu，重新竞争，防止 cpu 飙高
                  else if (s == COMPLETING) // cannot time out yet
                 Thread.yield();
                 // 如果第一次运行，新建 waitNode，当前线程就是 waitNode 的属性
                  else if (q == null)
                  q = new WaitNode()
                  // 默认第一次都会执行这里，执行成功之后，queued 就为 true，就不会再执行了
                 // 把当前 waitNode 当做 waiters 链表的第一个
                 else if (!queued)
                    queued = UNSAFE.compareAndSwapObject(this, waitersOffset,
                                                 q.next = waiters, q);
                 // 如果设置了超时时间，并过了超时时间的话，从 waiters 链表中删除当前 wait
                 else if (timed) {
                     nanos = deadline - System.nanoTime();
                  if (nanos <= 0L) {
                   removeWaiter(q);  
                   // 没有设置超时时间，进入 WAITING 状态
                  else
                   LockSupport.park(this);  
                 get 方法虽然名字叫做 get，但却做了很多 wait 的事情，当发现任务还在进行中，没有完成时，就会阻塞当前进程，等待任务完成后再返回结果值。阻塞底层使用的是 LockSupport.park 方法，使当前线程进入 WAITING 或 TIMED_WAITING 状态。                               
               run 
                // 调用执行
                result = c.call();
                // 给 outcome 赋值
                if (ran)
                set(result);
                run 方法是没有返回值的，通过给 outcome 属性赋值（set(result)），get 时就能从 outcome 属性中拿到返回值；
                FutureTask 两种构造器，最终都转化成了 Callable，所以在 run 方法执行的时候，只需要执行 Callable 的 call 方法即可，在执行 c.call() 代码时，如果入参是 Runnable 的话， 调用路径为 c.call() -> RunnableAdapter.call() -> Runnable.run()，如果入参是 Callable 的话，直接调用。
               cancel 
                // 取消任务，如果正在运行，尝试去打断
                if (!(state == NEW &&//任务状态不是创建 并且不能把 new 状态置为取消，直接返回 false
                 UNSAFE.compareAndSwapInt(this, stateOffset, NEW,
                  mayInterruptIfRunning ? INTERRUPTING : CANCELLED)))
                  return false;  
                // 进行取消操作，打断可能会抛出异常，选择 try finally 的结构 
                 t.interrupt(); 
          --线程源码面试
               创建子线程时，子线程是得不到父线程的 ThreadLocal，有什么办法可以解决这个问题？     
                 可以使用 InheritableThreadLocal 来代替 ThreadLocal，ThreadLocal 和 InheritableThreadLocal 都是线程的属性，所以可以做到线程之间的数据隔离，在多线程环境下我们经常使用，但在有子线程被创建的情况下，父线程 ThreadLocal 是无法传递给子线程的，但 InheritableThreadLocal 可以，主要是因为在线程创建的过程中，会把
                 InheritableThreadLocal 里面的所有值传递给子线程，具体代码如下：
               线程创建有几种实现方式？  
                  第一类是子线程没有返回值，第二类是子线程有返回值。
                  无返回值的线程有两种写法，第一种是继承 Thread 第二种是实现 Runnable 接口，并作为 Thread 构造器的入参
                  这两种都会开一个子线程去执行任务，并且是没有返回值的，如果需要子线程有返回值，需要使用 Callable 接口，但 Callable 接口是无法直接作为 Thread 构造器的入参的，必须结合 FutureTask 一起使用
                      FutureTask futureTask = new FutureTask(new Callable<String> ()
                      new Thread(futureTask).start();
                      log.info("返回的结果是 {}",futureTask.get());
                   把 FutureTask 作为 Thread 的入参就可以了，FutureTask 组合了 Callable ，使我们可以使用 Callable，并且 FutureTask 实现了 Runnable 接口，使其可以作为 Thread 构造器的入参，还有 FutureTask 实现了 Future，使其对任务有一定的管理功能。   
               子线程 1 去等待子线程 2 执行完成之后才能执行，如何去实现？

                 子线程 1 需要等待子线程 2，只需要子线程 1 运行的时候，调用子线程 2 的 join 方法即可，这样线程 1 执行到 join 代码时，就会等待线程 2 执行完成之后，才会继续执行。   
               守护线程和非守护线程的区别？如果我想在项目启动的时候收集代码信息，请问是守护线程好，还是非守护线程好，为什么？
                  两者的主要区别是，在 JVM 退出时，JVM 是不会管守护线程的，只会管非守护线程，如果非守护线程还有在运行的，JVM 就不会退出，如果没有非守护线程了，但还有守护线程的，JVM 直接退出。
                  如果需要在项目启动的时候收集代码信息，就需要看收集工作是否重要了，如果不太重要，又很耗时，就应该选择守护线程，这样不会妨碍 JVM 的退出，如果收集工作非常重要的话，那么就需要非守护进程，这样即使启动时发生未知异常，JVM 也会等到代码收集信息线程结束后才会退出，不会影响收集工作。
               线程 start 和 run 之间的区别

                  调用 Thread.start 方法会开一个新的线程，run 方法不会。   
               Thread、Runnable、Callable 三者之间的区别
                  Thread 实现了 Runnable，本身就是 Runnable，但同时负责线程创建、线程状态变更等操作。
                  Runnable 是无返回值任务接口，Callable 是有返回值任务接口，如果任务需要跑起来，必须需要 Thread 的支持才行，Runnable 和 Callable 只是任务的定义，具体执行还需要靠 Thread 
               线程池 submit 有两个方法，方法一可接受 Runnable，方法二可接受 Callable，但两个方法底层的逻辑却是同一套，这是如何适配的
                  Runnable 和 Callable 是通过 FutureTask 进行统一的，FutureTask 有个属性是 Callable，同时也实现了 Runnable 接口，两者的统一转化是在 FutureTask 的构造器里实现的，FutureTask 的最终目标是把 Runnable 和 Callable 都转化成 Callable，Runnable 转化成 Callable 是通过 RunnableAdapter 适配器进行实现的。
                  线程池的 submit 底层的逻辑只认 FutureTask，不认 Runnable 和 Callable 的差异，所以只要都转化成 FutureTask，底层实现都会是同一套
               Callable 能否丢给 Thread 去执行

                  可以的，可以新建 Callable，并作为 FutureTask 的构造器入参，然后把 FutureTask 丢给 Thread 去执行即可     
               FutureTask 有什么作用
                  组合了 Callable，实现了 Runnable，把 Callable 和 Runnnable 串联了起来。
                  统一了有参任务和无参任务两种定义方式，方便了使用。
                  实现了 Future 的所有方法，对任务有一定的管理功能，比如说拿到任务执行结果，取消任务，打断任务等等。   
               聊聊对 FutureTask 的 get、cancel 方法的理解
                  get 方法主要作用是得到 Callable 异步任务执行的结果，无参 get 会一直等待任务执行完成之后才返回，有参 get 方法可以设定固定的时间，在设定的时间内，如果任务还没有执行成功，直接返回异常，在实际工作中，建议多多使用 get 有参方法，少用 get 无参方法，防止任务执行过慢时，多数线程都在等待，造成线程耗尽的问题。
                  cancel 方法主要用来取消任务，如果任务还没有执行，是可以取消的，如果任务已经在执行过程中了，你可以选择不取消，或者直接打断执行中的任务
               Thread.yield 方法在工作中有什么用
                  yield 方法表示当前线程放弃 cpu，重新参与到 cpu 的竞争中去，再次竞争时，自己有可能得到 cpu 资源，也有可能得不到，这样做的好处是防止当前线程一直霸占 cpu。
                  我们在工作中可能会写一些 while 自旋的代码，如果我们一直 while 自旋，不采取任何手段，我们会发现 cpu 一直被当前 while 循环占用，如果能预见 while 自旋时间很长，我们会设置一定的判断条件，让当前线程陷入阻塞，如果能预见 while 自旋时间很短，我们通常会使用 Thread.yield 方法，使当前自旋线程让步，不一直霸占 cpu，       
               wait()和sleep()的相同点和区别
                  相同点：
                  两者都让线程进入到 TIMED_WAITING 状态，并且可以设置等待的时间。
                  不同点：
                   wait 是 Object 类的方法，sleep 是 Thread 类的方法。
                  sleep 不会释放锁，沉睡的时候，其它线程是无法获得锁的，但 wait 会释放锁。      
        -- ThreadLocal 源码解析  
            ThreadLocal 提供了一种方式，让在多线程环境下，每个线程都可以拥有自己独特的数据，并且可以在整个线程执行过程中，从上而下的传递
             1 用法演示
               * ThreadLocal 中保存的数据是 Map
               static final ThreadLocal<Map<String, String>> context = new ThreadLocal<>();
               // 从上下文中拿出 Map
               Map<String, String> contextMap = context.get();
               context.set(contextMap)
             2 类结构
                类泛型
                  ThreadLocal 定义类时带有泛型，说明 ThreadLocal 可以储存任意格式的数据，
                   public class ThreadLocal<T> {}
                关键属性   
                  // threadLocalHashCode 表示当前 ThreadLocal 的 hashCode，用于计算当前 ThreadLocal 在 ThreadLocalMap 中的索引位置
                  private final int threadLocalHashCode = nextHashCode();
                  // 计算 ThreadLocal 的 hashCode 值(就是递增)
                  private static int nextHashCode() {
                     return nextHashCode.getAndAdd(HASH_INCREMENT);
                   } 
                  // static + AtomicInteger 保证了在一台机器中每个 ThreadLocal 的 threadLocalHashCode 是唯一的
                  // 被 static 修饰非常关键，因为一个线程在处理业务的过程中，ThreadLocalMap 是会被 set  多个 ThreadLocal 的，多个 ThreadLocal 就依靠 threadLocalHashCode 进行区分
                  private static AtomicInteger nextHashCode = new AtomicInteger(); 
                  还有一个重要属性：ThreadLocalMap，当一个线程有多个 ThreadLocal 时，需要一个容器来管理多个 ThreadLocal，ThreadLocalMap 的作用就是这个，管理线程中多个 ThreadLocal。
                ThreadLocalMap 
                  ThreadLocalMap 本身就是一个简单的 Map 结构，key 是 ThreadLocal，value 是 ThreadLocal 保存的值，底层是数组的数据结构  

                  static class ThreadLocalMap {
                  // 数组中的每个节点值，WeakReference 是弱引用，当没有引用指向时，会直接被回收
                 static class Entry extends WeakReference<ThreadLocal<?>> {
                 // 当前 ThreadLocal 关联的值
                 Object value;
                 / WeakReference 的引用 referent 就是 ThreadLocal
                 Entry(ThreadLocal<?> k, Object v) {
                    super(k);
                   value = v;
                 }
                 // 数组的初始化大小
                 private static final int INITIAL_CAPACITY = 16;
                 // 存储 ThreadLocal 的数组
                 private Entry[] table;
                 
                 // 扩容的阈值，默认是数组大小的三分之二
                 private int threshold;

                  ThreadLocalMap 其实就是一个简单的 Map 结构，底层是数组，有初始化大小，也有扩容阈值大小，数组的元素是 Entry，Entry 的 key 就是 ThreadLocal 的引用，value 是 ThreadLocal 的值。
                ThreadLocal 是如何做到线程之间数据隔离的
                  ThreadLocal 是线程安全的，我们可以放心使用，主要因为是 ThreadLocalMap 是线程的属性，我们看下线程 Thread 的源码

                  从上图中，我们可以看到 ThreadLocal.ThreadLocalMap 和 InheritableThreadLocals.ThreadLocalMap 分别是线程的属性，所以每个线程的 ThreadLocals 都是隔离独享的。
                set 方法 
                  // set 操作每个线程都是串行的，不会有线程安全的问题
                  public void set(T value) {
                  Thread t = Thread.currentThread();
                  ThreadLocalMap map = getMap(t);
                  // 当前 thradLocal 之前有设置值，直接设置，否则初始化
                  if (map != null)
                  map.set(this, value); 
                 ThreadLocalMap.set  
                     // 计算 key 在数组中的下标，其实就是 ThreadLocal 的 hashCode 和数组大小-1取余
                     int i = key.threadLocalHashCode & (len-1);
                      // 整体策略：查看 i 索引位置有没有值，有值的话，索引位置 + 1，直到找到没有值的位置
                    // 这种解决 hash 冲突的策略，也导致了其在 get 时查找策略有所不同，体现在 getEntryAfterMiss 中
                    for (Entry e = tab[i];
                      e != null;
                    // nextIndex 就是让在不超过数组长度的基础上，把数组的索引位置 + 1
                    e = tab[i = nextIndex(i, len)]) {
                     ThreadLocal<?> k = e.get();
                    // 找到内存地址一样的 ThreadLocal，直接替换
                    if (k == key) {
                     e.value = value;
                      return;
                 是通过递增的 AtomicInteger 作为 ThreadLocal 的 hashCode 的；
                 计算数组索引位置的公式是：hashCode 取模数组大小，由于 hashCode 不断自增，所以不同的 hashCode 大概率上会计算到同一个数组的索引位置（但这个不用担心，在实际项目中，ThreadLocal 都很少，基本上不会冲突）；
                  通过 hashCode 计算的索引位置 i 处如果已经有值了，会从 i 开始，通过 +1 
                  不断的往后寻找，直到找到索引位置为空的地方，把当前 ThreadLocal 作为 key 放进去。

                get 方法
                   // 从线程中拿到 ThreadLocalMap
                    ThreadLocalMap map = getMap(t);
                   if (map != null) {
                   // 从 map 中拿到 entry，由于 ThreadLocalMap 在 set 时的 hash 冲突的策略不同，导致拿的时候逻辑也不太一样
                   ThreadLocalMap.Entry e = map.getEntry(this);
                   // 如果不为空，读取当前 ThreadLocal 中保存的值
                   if (e != null) {
                       @SuppressWarnings("unchecked")
                       T result = (T)e.value;
                        return result;
                   } 
                   看下 ThreadLocalMap 的 getEntry 方法 
                扩容
                   ThreadLocalMap 中的 ThreadLocal 的个数超过阈值时，ThreadLocalMap 就要开始扩容了
                   扩容后数组大小是原来数组的两倍；
                   扩容时是绝对没有线程安全问题的，因为 ThreadLocalMap 是线程的一个属性，一个线程同一时刻只能对 ThreadLocalMap 进行操作，因为同一个线程执行业务逻辑必然是串行的，那么操作 ThreadLocalMap 必然也是串行的。   
        -- ThreadLocal 在上下文传值场景下的实践 
            ThreadLocal 实现  
              定义 ThreadLocal 上下文工具类
                如果你想往 ThreadLocal 放数据，调用 ContextCache.putAttribute 方法，如果想从 ThreadLocal 拿数据，调用 ContextCache.getAttribute 方法即可。
              开启子线程
                 从打印的日志中，我们发现在子线程中从 ThreadLocal 取值时，并没有取得值，这个原因主要是我们之前说的，线程在创建的时候，并不会把父线程的 ThreadLocal 中的值拷贝给子线程的 ThreadLocal，解决方案就是把 ThreadLocal 修改成 InheritableThreadLocal
            线程池 + ThreadLocal
        -- 线程安全队列 
           - LinkedBlockingQueue 源码解析
              整体架构
                LinkedBlockingQueue 中文叫做链表阻塞队列 底层结构时链表 
                类图
                  iterable-collection-abstractCollection-abstractQueue
                                     -queue       -blockingQueue
                                                   serializable     -linkedBlockingQueue  
                  Queue 是最基础的接口，几乎所有的队列实现类都会实现这个接口，该接口定义出了队列的三大类操作：

                     新增操作：
                       add 队列满的时候抛出异常；
                       offer 队列满的时候返回 false。
                     查看并删除操作：
                       remove 队列空的时候抛异常；
                       poll 队列空的时候返回 null。  
                     只查看不删除操作：
                        element 队列空的时候抛异常；
                        peek 队列空的时候返回 null。 
                     一共 6 种方法，除了以上分类方法，也可以分成两类：
                         遇到队列满或空的时候，抛异常，如 add、remove、element；
                        遇到队列满或空的时候，返回特殊值，如 offer、poll、peek。    
                        一直阻塞  put take
                        阻塞一段时间 offer poll 
                  在新增和查看并删除两大类操作上，BlockingQueue 增加了阻塞的功能，而且可以选择一直阻塞，或者阻塞一段时间后，返回特殊值。      

                  类注释：
                    基于链表的阻塞队列，其底层的数据结构是链表；
                    链表维护先入先出队列，新元素被放在队尾，获取元素从队头部拿；
                    链表大小在初始化的时候可以设置，默认是 Integer 的最大值；
                    可以使用 Collection 和 Iterator 两个接口的所有操作，因为实现了两者的接口。

                  内部构成：
                    LinkedBlockingQueue 内部构成简单来说，分成三个部分：链表存储 + 锁 + 迭代器
                     //链表的元素
                      static class Node<E> {
                      //链表的容量，默认 Integer.MAX_VALUE
                      private final int capacity;  
                      //链表已有元素大小，使用 AtomicInteger，所以是线程安全的
                      private final AtomicInteger count = new AtomicInteger();
                      / 锁 begin
                        private final ReentrantLock takeLock = new ReentrantLock();
                      // take 的条件队列，condition 可以简单理解为基于 ASQ 同步机制建立的条件队列
                      private final Condition notEmpty = takeLock.newCondition();  
                      
                      链表的作用是为了保存当前节点，节点中的数据可以是任意东西，是一个泛型，比如说队列被应用到线程池时，节点就是线程，比如队列被应用到消息队列中，节点就是消息，节点的含义主要看队列被使用的场景；
                        锁有 take 锁和 put 锁，是为了保证队列操作时的线程安全，设计两种锁，是为了 take 和 put 两种操作可以同时进行，互不影响。
                  初始化
                      初始化三种方式
                         指定链表容量大小；
                         不指定链表容量大小，默认是 Integer 的最大值；
                         对已有集合数据进行初始化 

                       源码
                         初始化时，容量大小是不会影响性能的，只影响在后面的使用，因为初始化队列太小，容易导致没有放多少就会报队列已满的错误；
                         在对给定集合数据进行初始化时，源码给了一个不优雅的示范，我们不反对在每次 for 循环的时候，都去检查当前链表的大小是否超过容量，但我们希望在 for 循环开始之前就做一步这样的工作。举个列子，给定集合大小是 1 w，链表大小是 9k，按照现在代码实现，只能在 for 循环 9k 次时才能发现，原来给定集合的大小已经大于链表大小了，导致 9k 次循环都是在浪费资源，还不如在 for 循环之前就 check 一次，如果 1w > 9k，直接报错即可。  
                    阻塞新增
                        // 设置可中断锁
                            putLock.lockInterruptibly()
                         // 队列满了
                        // 当前线程阻塞，等待其他线程的唤醒(其他线程 take 成功后就会唤醒此处被阻塞的线程)
                      while (count.get() == capacity) {
                             // await 无限等待
                           notFull.await();
                       }    

                       往队列新增数据，第一步是上锁，所以新增数据是线程安全的；
                       队列新增数据，简单的追加到链表的尾部即可；
                       新增时，如果队列满了，当前线程是会被阻塞的，阻塞的底层使用是锁的能力，底层实现其它也和队列相关，原理我们在锁章节会说到；
                       新增数据成功后，在适当时机，会唤起 put 的等待线程（队列不满时），或者 take 的等待线程（队列不为空时），这样保证队列一旦满足 put 或者 take 条件时，立马就能唤起阻塞线程，继续运行，保证了唤起的时机不被浪费。

                     阻塞删除
                        删除的原理是怎样的；
                        查看并删除和只查看不删除两种的区别是如何实现的。
                         
                         队列本身就是一个阻塞工具，我们可以把这个工具应用到各种阻塞场景中，比如说队列应用到线程池，当线程池跑满时，我们把新的请求都放到阻塞队列中等待；队列应用到消息队列，当消费者处理能力有限时，我们可以把消息放到队列中等待，让消费者慢慢消费；
           - SynchronousQueue 源码解析
              SynchronousQueue 是比较独特的队列，其本身是没有容量大小，比如我放一个数据到队列中，我是不能够立马返回的，我必须等待别人把我放进去的数据消费掉了，才能够返回。SynchronousQueue 在消息队列技术中间件中被大量使用
              整体架构
                SynchronousQueue 的整体设计比较抽象，在内部抽象出了两种算法实现，一种是先入先出的队列，一种是后入先出的堆栈，两种算法被两个内部类实现，而直接对外的 put，take 方法的实现就非常简单，都是直接调用两个内部类的 transfer 方法进行实现
              类注释
                 队列不存储数据，所以没有大小，也无法迭代；
                 插入操作的返回必须等待另一个线程完成对应数据的删除操作，反之亦然；
                 队列由两种数据结构组成，分别是后入先出的堆栈和先入先出的队列，堆栈是非公平的，队列是公平的。  

              类图
                因为不存储数据结构，有一些方法是没有实现的，比如说 isEmpty、size、contains、remove 和迭代等方法，这些方法都是默认实现
               结构细节
                  // 堆栈 后入先出 非公平
                // Scherer-Scott 算法
                  static final class TransferStack<E> extends Transferer<E> {
                  }

                  // 队列 先入先出 公平
                     static final class TransferQueue<E> extends Transferer<E> {
                 }
                堆栈和队列都有一个共同的接口，叫做 Transferer，该接口有个方法：transfer，该方法很神奇，会承担 take 和 put 的双重功能；
                在我们初始化的时候，是可以选择是使用堆栈还是队列的，如果你不选择，默认的就是堆栈，类注释中也说明了这一点，堆栈的效率比队列更高
             
             非公平的堆栈
               我们有一个大的堆栈池，池的开口叫做堆栈头，put 的时候，就往堆栈池中放数据。take 的时候，就从堆栈池中拿数据，两者操作都是在堆栈头上操作数据，从图中可以看到，越靠近堆栈头，数据越新，所以每次 take 的时候，都会拿到堆栈头的最新数据，这就是我们说的后入先出，也就是非公平的。

               static final class SNode {
                //栈的下一个，就是被当前栈压在下面的栈元素
                     volatile SNode next;
                 //节点匹配，用来判断阻塞栈元素能被唤醒的时机

                 //比如我们先执行 take，此时队列中没有数据，take 被阻塞了，栈元素为 SNode1
                   //当有 put 操作时，会把当前 put 的栈元素赋值给 SNode1 的 match 属性，并唤醒 take 操作
                     //当 take 被唤醒，发现 SNode1 的 match 属性有值时，就能拿到 put 进来的数据，从而返回
                volatile SNode match;
               // 栈元素的阻塞是通过线程阻塞来实现的，waiter 为阻塞的线程
                volatile Thread waiter;
                // 未投递的消息，或者未消费的消息
                  Object item;             
                } 
               入栈和出栈
                 1判断是 put 方法还是 take 方法；
                 2判断栈头数据是否为空，如果为空或者栈头的操作和本次操作一致，是的话走 3，否则走 5；
                 3判断操作有无设置超时时间，如果设置了超时时间并且已经超时，返回 null，否则走 4；
                 4 如果栈头为空，把当前操作设置成栈头，或者栈头不为空，但栈头的操作和本次操作相同，也把当前操作设置成栈头，并看看其它线程能否满足自己，不能满足则阻塞自己。比如当前操作是 take，但队列中没有数据，则阻塞自己；
                 5如果栈头已经是阻塞住的，需要别人唤醒的，判断当前操作能否唤醒栈头，可以唤醒走 6，否则走 4；
                 6把自己当作一个节点，赋值到栈头的 match 属性上，并唤醒栈头节点；
                 7栈头被唤醒后，拿到 match 属性，就是把自己唤醒的节点的信息，返回

                    // 通过 park 进行阻塞，这个我们在锁章节中会说明
                   LockSupport.park(this);
                  
                线程 1 从队列中拿数据，发现队列中没有数据，于是被阻塞，成为 A ；
                线程 2 往队尾 put 数据，会从队尾往前找到第一个被阻塞的节点，假设此时能找到的就是节点 A，然后线程 B 把将 put 的数据放到节点 A 的 item 属性里面，并唤醒线程 1；
                线程 1 被唤醒后，就能从 A.item 里面拿到线程 2 put 的数据了，线程 1 成功返回

                 我们能看出公平主要体现在，每次 put 数据的时候，都 put 到队尾上，而每次拿数据时，并不是直接从堆头拿数据，而是从队尾往前寻找第一个被阻塞的线程，这样就会按照顺序释放被阻塞的线程。


             ArrayBlockingQueue 
			     - DelayQueue源码解析
              延迟队列，意思是延迟执行，并且可以设置延迟多久之后执行，比如设置过 5 秒钟之后再执行，在一些延迟执行的场景被大量使用，比如说延迟对账

              整体设计
                 DelayQueue 延迟队列底层使用的是锁的能力，比如说要在当前时间往后延迟 5 秒执行，那么当前线程就会沉睡 5 秒，等 5 秒后线程被唤醒时，如果能获取到资源的话，线程即可立马执行。
              类注释
                 队列中元素将在过期时被执行，越靠近队头，越早过期；
                 未过期的元素不能够被 take；
                 不允许空元素。  
               类图
                  DelayQueue 的类图和之前的队列一样，不多说，关键是 DelayQueue 类上是有泛型的   
                   public class DelayQueue<E extends Delayed> extends AbstractQueue<E>
                    implements BlockingQueue<E> { 
                  从泛型中可以看出，DelayQueue 中的元素必须是 Delayed 的子类，Delayed 是表达延迟能力的关键接口，其继承了 Comparable 接口，并定义了还剩多久过期的方法    
                    public interface Delayed extends Comparable<Delayed> {
                   long getDelay(TimeUnit unit);
                    }
                  DelayQueue 还大量使用了 PriorityQueue 队列的大量功能，这个和 SynchronousQueue 队列很像，大量复用了其它基础类的逻辑，
                   PriorityQueue 中文叫做优先级队列，在此处的作用就是可以根据过期时间做优先级排序，让先过期的可以先执行，用来实现类注释中的第一点。
                   采用组合或继承两种手段进行复用，比如 LinkedHashMap 采用的继承、 Set 和 DelayQueue 采用的组合，组合的意思就是把可复用的类给依赖进来
                放数据
                    put 调用的是 offer 的方法，offer 的源码 
                     // 使用 PriorityQueue 的扩容，排序等能力
                   q.offer(e);
                   // 如果恰好刚放进去的元素正好在队列头
                   // 立马唤醒 take 的阻塞线程，执行 take 操作
                     // 如果元素需要延迟执行的话，可以使其更快的沉睡计时
             
                   Priority 扩容
                     // 扩容策略是：如果老容量小于 64，2 倍扩容，如果大于 64，50 % 扩容
               
                   对新增元素进行判空；
                      对队列进行扩容，扩容策略和集合的扩容策略很相近；
                      根据元素的 compareTo 方法进行排序，我们希望最终排序的结果是从小到大的，因为我们想让队头的都是过期的数据，我们需要在 compareTo 方法里面实现：通过每个元素的过期时间进行排序，如下：
                  DelayQueue 是非常有意思的队列，底层使用了排序和超时阻塞实现了延迟队列，排序使用的是 PriorityQueue 排序能力，超时阻塞使用得是锁的等待能力，可以看出 DelayQueue 其实就是为了满足延迟执行的场景，在已有 API 的基础上进行了封装
           - ArrayBlockingQueue 源码解析
              数组阻塞队列
               举新增场景来说 ArrayList 通过 size ++ 找到新增的数组下标位置，HashMap 通过 hash 算法计算出下标位置
              整体架构
               类注释
                有界的阻塞数组，容量一旦创建，后续大小无法修改；
                元素是有顺序的，按照先入先出进行排序，从队尾插入数据数据，从队头拿数据；
                队列满时，往队列中 put 数据会被阻塞，队列空时，往队列中拿数据也会被阻塞
                ArrayBlockingQueue 和一般的数组结构的类不太一样，是不能够动态扩容的，如果队列满了或者空时，take 和 put 都会被阻塞。
                
               数据结构
                 / 队列存放在 object 的数组里面
                // 数组大小必须在初始化的时候手动设置，没有默认大小
                final Object[] items;
                 // 下次拿数据的时候的索引位置
                  int takeIndex;
                 // 下次放数据的索引位置
                  int putIndex;
                  // 当前已有元素的大小
                  int count;
                 // 可重入的锁
                  final ReentrantLock lock;
                 // take的队列
                 private final Condition notEmpty;
                  // put的队列
                   private final Condition notFull;
                  takeIndex 和 putIndex，分别表示下次拿数据和放数据的索引位置。所以说在新增数据和拿数据时，都无需计算，就能知道应该新增到什么位置，应该从什么位置拿数据。 
              初始化
                   初始化时，有两个重要的参数：数组的大小、是否是公平
                     ArrayBlockingQueue(int capacity, boolean fair)
                         this.items = new Object[capacity];
                         lock = new ReentrantLock(fair);
                         // 队列不为空 Condition，在 put 成功时使用
                          notEmpty = lock.newCondition();
                         // 队列不满 Condition，在 take 成功时使用
                         notFull =  lock.newCondition();
                    第二个参数是否公平，主要用于读写锁是否公平，如果是公平锁，那么在锁竞争时，就会按照先来先到的顺序，如果是非公平锁，锁竞争时随机的。  
                    
                    比如说现在队列是满的，还有很多线程执行 put 操作，必然会有很多线程阻塞等待，当有其它线程执行 take 时，会唤醒等待的线程，如果是公平锁，会按照阻塞等待的先后顺序，依次唤醒阻塞的线程，如果是非公平锁，会随机唤醒沉睡的线程。   
                     
                    队列满很多线程执行 put 操作时，如果是公平锁，数组元素新增的顺序就是阻塞线程被释放的先后顺序，是有顺序的，而非公平锁，由于阻塞线程被释放的顺序是随机的，所以元素插入到数组的顺序也就不会按照插入的顺序了。

                    ArrayBlockingQueue 通过锁的公平和非公平，轻松实现了数组元素的插入顺序的问题

                    初始化时，如果给定了原始数据的话，一定要注意原始数据的大小一定要小于队列的容量
               新增数据
                  数据新增都会按照 putIndex 的位置进行新增
                     // putIndex 为本次插入的位置
                     items[putIndex] = x;
                     // ++ putIndex 计算下次插入的位置
                    // 如果下次插入的位置，正好等于队尾，下次插入就从 0 开始
                   if (++putIndex == items.length)
                        putIndex = 0;
                       
                拿数据
                   拿数据都是从队头开始拿数据
                    // ++ takeIndex 计算下次拿数据的位置
                   / / 如果正好等于队尾的话，下次就从 0 开始拿数据
                   if (++takeIndex == items.length)
                        takeIndex = 0;    
                删除数据
                   // 一共有两种情况：
                  // 1：删除位置和 takeIndex 的关系：删除位置和 takeIndex 一样，
                  比如 takeIndex 是 2， 而要删除的位置正好也是 2，那么就把位置 2 的数据置为 null ,并重新计算 takeIndex 为 3。
                  // 2：找到要删除元素的下一个，计算删除元素和 putIndex 的关系
                  // 如果下一个元素不是 putIndex，就把下一个元素往前移动一位（果 removeIndex + 1 != putIndex 的话，就把下一个元素往前移动一位）
                  // 如果下一个元素是 putIndex，把 putIndex 的值修改成删除的位置（如果 removeIndex + 1 == putIndex 的话，就把 putIndex 的值修改成删除的位置）
                ArrayBlockingQueue 底层是有界的数组，整体来说，和其它队列差别不多，需要注意的是，当 takeIndex、putIndex 到队尾的时候，都会重新从 0 开始循环，
           - 并发包中的 ConcurrentLinkedQueue 和 LinkedBlockingQueue 有什么区别？
               Concurrent 类型基于 lock-free，在常见的多线程访问场景，一般可以提供较高吞吐量。而 LinkedBlockingQueue 内部则是基于锁，并提供了 BlockingQueue 的等待性方法。不知道你有没有注意到，java.util.concurrent 包提供的容器（Queue、List、Set）、Map，从命名上可以大概区分为 Concurrent*、CopyOnWrite和 Blocking等三类，同样是线程安全容器，可以简单认为：Concurrent 类型没有类似 CopyOnWrite 之类容器相对较重的修改开销。但是，凡事都是有代价的，Concurrent 往往提供了较低的遍历一致性。你可以这样理解所谓的弱一致性，例如，当利用迭代器遍历时，如果容器发生修改，迭代器仍然可以继续进行遍历。与弱一致性对应的，就是我介绍过的同步容器常见的行为“fail-fast”，也就是检测到容器在遍历过程中发生了修改，则抛出 ConcurrentModificationException，不再继续遍历。弱一致性的另外一个体现是，size 等操作准确性是有限的，未必是 100% 准确。与此同时，读取的性能具有一定的不确定性。
         
               类结构
                 queue->ConcurrentLinkedQueue
                       ->Blockingqueue 
                                     ->ArrayBlockingQueue
                                     ->delayQueue
                                     ->linkedBlockingQueue
                                     ->priorityQueue
                                     ->synchronousQueue
                                     ->transferQueue     
                       ->dequeue->concurrentLinkedDeque 
                                ->blockingDeque->linkedBlockingDeque   
                  有两个特别的Deque实现，ConcurrentLinkedDeque 和 LinkedBlockingDeque。Deque 的侧重点是支持对队列头尾都进行插入和删除，所以提供了特定的方法，如:尾部插入时需要的addLast(e)、offerLast(e)。尾部删除所需要的removeLast()、pollLast()  
                  从行为特征来看，绝大部分 Queue 都是实现了 BlockingQueue 接口。在常规队列操作基础上，Blocking 意味着其提供了特定的等待性操作，获取时（take）等待元素进队，或者插入时（put）等待队列出现空位                  
               队列是否有界
                 ArrayBlockingQueue 是最典型的的有界队列，其内部以 final 的数组保存数据，数组的大小就决定了队列的边界，所以我们在创建 ArrayBlockingQueue 时，都要指定容量，如public ArrayBlockingQueue(int capacity, boolean fair)
                 LinkedBlockingQueue，容易被误解为无边界，但其实其行为和内部代码都是基于有界的逻辑实现的，只不过如果我们没有在创建队列时就指定容量，那么其容量限制就自动被设置为 Integer.MAX_VALUE，成为了无界队列。
                 SynchronousQueue，这是一个非常奇葩的队列实现，每个删除操作都要等待插入操作，反之每个插入操作也都要等待删除动作。那么这个队列的容量是多少呢？是 1 吗？其实不是的，其内部容量是 0。
                 PriorityBlockingQueue 是无边界的优先队列，虽然严格意义上来讲，其大小总归是要受系统资源影响。
                 DelayedQueue 和 LinkedTransferQueue 同样是无边界的队列。对于无边界的队列，有一个自然的结果，就是 put 操作永远也不会发生其他 BlockingQueue 的那种等待情况。
               队列中条件变量区分及元素数量值维护
                 ArrayBlockingQueue，其条件变量与 LinkedBlockingQueue 版本的实现是有区别的。notEmpty、notFull 都是同一个再入锁的条件变量，而 LinkedBlockingQueue 则改进了锁操作的粒度，头、尾操作使用不同的锁，所以在通用场景下，它的吞吐量相对要更好一些。
                 take 方法与 ArrayBlockingQueue 中的实现，也是有不同的，由于其内部结构是链表，需要自己维护元素数量值
                  c = count.getAndDecrement();
                  类似 ConcurrentLinkedQueue 等，则是基于 CAS 的无锁技术，不需要在每个操作时使用锁，所以扩展性表现要更加优异。相对比较另类的 SynchronousQueue，在 Java 6 中，其实现发生了非常大的变化，利用 CAS 替换掉了原本基于锁的逻辑，同步开销比较小。它是 Executors.newCachedThreadPool() 的默认队列。
               队列的使用场景和典型用例
                  以 LinkedBlockingQueue、ArrayBlockingQueue 和 SynchronousQueue 为例，我们一起来分析一下，根据需求可以从很多方面考量：考虑应用场景中对队列边界的要求。
                  ArrayBlockingQueue 是有明确的容量限制的，而 LinkedBlockingQueue 则取决于我们是否在创建时指定，SynchronousQueue 则干脆不能缓存任何元素。从空间利用角度，数组结构的 ArrayBlockingQueue 要比 LinkedBlockingQueue 紧凑，因为其不需要创建所谓节点，但是其初始分配阶段就需要一段连续的空间，所以初始内存需求更大。通用场景中，LinkedBlockingQueue 的吞吐量一般优于 ArrayBlockingQueue，因为它实现了更加细粒度的锁操作。ArrayBlockingQueue 实现比较简单，性能更好预测，属于表现稳定的“选手”。如果我们需要实现的是两个线程之间接力性（handoff）的场景，按照专栏上一讲的例子，你可能会选择 CountDownLatch，但是SynchronousQueue也是完美符合这种场景的，而且线程间协调和数据传输统一起来，代码更加规范。可能令人意外的是，很多时候 SynchronousQueue 的性能表现，往往大大超过其他实现，尤其是在队列元素较小的场景。  
           - 队列面试
               说说你对队列的理解，队列和集合的区别
                 对队列的理解：
                   首先队列本身也是个容器，底层也会有不同的数据结构，比如 LinkedBlockingQueue 是底层是链表结构，所以可以维持先入先出的顺序，比如 DelayQueue 底层可以是队列或堆栈，所以可以保证先入先出，或者先入后出的顺序等等，底层的数据结构不同，也造成了操作实现不同；
                    部分队列（比如 LinkedBlockingQueue ）提供了暂时存储的功能，我们可以往队列里面放数据，同时也可以从队列里面拿数据，两者可以同时进行；
                    队列把生产数据的一方和消费数据的一方进行解耦，生产者只管生产，消费者只管消费，两者之间没有必然联系，队列就像生产者和消费者之间的数据通道一样，如 LinkedBlockingQueue；
                    队列还可以对消费者和生产者进行管理，比如队列满了，有生产者还在不停投递数据时，队列可以使生产者阻塞住，让其不再能投递，比如队列空时，有消费者过来拿数据时，队列可以让消费者 hodler 住，等有数据时，唤醒消费者，让消费者拿数据返回，如 ArrayBlockingQueue；
                    队列还提供阻塞的功能，比如我们从队列拿数据，但队列中没有数据时，线程会一直阻塞到队列有数据可拿时才返回。
                 队列和集合的区别：
                    和集合的相同点，队列（部分例外）和集合都提供了数据存储的功能，底层的储存数据结构是有些相似的，比如说 LinkedBlockingQueue 和 LinkedHashMap 底层都使用的是链表，ArrayBlockingQueue 和 ArrayList 底层使用的都是数组。
                    和集合的区别：
                     2.1 部分队列和部分集合底层的存储结构很相似的，但两者为了完成不同的事情，提供的 API 和其底层的操作实现是不同的。
                     2.2 队列提供了阻塞的功能，能对消费者和生产者进行简单的管理，队列空时，会阻塞消费者，有其他线程进行 put 操作后，会唤醒阻塞的消费者，让消费者拿数据进行消费，队列满时亦然。
                     2.3 解耦了生产者和消费者，队列就像是生产者和消费者之间的管道一样，生产者只管往里面丢，消费者只管不断消费，两者之间互不关心。  
               哪些队列具有阻塞的功能，大概是如何阻塞的
                 队列主要提供了两种阻塞功能，如下：
                  LinkedBlockingQueue 链表阻塞队列和 ArrayBlockingQueue 数组阻塞队列是一类，前者容量是 Integer 的最大值，后者数组大小固定，两个阻塞队列都可以指定容量大小，当队列满时，如果有线程 put 数据，线程会阻塞住，直到有其他线程进行消费数据后，才会唤醒阻塞线程继续 put，当队列空时，如果有线程 take 数据，线程会阻塞到队列不空时，继续 take。
                  SynchronousQueue 同步队列，当线程 put 时，必须有对应线程把数据消费掉，put 线程才能返回，当线程 take 时，需要有对应线程进行 put 数据时，take 才能返回，反之则阻塞，举个例子，线程 A put 数据 A1 到队列中了，此时并没有任何的消费者，线程 A 就无法返回，会阻塞住，直到有线程消费掉数据 A1 时，线程 A 才能返回。
                底层是如何实现阻塞的？
                     队列本身并没有实现阻塞的功能，而是利用 Condition 的等待唤醒机制，阻塞底层实现就是更改线程的状态为沉睡

                LinkedBlockingQueue 和 ArrayBlockingQueue 有啥区别。
                  相同点：
                    两者的阻塞机制大体相同，比如在队列满、空时，线程都会阻塞住。
                  不同点：
                     LinkedBlockingQueue 底层是链表结构，容量默认是 Interge 的最大值，ArrayBlockingQueue 底层是数组，容量必须在初始化时指定。
                      两者的底层结构不同，所以 take、put、remove 的底层实现也就不同。     
                往队列里面 put 数据是线程安全的么？为什么？
                      是线程安全的，在 put 之前，队列会自动加锁，put 完成之后，锁会自动释放，保证了同一时刻只会有一个线程能操作队列的数据，以 LinkedBlockingQueue 为例子，put 时，会加 put 锁，并只对队尾 tail 进行操作，take 时，会加 take 锁，并只对队头 head 进行操作，remove 时，会同时加 put 和 take 锁，所以各种操作都是线程安全的，我们工作中可以放心使用。
                take 的时候也会加锁么？既然 put 和 take 都会加锁，是不是同一时间只能运行其中一个方法。
                      1：是的，take 时也会加锁的，像 LinkedBlockingQueue 在执行 take 方法时，在拿数据的同时，会把当前数据删除掉，就改变了链表的数据结构，所以需要加锁来保证线程安全。

                      2：这个需要看情况而言，对于 LinkedBlockingQueue 来说，队列的 put 和 take 都会加锁，但两者的锁是不一样的，所以两者互不影响，可以同时进行的，对于 ArrayBlockingQueue 而言，put 和 take 是同一个锁，所以同一时刻只能运行一个方法。      
                经常使用队列的 put、take 方法有什么危害，如何避免。
                         当队列满时，使用 put 方法，会一直阻塞到队列不满为止。
                         当队列空时，使用 take 方法，会一直阻塞到队列有数据为止。
                         两个方法都是无限（永远、没有超时时间的意思）阻塞的方法，容易使得线程全部都阻塞住，大流量时，导致机器无线程可用，所以建议在流量大时，使用 offer 和 poll 方法来代替两者，我们只需要设置好超时阻塞时间，这两个方法如果在超时时间外，还没有得到数据的话，就会返回默认值（LinkedBlockingQueue 为例），这样就不会导致流量大时，所有的线程都阻塞住了。
                         这个也是生产事故常常发生的原因之一，尝试用 put 和 take 方法，在平时自测中根本无法发现，对源码不熟悉的同学也不会意识到会有问题，当线上大流量打进来时，很有可能会发生故障，所以我们平时工作中使用队列时，需要谨慎再谨慎  
                 把数据放入队列中后，有木有办法让队列过一会儿再执行？
                     可以的，DelayQueue 提供了这种机制，可以设置一段时间之后再执行，该队列有个唯一的缺点，就是数据保存在内存中，在重启和断电的时候，数据容易丢失，所以定时的时间我们都不会设置很久，一般都是几秒内，如果定时的时间需要设置很久的话，可以考虑采取延迟队列中间件（这种中间件对数据会进行持久化，不怕断电的发生）进行实现。   
                  DelayQueue 对元素有什么要求么，我把 String 放到队列中去可以么？
                      DelayQueue 要求元素必须实现 Delayed 接口，Delayed 本身又实现了 Comparable 接口，Delayed 接口的作用是定义还剩下多久就会超时，给使用者定制超时时间的，Comparable 接口主要用于对元素之间的超时时间进行排序的，两者结合，就可以让越快过期的元素能够排在前面。

                      所以把 String 放到 DelayQueue 中是不行的，编译都无法通过，DelayQueue 类在定义的时候，是有泛型定义的，泛型类型必须是 Delayed 接口的子类才行。 
                   DelayQueue 如何让快过期的元素先执行的？
                         DelayQueue 中的元素都实现 Delayed 和 Comparable 接口的，其内部会使用 Comparable 的 compareTo 方法进行排序，我们可以利用这个功能，在 compareTo 方法中实现过期时间和当前时间的差，这样越快过期的元素，计算出来的差值就会越小，就会越先被执行。 
                   没有容量无法查看 SynchronousQueue 队列的大小  返回0 

                   SynchronousQueue 底层有几种数据结构，两者有何不同？
                     底层有两种数据结构，分别是队列和堆栈。
                     两者不同点：
                       队列维护了先入先出的顺序，所以最先进去队列的元素会最先被消费，我们称为公平的，而堆栈则是先入后出的顺序，最先进入堆栈中的数据可能会最后才会被消费，我们称为不公平的。
                       两者的数据结构不同，导致其 take 和 put 方法有所差别
                    假设 SynchronousQueue 底层使用的是堆栈，线程 1 执行 take 操作阻塞住了，然后有线程 2 执行 put 操作，问此时线程 2 是如何把 put 的数据传递给 take 的？  
                       首先线程 1 被阻塞住，此时堆栈头就是线程 1 了，此时线程 2 执行 put 操作，会把 put 的数据赋值给堆栈头的 match 属性，并唤醒线程 1，线程 1 被唤醒后，拿到堆栈头中的 match 属性，就能够拿到 put 的数据了。
                       严格上说并不是 put 操作直接把数据传递给了 take，而是 put 操作改变了堆栈头的数据，从而 take 可以从堆栈头上直接拿到数据，堆栈头是 take 和 put 操作之间的沟通媒介 
                   如果想使用固定大小的队列，有几种队列可以选择，有何不同？
                      可以使用 LinkedBlockingQueue 和 ArrayBlockingQueue 两种队列。
                       前者是链表，后者是数组，链表新增时，只要建立起新增数据和链尾数据之间的关联即可，数组新增时，需要考虑到索引的位置（takeIndex 和 putIndex 分别记录着下次拿数据、放数据的索引位置），如果增加到了数组最后一个位置，下次就要重头开始新增    
                    ArrayBlockingQueue 可以动态扩容么？用到数组最后一个位置时怎么办？
                        不可以的，虽然 ArrayBlockingQueue 底层是数组，但不能够动态扩容的。
                        假设 put 操作用到了数组的最后一个位置，那么下次 put 就需要从数组 0 的位置重新开始了。
                        假设 take 操作用到数组的最后一个位置，那么下次 take 的时候也会从数组 0 的位置重新开始。
                    ArrayBlockingQueue take 和 put 都是怎么找到索引位置的？是利用 hash 算法计算得到的么？
                        ArrayBlockingQueue 有两个属性，为 takeIndex 和 putIndex，分别标识下次 take 和 put 的位置，每次 take 和 put 完成之后，都会往后加一，虽然底层是数组，但和 HashMap 不同，并不是通过 hash 算法计算得到的。    
           - 队列在java其他源码中的应用    
                1 队列和线程池的结合
                 队列在线程池中的作用
                   线程池大家应该都使用过，比如我们想新建一个固定大小的线程池，并让运行的线程打印一句话出来，
                   ExecutorService executorService = Executors.newFixedThreadPool(10);
                   // submit 是提交任务的意思
                   executorService.submit(() -> System.out.println(Thread.currentThread().getName()   
                    代码中的 Executors 是并发的工具类，主要是为了帮助我们更方便的构造线程池的，其中 newFixedThreadPool 方法表示会构造出固定大小的线程池，我们给的入参是 10，代表线程池最大可以构造 10 个线程出来。   

                    在实际的工作中，我们对流量的大小是无法控制的，这里我们设定的最大是 10 个线程，但如果一下子来了 100 个请求，这时候 10 个线程肯定是忙不过来了，那么剩余的 90 个请求怎么办呢？
                    这时候就需要队列出马了，我们会把线程无法消化的数据放到队列中去，让数据在队列中排队，等线程有能力消费了，再从队列中拿出来慢慢去消费。  
                   

                    10 个线程正在全力消费请求，左边表示剩余请求正在队列中排队，等待消费。
                    由此可见，队列在线程池中占有很重要的地位，当线程池中的线程忙不过来的时候，请求都可以在队列中等待，从而慢慢地消费。
                  1.1 线程池中使用到的队列的类型
                  LinkedBlockingQueue 队列的使用
                        刚刚我们说的 newFixedThreadPool 是一种固定大小的线程池，意思是当线程池初始化好后，线程池里面的线程大小是不会变的了（线程池默认设置是不会回收核心线程数的），我们来看下 newFixedThreadPool 的源码：
                          / ThreadPoolExecutor 初始化时，第一个参数表示 coreSize，第二个参数是 maxSize，coreSize == maxSize,
                        // 表示线程池初始化时，线程大小已固定，所以叫做固定(Fixed)线程池。 
                     public static ExecutorService newFixedThreadPool(int nThreads) {
                            return new ThreadPoolExecutor(nThreads, nThreads,
                                  0L, TimeUnit.MILLISECONDS,
                                  new LinkedBlockingQueue<Runnable>());
                     }

                     源码中可以看到初始化了 ThreadPoolExecutor，ThreadPoolExecutor 是线程池的 API，我们在线程池章节会细说，它的第五个构造参数就是队列，线程池根据场景会选择不同的队列，此处使用的是 LinkedBlockingQueue，并且是默认参数的 Queue，这说明此阻塞队列的最大容量是 Integer 的最大值，也就是说当线程池的处理能力有限时，阻塞队列中最大可以存放 Integer 最大值个任务。

                     但我们在实际工作中，常常不建议直接使用 newFixedThreadPool，主要是因为其使用的是 LinkedBlockingQueue 的默认构造器，队列容量太大了，在要求实时响应的请求中，队列容量太大往往危害也很大。

                     比如说我们用上述的线程池，线程 10 个，队列是 Integer 的最大值，当并发流量很大时，比如来了 1w/qps 请求，这时候 10 个线程根本消费不完，就会有很多请求被阻塞在队列中，虽然 10 个线程仍然在不断地消费，但需要消费完队列中的所有数据是需要时间的，假设需要 3 秒才能全部消费完，而这些实时请求都是有超时时间的，默认超时时间是 2 秒，当时间到达 2 秒时，请求已经超时了，返回报错，可这时候队列中的任务还有很多都在等待消费呢，即使后来消费完成，也无法返回给调用方了。

                     以上情况就会造成，调用方看到接口是超时报错返回的，但服务端的任务其实还在排队执行，过了 3 秒后，服务端的任务可能都会执行成功，但调用方已经无法感知了，调用方再次调用时，就会发现其实这笔请求已经成功了。

                     如果调用方是从页面发起的，那么体验就会更差，页面上第一次调用页面报错，用户重新刷新页面时，页面显示上次的请求已经成功了，这个就是很不好的体验了。

                      所以我们希望队列的大小不要设置成那么大，可以根据实际的消费情况来设置队列的大小，这样就可以保证在接口超时前，队列中排队的请求可以执行完。  

                     和 newFixedThreadPool 相同的是，newSingleThreadExecutor 方法底层使用的也是 LinkedBlockingQueue，newSingleThreadExecutor 线程池底层线程只会有一个，这代表着这个线程池一次只能处理一个请求，其余的请求都会在队列中排队等待执行 
                  SynchronousQueue 队列
                      newCachedThreadPool，newCachedThreadPool 底层对应的是 SynchronousQueue 队列，
                        // 第五个参数是 SynchronousQueue
                         return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                  60L, TimeUnit.SECONDS,
                                  new SynchronousQueue<Runnable>());
                      SynchronousQueue 队列是没有大小限制的，请求多少队列都能承受的住，可以说这是他的优点，缺点就是每次往队列里面 put 数据时，并不能立马返回，而是需要等待有线程 take 数据之后，才能正常返回，如果请求量大，而消费能力较差时，就会导致大量请求被 hodler 住，必须等到慢慢消费完成之后才能被释放，所以在平时工作使用中也需要慎重   
                      
                  DelayedWorkQueue  队列    
                        newScheduledThreadPool 代表定时任务线程池，    
                        底层队列使用的是 DelayedWorkQueue 延迟队列，说明线程池底层延时的功能就是 DelayedWorkQueue 队列提供的，新的延迟请求都先到队列中去，延迟时间到了，线程池自然就能从队列中拿出线程进行执行了

                   队列在线程池的设计中，起着缓冲数据，延迟执行数据的作用，当线程池消费能力有限时，可以让请求进行排队，让线程池可以慢慢消费。
                   线程池根据不同的场景，选择使用了 DelayedWorkQueue、SynchronousQueue、LinkedBlockingQueue 多种队列，从而实现自己不同的功能，比如使用 DelayedWorkQueue 的延迟功能来实现定时执行线程池。     

                队列和锁的结合
                    初始化锁 -> 加锁 -> 执行业务逻辑 -> 释放锁，这是正常的流程，但我们知道同一时刻只能有一个线程才能获得锁的，那么此时其他获取不到锁的线程该怎么办呢？
                     等待，其他获取不到锁的线程，都会到一个等待队列中去等待，等待锁被释放掉时，再去竞争锁

                     可以看出队列在锁中起的作用之一，就是帮助管理获取不到锁的线程，让这些线程可以耐心的等待。
                     同步队列并没有使用现有的队列的 API 去实现，但底层的结构，思想和目前队列是一致的，所以我们学好队列章节，对理解锁的同步队列，用处非常大
           - 整体设计 队列设计思想 工作中使用场景
                队列解耦了生产者和消费者，提供了生产者和消费者间关系的多种形式，比如 LinkedBlockingQueue、ArrayBlockingQueue 两种队列就把解耦了生产者和消费者，比如 SynchronousQueue 这种就把生产者和消费者相互对应（生产者的消息被消费者开始消费之后，生产者才能返回，为了方便理解，使用相互对应这个词）；
                不同的队列有着不同的数据结构，有链表（LinkedBlockingQueue）、数组（ArrayBlockingQueue）、堆栈（SynchronousQueue）等；
                不同的数据结构，决定了入队和出队的姿势是不同的。
               设计思想
                队列的数据结构
                  链表结构的队列就是 LinkedBlockingQueue，其特征如下：
                    初始大小默认是 Integer 的最大值，也可以设置初始大小；
                    链表元素通过 next 属性关联下一个元素；
                    新增是从链表的尾部新增，拿是从链表头开始拿。
                   数组结构的队列是 ArrayBlockingQueue，特征如下：
                     容量大小是固定的，不能动态扩容；
                     有 takeIndex 和 putIndex 两个索引记录下次拿和新增的位置；
                     当 takeIndex 和 putIndex 到达数组的最后一个位置时，下次都是从 0 开始循环。
                   SynchronousQueue 有着两种数据结构，分别是队列和堆栈，特征如下：
                     队列保证了先入先出的数据结构，体现了公平性；
                     堆栈是先入后出的数据结构，是不公平的，但性能高于先入先出。
                 入队和出队的方式
                    不同的队列有着不同的数据结构，导致其入队和出队的方式也不同：
                     链表是入队是直接追加到队尾，出队是从链表头拿数据；
                     数组是有 takeIndex 和 putIndex 两个索引位置记录下次拿和取的位置，如总体设计图，入队直接指向了 putIndex，出队指向了 takeIndex；
                      堆栈主要都是围绕栈头进行入栈和出栈的。
                  生产者和消费者之间的通信机制
                      从四种队列我们可以看出来生产者和消费者之间有两种通信机制，一种是强关联，一种是无关联。
                         强关联主要是指 SynchronousQueue 队列，生产者往队列中 put 数据，如果这时候没有消费者消费的话，生产者就会一直阻塞住，是无法返回的；消费者来队列里取数据，如果这时候队列中没有数据，消费者也会一直阻塞住，所以 SynchronousQueue 队列模型中，生产者和消费者是强关联的，如果只有其中一方存在，只会阻塞，是无法传递数据的。
                         无关联主要是说有数据存储功能的队列，比如说 LinkedBlockingQueue 和 ArrayBlockingQueue，只要队列容器不满，生产者就能放成功，生产者就可以直接返回，和有无消费者一点关系都没有，生产者和消费者完全解耦，通过队列容器的储存功能进行解耦。    

                工作中的使用场景
                   LinkedBlockingQueue
                     适合对生产的数据大小不定（时高时低），数据量较大的场景，
                     一般工作中，我们大多数都会选择 LinkedBlockingQueue 队列，但会设置 LinkedBlockingQueue 的最大容量，如果初始化时直接使用默认的 Integer 的最大值，当流量很大，而消费者处理能力很差时，大量请求都会在队列中堆积，会大量消耗机器的内存，就会降低机器整体性能甚至引起宕机，一旦宕机，在队列中的数据都会消失，因为队列的数据是保存在内存中的，一旦机器宕机，内存中的数据都会消失的，所以使用 LinkedBlockingQueue 队列时，建议还是要根据日常的流量设置合适的队列的大小。

                  ArrayBlockingQueue
                     一般用于生产数据固定的场景，比如说系统每天会进行对账，对账完成之后，会固定的产生 100 条对账结果，因为对账结果固定，我们就可以使用 ArrayBlockingQueue 队列，大小可以设置成 100。
                  DelayQueue
                      延迟队列，在工作中经常遇到，主要用于任务不想立马执行，想等待一段时间才执行的场景
                      ，延迟对账的核心技术就是 DelayQueue，我们大概这么做的：新建对账任务，设置 3 秒之后执行，把任务放到 DelayQueue 中，过了 3 秒之后，就会自动执行对账任务了。
                      DelayQueue 延迟执行的功能就在这个场景中得到应用。
           - 由浅入深给手写队列
              定义接口 
                 public interface Queue<T> {
                    /**
                       * 放数据
                        * @param item 入参
                         * @return true 成功、false 失败
                   */
                  boolean put(T item);

                   /**
                     * 拿数据，返回一个泛型值
                      * @return
                      */
                     T take();

                 // 队列中元素的基本结构
                class Node<T> {
               // 数据本身
                     T item;
                    // 下一个元素
                    Node<T> next;

                    // 构造器
                    public Node(T item) {
                     this.item = item;
                  }
               定义接口时，一定要写注释，接口的注释，方法的注释等等，这样别人看我们的接口时，会轻松很多‘；
               定义接口时，要求命名简洁明了，最好让别人一看见命名就知道这个接口是干啥的，比如我们命名为 Queue，别人一看就清楚这个接口是和队列相关的；
               用好泛型，因为我们不清楚放进队列中的到底都是那些值，所以我们使用了泛型 T，表示可以在队列中放任何值；
               接口里面无需给方法写上 public 方法，因为接口中的方法默认都是 public 的，你写上编译器也会置灰，如下图：           

             队列子类的实现
                数据结构
                  底层数据结构我们采用链表，一说到链表，大家应该马上就会想到三个关键要素：链表头、链表尾和链表元素    
                    private volatile Node<T> head;
                    private volatile Node<T> tail; 
                     class DIYNode extends Node<T>{
                   除了这些元素之外，我们还有队列容器的容量大小、队列目前的使用大小、放数据锁、拿数据锁  
                     private AtomicInteger size = new AtomicInteger(0); （队列的大小，使用 AtomicInteger 来保证其线程安全）
                     private final Integer capacity;（容量）
                     private ReentrantLock putLock = new ReentrantLock();
                     private ReentrantLock takeLock  = new ReentrantLock();
                  初始化   
                     使用默认容量和指定容量两种方式
                  put方法的实现
                       // 尝试加锁，500 毫秒未获得锁直接被打断
                     boolean lockSuccess = putLock.tryLock(300, TimeUnit.MILLISECONDS);
                     // 校验队列大小
                     if(size.get() >= capacity)
                     // 追加到队尾
                     tail = tail.next = new DIYNode(item);
                      // 计数
                      size.incrementAndGet();
                   注意 try catch finally 的节奏，catch 可以捕捉多种类型的异常，我们这里就捕捉了超时异常和未知异常，在 finally 里面一定记得要释放锁，不然锁不会自动释放的，这个一定不能用错，体现了我们代码的准确性；
                   必要的逻辑检查还是需要的，比如入参是否为空的空指针检查，队列是否满的临界检查，这些检查代码可以体现出我们逻辑的严密性；
                   在代码的关键地方加上日志和注释，这点也是非常重要的，我们不希望关键逻辑代码注释和日志都没有，不利于阅读代码和排查问题；
                   注意线程安全，此处实现我们除了加锁之外，对于容量的大小（size）我们选择线程安全的计数类：AtomicInteger，来保证了线程安全；
                   加锁的时候，我们最好不要使用永远阻塞的方法，我们一定要用带有超时时间的阻塞方法，此处我们设置的超时时间是 300 毫秒，也就是说如果 300 毫秒内还没有获得锁，put 方法直接返回 false，当然时间大小你可以根据情况进行设置；
                    根据不同的情况设置不同的返回值，put 方法返回的是 false，在发生异常时，我们可以选择返回 false，或者直接抛出异常；
                   put 数据时追加到队尾的，所以我们只需要把新数据转化成 DIYNode，放到队列的尾部即可   
                 take方法实现
        -- 线程池 
           -- ThreadPoolExecutor 源码解析
             类结构
                Executor-ExecutorService->AbstractExecutorService->ThreadPoolExecutor
                除了 Thread，这些 Executor 命名的类和接口也是可以执行这几种任务的
                
                Executor：定义 execute 方法来执行任务，入参是 Runnable，无出参：
                
                ExecutorService：Executor 的功能太弱，ExecutorService 丰富了对任务的执行和管理的功能，主要代码如下  
                / 关闭，不会接受新的任务，也不会等待未完成的任务
                // 如果需要等待未完成的任务，可以使用 awaitTermination 方法
                void shutdown();
                // executor 是否已经关闭了，返回值 true 表示已关闭
                boolean isShutdown();
                // 所有的任务是否都已经终止，是的话，返回 true
                boolean isTerminated();
                // 在超时时间内，等待剩余的任务终止
                boolean awaitTermination(long timeout, TimeUnit unit)
                 throws InterruptedException;
                // 提交有返回值的任务，使用 get 方法可以阻塞等待任务的执行结果返回
                <T> Future<T> submit(Callable<T> task);
                // 提交没有返回值的任务，如果使用 get 方法的话，任务执行完之后得到的是 null 值
                Future<?> submit(Runnable task);
                // 给定任务集合，返回已经执行完成的 Future 集合，每个返回的 Future 都是 isDone = true 的状态
                <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks)
                 throws InterruptedException;
                 // 给定任务中有一个执行成功就返回，如果抛异常，其余未完成的任务将被取消
                <T> T invokeAny(Collection<? extends Callable<T>> tasks)
                 throws InterruptedException, ExecutionException; 
               
                 AbstractExecutorService 是一个抽象类，封装了 Executor 的很多通用功能

                  // 把 Runnable 转化成 RunnableFuture
                  // RunnableFuture 是一个接口，实现了 Runnable 和 Future
                  // FutureTask 是 RunnableFuture 的实现类，主要是对任务进行各种管理
                  // Runnable + Future => RunnableFuture => FutureTask
                  protected <T> RunnableFuture<T> newTaskFor(Runnable runnable, T value) {
                       return new FutureTask<T>(runnable, value);
                   }
                  protected <T> RunnableFuture<T> newTaskFor(Callable<T> callable) {
                     return new FutureTask<T>(callable);
                   }
                    // 提交无返回值的任务
                   public Future<?> submit(Runnable task) {
                     if (task == null) throw new NullPointerException();
                     // ftask 其实是 FutureTask
                      RunnableFuture<Void> ftask = newTaskFor(task, null);
                      execute(ftask);
                    return ftask;
                   }
                   // 提交有返回值的任务
                   public <T> Future<T> submit(Callable<T> task) {
                       if (task == null) throw new NullPointerException();
                        // ftask 其实是 FutureTask
                         RunnableFuture<T> ftask = newTaskFor(task);
                         execute(ftask);
                      return ftask;
                   }

                  FutureTask 我们在第五章有说，其本身就是一个任务，而且具备对任务管理的功能，比如可以通过 get 方法拿到任务的执行结果；
                  submit 方法是我们平时使用线程池时提交任务的方法，支持 Runable 和 Callable 两种任务的提交，方法中 execute 方法是其子类 ThreadPoolExecutor 实现的，不管是那种任务入参，execute 方法最终执行的任务都是 FutureTask；
                  ThreadPoolExecutor 继承了 AbstractExecutorService 抽象类，具备以上三个类的所有功能。 
             类注释
                1ExecutorService 使用线程池中的线程执行提交的任务，线程池我们可以使用 Executors 进行配置；
                2线程池解决两个问题：
                1：通过减少任务间的调度开销 (主要是通过线程池中的线程被重复使用的方式)，来提高大量任务时的执行性能；
                2：提供了一种方式来管理线程和消费，维护基本数据统计等工作，比如统计已完成的任务数；
                3Executors 为常用的场景设定了可直接初始化线程池的方法，比如 Executors#newCachedThreadPool 无界的线程池，并且可以自动回收；Executors#newFixedThreadPool 固定大小线程池；Executors#newSingleThreadExecutor 单个线程的线程池；
                4为了在各种上下文中使用线程池，线程池提供可供扩展的参数设置：1：coreSize：当新任务提交时，发现运行的线程数小于 coreSize，一个新的线程将被创建，即使这时候其它工作线程是空闲的，可以通过 getCorePoolSize 方法获得 coreSize；2：maxSize: 当任务提交时，coreSize < 运行线程数 <= maxSize，但队列没有满时，任务提交到队列中，如果队列满了，在 maxSize 允许的范围内新建线程；
                5一般来说，coreSize 和 maxSize 在线程池初始化时就已经设定了，但我们也可以通过 setCorePoolSize、setMaximumPoolSize 方法动态的修改这两个值；
                6默认的，core threads 需要到任务提交后才创建的，但我们可以分别使用 prestartCoreThread、prestartAllCoreThreads 两个方法来提前创建一个、所有的 core threads；
                7新的线程被默认 ThreadFactory 创建时，优先级会被限制成 NORM_PRIORITY，默认会被设置成非守护线程，这个和新建线程的继承是不同的；
                8 Keep-alive times 参数的作用：1：如果当前线程池中有超过 coreSize 的线程；2：并且线程空闲的时间超过 keepAliveTime，当前线程就会被回收，这样可以避免线程没有被使用时的资源浪费
                9 Keep-alive times 参数的作用：1：如果当前线程池中有超过 coreSize 的线程；2：并且线程空闲的时间超过 keepAliveTime，当前线程就会被回收，这样可以避免线程没有被使用时的资源浪费
                10 如果设置 allowCoreThreadTimeOut 为 ture 的话，core thread 空闲时间超过 keepAliveTime 的话，也会被回收
                11 线程池新建时，有多种队列可供选择，比如：1：SynchronousQueue，为了避免任务被拒绝，要求线程池的 maxSize 无界，缺点是当任务提交的速度超过消费的速度时，可能出现无限制的线程增长；2：LinkedBlockingQueue，无界队列，未消费的任务可以在队列中等待；3：ArrayBlockingQueue，有界队列，可以防止资源被耗尽
                12 队列的维护：提供了 getQueue () 方法方便我们进行监控和调试，严禁用于其他目的，remove 和 purge 两个方法可以对队列中的元素进行操作
                - 线程池的拒绝策略有哪4种？
                13 在 Executor 已经关闭或对最大线程和最大队列都使用饱和时，可以使用 RejectedExecutionHandler 类进行异常捕捉，有如下四种处理策略：ThreadPoolExecutor.AbortPolicy、ThreadPoolExecutor.DiscardPolicy、ThreadPoolExecutor.CallerRunsPolicy、ThreadPoolExecutor.DiscardOldestPolicy；
                14 线程池提供了很多可供扩展的钩子函数，比如有：1：提供在每个任务执行之前 beforeExecute 和执行之后 afterExecute 的钩子方法，主要用于操作执行环境，比如初始化 ThreadLocals、收集统计数据、添加日志条目等；2: 如果在执行器执行完成之后想干一些事情，可以实现 terminated 方法，如果钩子方法执行时发生异常，工作线程可能会失败并立即终止。
             ThreadPoolExecutor 重要属性
                 //ctl 线程池状态控制字段，由两部分组成：
                 //1:workerCount  wc 工作线程数，我们限制 workerCount 最大到(2^29)-1，大概 5 亿个线程
                 //2:runState rs 
                 线程池的生命周期
                 线程池的状态，提供了生命周期的控制，源码中有很多关于状态的校验，状态枚举如下：
                 //RUNNING（-536870912）：接受新任务或者处理队列里的任务。
                  //SHUTDOWN（0）：不接受新任务，但仍在处理已经在队列里面的任务。
                  //STOP（-536870912）：不接受新任务，也不处理队列中的任务，对正在执行的任务进行中断。
                   //TIDYING（1073741824）： 所以任务都被中断，workerCount 是 0，整理状态
                   /TERMINATED（1610612736）： terminated() 已经完成的时候
                  //runState 之间的转变过程：
                  //RUNNING -> SHUTDOWN：调用 shudown(),finalize()
                  //(RUNNING or SHUTDOWN) -> STOP：调用shutdownNow()
                  //SHUTDOWN -> TIDYING -> workerCount ==0
                  //STOP -> TIDYING -> workerCount ==0
                  //TIDYING -> TERMINATED -> terminated() 执行完成之后
                  // 已完成任务的计数
                  volatile long completedTasks;
                  // 线程池最大容量
                  private int largestPoolSize;
                  // 已经完成的任务数
                  private long completedTaskCount;
                  // 用户可控制的参数都是 volatile 修饰的
                  // 可以使用 threadFactory 创建 thread
                  // 创建失败一般不抛出异常，只有在 OutOfMemoryError 时候才会
                   private volatile ThreadFactory threadFactory;
                  / 饱和或者运行中拒绝任务的 handler 处理类
                   private volatile RejectedExecutionHandler handler;
                   // 线程存活时间设置
                   private volatile long keepAliveTime;
                   // 设置 true 的话，核心线程空闲 keepAliveTime 时间后，也会被回收
                    private volatile boolean allowCoreThreadTimeOut;
                   // coreSize
                   private volatile int corePoolSize;
                   // maxSize 最大限制 (2^29)-1
                   private volatile int maximumPoolSize;
                   // 默认的拒绝策略
                   private static final RejectedExecutionHandler defaultHandler =
                     new AbortPolicy();
                   // 队列会 hold 住任务，并且利用队列的阻塞的特性，来保持线程的存活周期
                    private final BlockingQueue<Runnable> workQueue;
                  // 大多数情况下是控制对 workers 的访问权限
                   private final ReentrantLock mainLock = new ReentrantLock();
                   private final Condition termination = mainLock.newCondition();
                  // 包含线程池中所有的工作线程
                   private final HashSet<Worker> workers = new HashSet<Worker>();
             Worker 我们可以理解成线程池中任务运行的最小单元
              - 线程池的核心模型Worker对象的运作流程是怎样的？
                 // 线程池中任务执行的最小单元
                 // Worker 继承 AQS，具有锁功能
                 // Worker 实现 Runnable，本身是一个可执行的任务
                  private final class Worker
                    extends AbstractQueuedSynchronizer
                   implements Runnable
                       {
                    // 任务运行的线程
                     final Thread thread;

                      // 需要执行的任务
                       Runnable firstTask;
                     // Lock methods
                    // 0 代表没有锁住，1 代表锁住
                   protected boolean isHeldExclusively() {
                          return getState() != 0;
                   }
                   // 尝试加锁，CAS 赋值为 1，表示锁住
                   protected boolean tryAcquire(int unused) {
                    if (compareAndSetState(0, 1)) {
                           setExclusiveOwnerThread(Thread.currentThread());
                           return true;
                    }
                       return false;
                    }
                  // 尝试释放锁，释放锁没有 CAS 校验，可以任意的释放锁
                  protected boolean tryRelease(int unused) {
                       setExclusiveOwnerThread(null);
                       setState(0);
                       return true;
                   }  

                  Worker 很像是任务的代理，在线程池中，最小的执行单位就是 Worker，所以 Worker 实现了 Runnable 接口，实现了 run 方法；
                   在 Worker 初始化时 this.thread = getThreadFactory ().newThread (this) 这行代码比较关键，它把当前 Worker 作为线程的构造器入参，我们在后续的实现中会发现这样的代码：Thread t = w.thread;t.start ()，此时的 w 是 Worker 的引用申明，此处 t.start 实际上执行的就是 Worker 的 run 方法；
                   Worker 本身也实现了 AQS，所以其本身也是一个锁，其在执行任务的时候，会锁住自己，任务执行完成之后，会释放自己。 
             线程池的任务提交
               线程池的任务提交从 submit 方法说起，submit 方法是 AbstractExecutorService 抽象类定义的，主要做了两件事情：
               把 Runnable 和 Callable 都转化成 FutureTask，这个我们之前看过源码了；
               使用 execute 方法执行 FutureTask。
               execute 方法是 ThreadPoolExecutor 中的方法
                   // 工作的线程小于核心线程数，创建新的线程，成功返回，失败不抛异常
                  if (workerCountOf(c) < corePoolSize) {
              if (addWorker(command, true))
                 return;
              // 线程池状态可能发生变化
              c = ctl.get();
              // 工作的线程大于等于核心线程数，或者新建线程失败
             // 线程池状态正常，并且可以入队的话，尝试入队列
             if (isRunning(c) && workQueue.offer(command)) {
                 // 如果线程池状态异常 尝试从队列中移除任务，可以移除的话就拒绝掉任务
                  if (!isRunning(recheck) && remove(command))
                  reject(command);
                  // 发现可运行的线程数是 0，就初始化一个线程，这里是个极限情况，入队的时候，突然发现
                   // 可用线程都被回收了
                   else if (workerCountOf(recheck) == 0)
                   // Runnable是空的，不会影响新增线程，但是线程在 start 的时候不会运行
                  // Thread.run() 里面有判断
                    addWorker(null, false);

                // 队列满了，开启线程到 maxSize，如果失败直接拒绝,
               else if (!addWorker(command, false))
              reject(command);
              execute 方法执行的就是整体架构图的左半边的逻辑，其中多次调用 addWorker 方法，addWorker 方法的作用是新建一个 Worker

              // 结合线程池的情况看是否可以添加新的 worker
              // firstTask 不为空可以直接执行，为空执行不了，Thread.run()方法有判断，Runnable为空不执行
             // core 为 true 表示线程最大新增个数是 coresize，false 表示最大新增个数是 maxsize
             // 返回 true 代表成功，false 失败
             // break retry 跳到retry处，且不再进入循环
             // continue retry 跳到retry处，且再次进入循环
              addWorker 方法首先是执行了一堆校验，然后使用 new Worker (firstTask) 新建了 Worker，最后使用 t.start () 执行 Worker，上文我们说了 Worker 在初始化时的关键代码：this.thread = getThreadFactory ().newThread (this)，Worker（this） 是作为新建线程的构造器入参的，所以 t.start () 会执行到 Worker 的 run 方法上

              runworker 
                 // task 为空的情况：
              // 1：任务入队列了，极限情况下，发现没有运行的线程，于是新增一个线程；
              // 2：线程执行完任务执行，再次回到 while 循环。
              // 如果 task 为空，会使用 getTask 方法阻塞从队列中拿数据，如果拿不到数据，会阻塞住

             task.run为 futureTask
               run 方法中有两行关键代码：
               result = c.call () 这行代码是真正执行业务代码的地方；
               set (result) 这里是给 outCome 赋值，这样 Future.get 方法执行时，就可以从 outCome 中拿值，
              submit提交流程  threadPoolExecutor.submit->abstractExecutorService.submit->
              ThreadPoolExecutor.execute->ThreadPoolExecutor.addWorker->worker.run->future.run
             线程执行完任务之后都在干啥
               从 ThreadPoolExecutor 的 runWorker 方法中
               这个 while 循环有个 getTask 方法，getTask 的主要作用是阻塞从队列中拿任务出来，如果队列中有任务，那么就可以拿出来执行，如果队列中没有任务，这个线程会一直阻塞到有任务为止（或者超时阻塞）
                // 队列以 LinkedBlockingQueue 为例，timedOut 为 true 的话说明下面 poll 方法执行返回的是 null
               // 说明在等待 keepAliveTime 时间后，队列中仍然没有数据
               // 说明此线程已经空闲了 keepAliveTime 了
                // 再加上 wc > 1 || workQueue.isEmpty() 的判断
               // 所以使用 compareAndDecrementWorkerCount 方法使线程池数量减少 1
               // 并且直接 return，return 之后，此空闲的线程会自动被回收
                使用队列的 poll 或 take 方法从队列中拿数据，根据队列的特性，队列中有任务可以返回，队列中无任务会阻塞；
                方法中的第二个 if 判断，说的是在满足一定条件下（条件看注释），会减少空闲的线程，减少的手段是使可用线程数减一，并且直接 return，直接 return 后，该线程就执行结束了，JVM 会自动回收该线程。
		     	 -- 线程池参数说明，线程池的线程回收、shutdown
			     -- 线程池的提交，execute与submit有什么区别？在实际开发中需要注意哪些问题？
           --线程池源码面试题
			         说说你对线程池的理解？
			            线程池结合了锁、线程、队列等元素，在请求量较大的环境下，可以多线程的处理请求，充分的利用了系统的资源，提高了处理请求的速度，细节可以从以下几个方面阐述：
                     ThreadPoolExecutor 类结构；
                     ThreadPoolExecutor coreSize、maxSize 等重要属性；
                     Worker 的重要作用；
                    submit 的整个过程。
               ThreadPoolExecutor、Executor、ExecutorService、Runnable、Callable、FutureTask 之间的关系？
                    以上 6 个类可以分成两大类：一种是定义任务类，一种是执行任务类。
                 定义任务类：Runnable、Callable、FutureTask。Runnable 是定义无返回值的任务，Callable 是定义有返回值的任务，FutureTask 是对 Runnable 和 Callable 两种任务的统一，并增加了对任务的管理功能；
                 执行任务类：ThreadPoolExecutor、Executor、ExecutorService。Executor 定义最基本的运行接口，ExecutorService 是对其功能的补充，ThreadPoolExecutor 提供真正可运行的线程池类，三个类定义了任务的运行机制。
                 日常的做法都是先根据定义任务类定义出任务来，然后丢给执行任务类去执行。   
               说一说队列在线程池中起的作用？
                    当请求数大于 coreSize 时，可以让任务在队列中排队，让线程池中的线程慢慢的消费请求，实际工作中，实际线程数不可能等于请求数，队列提供了一种机制让任务可排队，起一个缓冲区的作用；
                  当线程消费完所有的线程后，会阻塞的从队列中拿数据，通过队列阻塞的功能，使线程不消亡，一旦队列中有数据产生后，可立马被消费。   
               结合请求不断增加时，说一说线程池构造器参数的含义和表现？
                   线程池构造器各个参数的含义如下：
                   coreSize 核心线程数；
                   maxSize 最大线程数；
                    keepAliveTime 线程空闲的最大时间；
                    queue 有多种队列可供选择，比如：1：SynchronousQueue，为了避免任务被拒绝，要求线程池的 maxSize 无界，缺点是当任务提交的速度超过消费的速度时，可能出现无限制的线程增长；2：LinkedBlockingQueue，无界队列，未消费的任务可以在队列中等待；3：ArrayBlockingQueue，有界队列，可以防止资源被耗尽；
                    线程新建的 ThreadFactory 可以自定义，也可以使用默认的 DefaultThreadFactory，DefaultThreadFactory 创建线程时，优先级会被限制成 NORM_PRIORITY，默认会被设置成非守护线程；
                   在 Executor 已经关闭或对最大线程和最大队列都使用饱和时，可以使用 RejectedExecutionHandler 类进行异常捕捉，有如下四种处理策略：ThreadPoolExecutor.AbortPolicy、ThreadPoolExecutor.DiscardPolicy、ThreadPoolExecutor.CallerRunsPolicy、ThreadPoolExecutor.DiscardOldestPolicy。 

                   当请求不断增加时，各个参数起的作用如下：
                     请求数 < coreSize：创建新的线程来处理任务；
                    coreSize <= 请求数 && 能够成功入队列：任务进入到队列中等待被消费；
                    队列已满 && 请求数 < maxSize：创建新的线程来处理任务；
                    队列已满 && 请求数 >= maxSize：使用 RejectedExecutionHandler 类拒绝请求。 
               coreSize 和 maxSize 可以动态设置么，有没有规则限制？
                   一般来说，coreSize 和 maxSize 在线程池初始化时就已经设定了，但我们也可以通过  setCorePoolSize、setMaximumPoolSize 方法动态的修改这两个值。 
                   // 如果新设置的值小于 coreSize,多余的线程在空闲时会被回收（不保证一定可以回收成功）
                   // 如果大于 coseSize，会新创建线程
                     // 并不清楚应该新增多少线程，取新增核心线程数和等待队列数据的最小值，够用就好
                      int k = Math.min(delta, workQueue.size());
                      // 新增线程直到k，如果期间等待队列空了也不会再新增
                    // 如果 maxSize 大于原来的值，直接设置。
                  // 如果 maxSize 小于原来的值，尝试干掉一些 worker
               说一说对于线程空闲回收的理解，源码中如何体现的？
                  空闲线程回收的时机：如果线程超过 keepAliveTime 时间后，还从阻塞队列中拿不到任务（这种情况我们称为线程空闲），当前线程就会被回收，如果 allowCoreThreadTimeOut 设置成 true，core thread 也会被回收，直到还剩下一个线程为止，如果 allowCoreThreadTimeOut 设置成 false，只会回收非 core thread 的线程。
                  线程在任务执行完成之后，之所有没有消亡，是因为阻塞的从队列中拿任务，在 keepAliveTime 时间后都没有拿到任务的话，就会打断阻塞，线程直接返回，线程的生命周期就结束了，JVM 会回收掉该线程对象，所以我们说的线程回收源码体现就是让线程不在队列中阻塞，直接返回了 
               如果我想在线程池任务执行之前和之后，做一些资源清理的工作，可以么，如何做？
                   可以的，ThreadPoolExecutor 提供了一些钩子函数，我们只需要继承 ThreadPoolExecutor 并实现这些钩子函数即可。在线程池任务执行之前实现 beforeExecute 方法，执行之后实现 afterExecute 方法。
               线程池中的线程创建，拒绝请求可以自定义实现么？如何自定义？
                  可以自定义的，线程创建默认使用的是 DefaultThreadFactory，自定义话的只需要实现 ThreadFactory 接口即可；拒绝请求也是可以自定义的，实现 RejectedExecutionHandler 接口即可；在 ThreadPoolExecutor 初始化时，将两个自定义类作为构造器的入参传递给 ThreadPoolExecutor 即可。 
               说一说线程执行任务之后，都在干啥？
                 线程执行任务完成之后，有两种结果：
                    线程会阻塞从队列中拿任务，没有任务的话无限阻塞；
                   线程会阻塞从队列中拿任务，没有任务的话阻塞一段时间后，线程返回，被 JVM 回收。     
                keepAliveTime 设置成负数或者是 0，表示无限阻塞？
                  这种是不对的，如果 keepAliveTime 设置成负数，在线程池初始化时，就会直接报 IllegalArgumentException 的异常，而设置成 0，队列如果是 LinkedBlockingQueue 的话，执行 workQueue.poll (keepAliveTime, TimeUnit.NANOSECONDS) 方法时，如果队列中没有任务，会直接返回 null，导致线程立马返回，不会无限阻塞。
                  如果想无限阻塞的话，可以把 keepAliveTime 设置的很大，把 TimeUnit 也设置的很大，接近于无限阻塞。
                说一说 Future.get 方法是如何拿到线程的执行结果的？
                  submit 方法的返回结果实际上是 FutureTask，我们平时都是针对接口编程，所以使用的是 Future.get 来拿到线程的执行结果，实际上是 FutureTask.get ，其方法底层是从 FutureTask 的 outcome 属性拿值的；
                  submit 方法最终会把线程的执行结果赋值给 outcome。
                  结合 1、2，当线程执行完成之后，自然就可以从 FutureTask 的 outcome 属性中拿到值。
           --不同场景，如何使用线程池
              1 coreSize == maxSize
               ThreadPoolExecutor executor = new ThreadPoolExecutor(10, 10, 600000L, TimeUnit.DAYS,
                                                     new LinkedBlockingQueue());

                请求数 < coreSize 时，新增线程；
                请求数 >= coreSize && 队列不满时，添加任务入队；
                队列满时，此时因为 coreSize 和 maxSize 相等，任务会被直接拒绝。
                 这么写的最大目的：是想让线程一下子增加到 maxSize，并且不要回收线程，防止线程回收，避免不断增加回收的损耗，一般来说业务流量都有波峰低谷，在流量低谷时，线程不会被回收；流量波峰时，maxSize 的线程可以应对波峰，不需要慢慢初始化到 maxSize 的过程。   
               这样设置有两个前提条件：
                  allowCoreThreadTimeOut 我们采取默认 false，而不会主动设置成 true，allowCoreThreadTimeOut 是 false 的话，当线程空闲时，就不会回收核心线程；
                  keepAliveTime 和 TimeUnit 我们都会设置很大，这样线程空闲的时间就很长，线程就不会轻易的被回收。  
              2 maxSize 无界 + SynchronousQueue
                  在线程池选择队列时，我们也会看到有同学选择 SynchronousQueue，SynchronousQueue 我们在 《SynchronousQueue 源码解析》章节有说过，其内部有堆栈和队列两种形式，默认是堆栈的形式，其内部是没有存储的容器的，放元素和拿元素是一一对应的，比如我使用 put 方法放元素，如果此时没有对应的 take 操作的话，put 操作就会阻塞，需要有线程过来执行 take 操作后，put 操作才会返回。
                  基于此特点，如果要使用 SynchronousQueue 的话，我们需要尽量将 maxSize 设置大一点，这样就可以接受更多的请求。

                 假设我们设置 maxSize 是 10 的话，选择 SynchronousQueue 队列，假设所有请求都执行 put 操作，没有请求执行 take 操作，前 10 个 put 请求会消耗 10 个线程，都阻塞在 put 操作上，第 11 个请求过来后，请求就会被拒绝，所以我们才说尽量把 maxSize 设置大一点，防止请求被拒绝。

                 maxSize 无界 + SynchronousQueue 这样的组合方式优缺点都很明显：

                 优点：当任务被消费时，才会返回，这样请求就能够知道当前请求是已经在被消费了，如果是其他的队列的话，我们只知道任务已经被提交成功了，但无法知道当前任务是在被消费中，还是正在队列中堆积。

                 缺点：
                  比较消耗资源，大量请求到来时，我们会新建大量的线程来处理请求；
                 如果请求的量难以预估的话，maxSize 的大小也很难设置。
              3 maxSize 有界 + Queue 无界
                  在一些对实时性要求不大，但流量忽高忽低的场景下，可以使用 maxSize 有界 + Queue 无界的组合方式。
                  比如我们设置 maxSize 为 20，Queue 选择默认构造器的 LinkedBlockingQueue，这样做的优缺点如下：
                  优点：
                 电脑 cpu 固定的情况下，每秒能同时工作的线程数是有限的，此时开很多的线程其实也是浪费，还不如把这些请求放到队列中去等待，这样可以减少线程之间的 CPU 的竞争；
                 LinkedBlockingQueue 默认构造器构造出来的链表的最大容量是 Integer 的最大值，非常适合流量忽高忽低的场景，当流量高峰时，大量的请求被阻塞在队列中，让有限的线程可以慢慢消费。
                  缺点：流量高峰时，大量的请求被阻塞在队列中，对于请求的实时性难以保证，所以当对请 求的实时性要求较高的场景，不能使用该组合。  
               4 maxSize 有界 + Queue 有界
                 这种组合是对 3 缺点的补充，我们把队列从无界修改成有界，只要排队的任务在要求的时间内，能够完成任务即可。

                 这种组合需要我们把线程和队列的大小进行配合计算，保证大多数请求都可以在要求的时间内，有响应返回。  
               5 keepAliveTime 设置无穷大
                  有些场景下我们不想让空闲的线程被回收，于是就把 keepAliveTime 设置成 0，实际上这种设置是错误的，当我们把 keepAliveTime 设置成 0 时，线程使用 poll 方法在队列上进行超时阻塞时，会立马返回 null，也就是空闲线程会立马被回收。

                  所以如果我们想要空闲的线程不被回收，我们可以设置 keepAliveTime 为无穷大值，并且设置 TimeUnit 为时间的大单位，比如我们设置 keepAliveTime 为 365，TimeUnit 为 TimeUnit.DAYS，意思是线程空闲 1 年内都不会被回收。
                  在实际的工作中，机器的内存一般都够大，我们合理设置 maxSize 后，即使线程空闲，我们也不希望线程被回收，我们常常也会设置 keepAliveTime 为无穷大。   
              6 线程池的公用和独立
                   在实际工作中，某一个业务下的所有场景，我们都不会公用一个线程池，一般有以下几个原则：

                   查询和写入不公用线程池，互联网应用一般来说，查询量远远大于写入的量，如果查询和写入都要走线程池的话，我们一定不要公用线程池，也就是说查询走查询的线程池，写入走写入的线程池，如果公用的话，当查询量很大时，写入的请求可能会到队列中去排队，无法及时被处理；
                   多个写入业务场景看情况是否需要公用线程池，原则上来说，每个业务场景都独自使用自己的线程池，绝不共用，这样在业务治理、限流、熔断方面都比较容易，一旦多个业务场景公用线程池，可能就会造成业务场景之间的互相影响，现在的机器内存都很大，每个写入业务场景独立使用自己的线程池也是比较合理的；
                   多个查询业务场景是可以公用线程池的，查询的请求一般来说有几个特点：查询的场景多、rt 时间短、查询的量比较大，如果给每个查询场景都弄一个单独的线程池的话，第一个比较耗资源，第二个很难定义线程池中线程和队列的大小，比较复杂，所以多个相似的查询业务场景是可以公用线程池的。  
              7 7 如何算线程大小和队列大小
                  在实际的工作中，我们使用线程池时，需要慎重考虑线程的大小和队列的大小，主要从几个方面入手：

                根据业务进行考虑，初始化线程池时，我们需要考虑所有业务涉及的线程池，如果目前所有的业务同时都有很大流量，那么在对于当前业务设置线程池时，我们尽量把线程大小、队列大小都设置小，如果所有业务基本上都不会同时有流量，那么就可以稍微设置大一点；
                根据业务的实时性要求，如果实时性要求高的话，我们把队列设置小一点，coreSize == maxSize，并且设置 maxSize 大一点，如果实时性要求低的话，就可以把队列设置大一点。
                假设现在机器上某一时间段只会运行一种业务，业务的实时性要求较高，每个请求的平均 rt 是 200ms，请求超时时间是 2000ms，机器是 4 核 CPU，内存 16G，一台机器的 qps 是 100，这时候我们可以模拟一下如何设置：

                4 核 CPU，假设 CPU 能够跑满，每个请求的 rt 是 200ms，就是 200 ms 能执行 4 条请求，2000ms 内能执行 2000/200 * 4 = 40 条请求；
                200 ms 能执行 4 条请求，实际上 4 核 CPU 的性能远远高于这个，我们可以拍脑袋加 10 条，也就是说 2000ms 内预估能够执行 50 条；
                一台机器的 qps 是 100，此时我们计算一台机器 2 秒内最多处理 50 条请求，所以此时如果不进行 rt 优化的话，我们需要加至少一台机器。
                线程池可以大概这么设置：

                   ThreadPoolExecutor executor = new ThreadPoolExecutor(15, 15, 365L, TimeUnit.DAYS,
                                                     new LinkedBlockingQueue(35));
                   线程数最大为 15，队列最大为 35，这样机器差不多可以在 2000ms 内处理最大的请求 50 条，当然根据你机器的性能和实时性要求，你可以调整线程数和队列的大小占比，只要总和小于 50 即可。     
           -- 线程池流程编排中的运用实战        
               异步执行 SpringBean
                  从上述代码中，我们可以看到所有的 SpringBean 都是串行执行的，效率较低，我们在实际业务中发现，有的 SpringBean 完全可以异步执行，这样既能完成业务请求，又能减少业务处理的 rt，对于这个需求，我们条件反射的有了两个想法：

                     需要新开线程来异步执行 SpringBean，可以使用 Runable 或者 Callable；
                     业务请求量很大，我们不能每次来一个请求，就开一个线程，我们应该让线程池来管理异步执行的线程。
                    于是我们决定使用线程池来完成这个需求。

                  接口没预留，使用注解
                    我们新建一个注解，只要 SpringBean 上有该注解，表示该 SpringBean 应该异步执行，否则应该同步执行，新建的注解如下  
                     @Target(ElementType.TYPE)// 表示该注解应该打在类上
                     @Retention(RetentionPolicy.RUNTIME)
                     @Documented
                     public @interface AsyncComponent {

                     }
                     实现了两个 SpringBean：BeanOne 和 BeanTwo，其中 BeanTwo 被打上了 AsyncComponent 注解，表明 BeanTwo 应该被异步执行，两个 SpringBean 都打印出执行的线程的名称。
                mock 流程引擎数据中心
                新建线程池
                   现在我们需要区分 SpringBean 是否是异步的，如果是异步的，丢到线程池中去执行，如果是同步的，仍然使用原来的方法进行执行，于是我们把这些逻辑封装到一个工具类中

                    // 如果 SpringBean 上有 AsyncComponent 注解，表示该 SpringBean 需要异步执行，就丢到线程池中去
                     public static final void run(DomainAbilityBean component, FlowContent content) {
                 // 判断类上是否有 AsyncComponent 注解
                  if (AnnotationUtils.isAnnotationPresent(AsyncComponent.class, AopUtils.getTargetClass(component))) {
                      // 提交到线程池中
                    executor.submit(() -> { component.invoke(content); });
                  return;
                   }
                     // 同步 SpringBean 直接执行。
                   component.invoke(content);
           -- Java 并发类库提供的线程池有哪几种？ 分别有什么特点
               利用 Executors 提供的通用线程池创建方法，去创建不同配置的线程池，主要区别在于不同的 ExecutorService 类型或者不同的初始参数
               Executors 目前提供了 5 种不同的线程池创建配置：newCachedThreadPool()，它是一种用来处理大量短时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置的时间超过 60 秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用 SynchronousQueue 作为工作队列。newFixedThreadPool(int nThreads)，重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有 nThreads 个工作线程是活动的。这意味着，如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的工作线程被创建，以补足指定的数目 nThreads。newSingleThreadExecutor()，它的特点在于工作线程数目被限制为 1，操作一个无界的工作队列，所以它保证了所有任务的都是被顺序执行，最多会有一个任务处于活动状态，并且不允许使用者改动线程池实例，因此可以避免其改变线程数目。newSingleThreadScheduledExecutor() 和 newScheduledThreadPool(int corePoolSize)，创建的是个 ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程。newWorkStealingPool(int parallelism)，这是一个经常被人忽略的线程池，Java 8 才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序

               线程池框架
                  Executor ->ExecutorService->AbstractExecutorService
                           ->Executors->ThreadPoolExecutor->ScheduledThreadExcutor
               设计目的
                  Executor 是一个基础的接口，其初衷是将任务提交和任务执行细节解耦，这一点可以体会其定义的唯一方法。
                  void execute(Runnable command);
                  ExecutorService 则更加完善，不仅提供 service 的管理功能，比如 shutdown 等方法，也提供了更加全面的提交任务机制，如返回Future而不是 void 的 submit 方法
                  <T> Future<T> submit(Callable<T> task);
                  Executors 则从简化使用的角度，为我们提供了各种方便的静态工厂方法。
                  ScheduledThreadPoolExecutor 是 ThreadPoolExecutor 的扩展，主要是增加了调度逻辑
                  ForkJoinPool 则是为 ForkJoinTask 定制的线程池，与通常意义的线程池有所不同
               简单理解
                 工作队列负责存储用户提交的各个任务，这个工作队列，可以是容量为 0 的 SynchronousQueue（使用 newCachedThreadPool），也可以是像固定大小线程池（newFixedThreadPool）那样使用 LinkedBlockingQueue。
                   private final BlockingQueue<Runnable> workQueue;
                 内部的“线程池”，这是指保持工作线程的集合，线程池需要在运行过程中管理线程创建、销毁。例如，对于带缓存的线程池，当任务压力较大时，线程池会创建新的工作线程；当业务压力退去，线程池会在闲置一段时间（默认 60 秒）后结束线程。  
                    rivate final HashSet<Worker> workers = new HashSet<>();
                 线程池的工作线程被抽象为静态内部类 Worker，基于AQS实现。
                 ThreadFactory 提供上面所需要的创建线程逻辑。如果任务提交时被拒绝，比如线程池已经处于 SHUTDOWN 状态，需要为其提供处理逻辑，Java 标准库提供了类似ThreadPoolExecutor.AbortPolicy等默认实现，也可以按照实际需求自定义。
               线程池构造函数参数
                  corePoolSize，所谓的核心线程数，可以大致理解为长期驻留的线程数目（除非设置了 allowCoreThreadTimeOut）。对于不同的线程池，这个值可能会有很大区别，比如 newFixedThreadPool 会将其设置为 nThreads，而对于 newCachedThreadPool 则是为 0。
                  maximumPoolSize，顾名思义，就是线程不够时能够创建的最大线程数。同样进行对比，对于 newFixedThreadPool，当然就是 nThreads，因为其要求是固定大小，而 newCachedThreadPool 则是 Integer.MAX_VALUE。
                  keepAliveTime 和 TimeUnit，这两个参数指定了额外的线程能够闲置多久，显然有些线程池不需要它。
                  workQueue，工作队列，必须是 BlockingQueue。 
               其他属性
                 这里有一个非常有意思的设计，ctl 变量被赋予了双重角色，通过高低位的不同，既表示线程池状态，又表示工作线程数目，这是一个典型的高效优化
                 // 线程池状态，存储在数字的高位private static final int RUNNING = -1 << COUNT_BITS;
                 线程池状态
                 idle-(execute/submit)>running-(shutdown)>shutdown
                                              -(shutdownNow)>stop  
                                                                -(队列和线程池为空/队列为空)>tidying
                                                                                        ->ternminated  
               execute源码分析
                  int c = ctl.get();// 检查工作线程数目，低于corePoolSize则添加Worker  if (workerCountOf(c) < corePoolSize) {      if (addWorker(command, true))
                  // isRunning就是检查线程池是否被shutdown// 工作队列可能是有界的，offer是比较友好的入队方式  if (isRunning(c) && workQueue.offer(command)) {      int recheck = ctl.get();  
               线程池实践
                  1避免任务堆积。前面我说过 newFixedThreadPool 是创建指定数目的线程，但是其工作队列是无界的，如果工作线程数目太少，导致处理跟不上入队的速度，这就很有可能占用大量系统内存，甚至是出现 OOM。诊断时，你可以使用 jmap 之类的工具，查看是否有大量的任务对象入队。
                  2避免过度扩展线程。我们通常在处理大量短时任务时，使用缓存的线程池，比如在最新的 HTTP/2 client API 中，目前的默认实现就是如此。我们在创建线程池的时候，并不能准确预计任务压力有多大、数据特征是什么样子（大部分请求是 1K 、100K 还是 1M 以上？），所以很难明确设定一个线程数目。
                  3另外，如果线程数目不断增长（可以使用 jstack 等工具检查），也需要警惕另外一种可能性，就是线程泄漏，这种情况往往是因为任务逻辑有问题，导致工作线程迟迟不能被释放。建议你排查下线程栈，很有可能多个线程都是卡在近似的代码处。
                  4避免死锁等同步问题，对于死锁的场景和排查，你可以复习专栏第 18 讲。
                  5尽量避免在使用线程池时操作 ThreadLocal，同样是专栏第 17 讲已经分析过的，通过今天的线程池学习，应该更能理解其原因，工作线程的生命周期通常都会超过任务的生命周期。
               线程池大小的选择策略
                   1如果我们的任务主要是进行计算，那么就意味着 CPU 的处理能力是稀缺的资源，我们能够通过大量增加线程数提高计算能力吗？往往是不能的，如果线程太多，反倒可能导致大量的上下文切换开销。所以，这种情况下，通常建议按照 CPU 核的数目 N 或者 N+1 
                   2 如果是需要较多等待的任务，例如 I/O 操作比较多 
                     线程数 = CPU核数 × 目标CPU利用率 ×（1 + 平均等待时间/平均工作时间）  
           -- Executor与线程池：如何创建正确的线程池？
              创建对象，仅仅是在 JVM 的堆里分配一块内存而已；而创建一个线程，却需要调用操作系统内核的 API，然后操作系统要为线程分配一系列的资源，这个成本就很高了，所以线程是一个重量级的对象，应该避免频繁创建和销毁。

              线程池是一种生产者 - 消费者模式 

                线程池的使用方是生产者，线程池本身是消费者。               
              如何使用 Java 中的线程池 
                Java 提供的线程池相关的工具类中，最核心的是 ThreadPoolExecutor，通过名字你也能看出来，它强调的是 Executor，而不是一般意义上的池化资源。
                你可以把线程池类比为一个项目组，而线程就是项目组的成员。

                corePoolSize：表示线程池保有的最小线程数。有些项目很闲，但是也不能把人都撤了，至少要留 corePoolSize 个人坚守阵地。maximumPoolSize：表示线程池创建的最大线程数。当项目很忙时，就需要加人，但是也不能无限制地加，最多就加到 maximumPoolSize 个人。当项目闲下来时，就要撤人了，最多能撤到 corePoolSize 个人。
                keepAliveTime & unit：上面提到项目根据忙闲来增减人员，那在编程世界里，如何定义忙和闲呢？很简单，一个线程如果在一段时间内，都没有执行任务，说明很闲，keepAliveTime 和 unit 就是用来定义这个“一段时间”的参数。也就是说，如果一个线程空闲了keepAliveTime & unit这么久，而且线程池的线程数大于 corePoolSize ，那么这个空闲的线程就要被回收了。
                workQueue：工作队列，和上面示例代码的工作队列同义。
                threadFactory：通过这个参数你可以自定义如何创建线程，例如你可以给线程指定一个有意义的名字。
                handler：通过这个参数你可以自定义任务的拒绝策略。如果线程池中所有的线程都在忙碌，并且工作队列也满了（前提是工作队列是有界队列），那么此时提交任务，线程池就会拒绝接收。至于拒绝的策略，你可以通过 handler 这个参数来指定。ThreadPoolExecutor 已经提供了以下 4 种策略。
                CallerRunsPolicy：提交任务的线程自己去执行该任务。
                AbortPolicy：默认的拒绝策略，会 throws RejectedExecutionException。
                DiscardPolicy：直接丢弃任务，没有任何异常抛出。
                DiscardOldestPolicy：丢弃最老的任务，其实就是把最早进入工作队列的任务丢弃，然后把新任务加入到工作队列。
                Java 在 1.6 版本还增加了 allowCoreThreadTimeOut(boolean value) 方法，它可以让所有线程都支持超时，这意味着如果项目很闲，就会将项目组的成员都撤走。 
              使用线程池要注意些什么 

                考虑到 ThreadPoolExecutor 的构造函数实在是有些复杂，所以 Java 并发包里提供了一个线程池的静态工厂类 Executors，利用 Executors 你可以快速创建线程池  
                不建议使用 Executors 的最重要的原因是：Executors 提供的很多方法默认使用的都是无界的 LinkedBlockingQueue，高负载情境下，无界队列很容易导致 OOM，而 OOM 会导致所有请求都无法处理，这是致命问题。所以强烈建议使用有界队列。

                使用有界队列，当任务过多时，线程池会触发执行拒绝策略，线程池默认的拒绝策略会 throw RejectedExecutionException 这是个运行时异常，对于运行时异常编译器并不强制 catch 它，所以开发人员很容易忽略。因此默认拒绝策略要慎重使用。如果线程池处理的任务非常重要，建议自定义自己的拒绝策略；并且在实际工作中，自定义的拒绝策略往往和降级策略配合使用。

                使用线程池，还要注意异常处理的问题，例如通过 ThreadPoolExecutor 对象的 execute() 方法提交任务时，如果任务在执行的过程中出现运行时异常，会导致执行任务的线程终止；不过，最致命的是任务虽然异常了，但是你却获取不到任何通知，这会让你误以为任务都执行得很正常。虽然线程池提供了很多用于异常处理的方法，但是最稳妥和简单的方案还是捕获所有异常并按需处理  
           -- Future：如何用多线程实现最优的“烧水泡茶”程序？      
                 我们仅仅介绍了 ThreadPoolExecutor 的 void execute(Runnable command) 方法，利用这个方法虽然可以提交任务，但是却没有办法获取任务的执行结果（execute() 方法没有返回值）。而很多场景下，我们又都是需要获取任务的执行结果的。那 ThreadPoolExecutor 是否提供了相关功能呢？必须的，这么重要的功能当然需要提供了

                 如何获取任务执行结果  
                   Java 通过 ThreadPoolExecutor 提供的 3 个 submit() 方法和 1 个 FutureTask 工具类来支持获得任务执行结果的需求 
                   // 提交Runnable任务
                   Future<?>   submit(Runnable task);
                   // 提交Callable任务
                   <T> Future<T>   submit(Callable<T> task);
                   // 提交Runnable任务及结果引用 
                   <T> Future<T>   submit(Runnable task, T result);
                   你会发现它们的返回值都是 Future 接口，Future 接口有 5 个方法，我都列在下面了，它们分别是取消任务的方法 cancel()、判断任务是否已取消的方法 isCancelled()、判断任务是否已结束的方法 isDone()以及2 个获得任务执行结果的 get() 和 get(timeout, unit)，
                   不过需要注意的是：这两个 get() 方法都是阻塞式的，如果被调用的时候，任务还没有执行完，那么调用 get() 方法的线程会阻塞，直到任务执行完才会被唤醒。
                   这 3 个 submit() 方法之间的区别在于方法参数不同 
                   提交 Runnable 任务 submit(Runnable task) ：这个方法的参数是一个 Runnable 接口，Runnable 接口的 run() 方法是没有返回值的，所以 submit(Runnable task) 这个方法返回的 Future 仅可以用来断言任务已经结束了，类似于 Thread.join()。
                   提交 Callable 任务 submit(Callable task)：这个方法的参数是一个 Callable 接口，它只有一个 call() 方法，并且这个方法是有返回值的，所以这个方法返回的 Future 对象可以通过调用其 get() 方法来获取任务的执行结果。
                   提交 Runnable 任务及结果引用 submit(Runnable task, T result)：这个方法很有意思，假设这个方法返回的 Future 对象是 f，f.get() 的返回值就是传给 submit() 方法的参数 result。这个方法该怎么用呢？下面这段示例代码展示了它的经典用法。需要你注意的是 Runnable 接口的实现类 Task 声明了一个有参构造函数 Task(Result r) ，创建 Task 对象的时候传入了 result 对象，这样就能在类 Task 的 run() 方法中对 result 进行各种操作了。
                   result 相当于主线程和子线程之间的桥梁，通过它主子线程可以共享数据。
                   // 创建Result对象rResult r = new Result();r.setAAA(a);// 提交任务Future<Result> future =   executor.submit(new Task(r), r); 

                   那如何使用 FutureTask 呢？其实很简单，FutureTask 实现了 Runnable 和 Future 接口，由于实现了 Runnable 接口，所以可以将 FutureTask 对象作为任务提交给 ThreadPoolExecutor 去执行，也可以直接被 Thread 执行；又因为实现了 Future 接口，所以也能用来获得任务的执行结果。下面的示例代码是将 FutureTask 对象提交给 ThreadPoolExecutor 去执行。

                   // 创建FutureTaskFutureTask<Integer> futureTask  = new FutureTask<>(()-> 1+2);// 创建并启动线程Thread T1 = new Thread(futureTask);T1.start();// 获取计算结果Integer result = futureTask.get();  

                 实现最优的“烧水泡茶”程序 
                   并发编程可以总结为三个核心问题：分工、同步和互斥。编写并发程序，首先要做的就是分工，所谓分工指的是如何高效地拆解任务并分配给线程
                   下面的示例代码就是用这一章提到的 Future 特性来实现的。首先，我们创建了两个 FutureTask——ft1 和 ft2，ft1 完成洗水壶、烧开水、泡茶的任务，ft2 完成洗茶壶、洗茶杯、拿茶叶的任务；这里需要注意的是 ft1 这个任务在执行泡茶任务前，需要等待 ft2 把茶叶拿来，所以 ft1 内部需要引用 ft2，并在执行泡茶之前，调用 ft2 的 get() 方法实现等待。

                   // 创建任务T2的FutureTaskFutureTask<String> ft2  = new FutureTask<>(new T2Task());// 创建任务T1的FutureTaskFutureTask<String> ft1  = new FutureTask<>(new T1Task(ft2));// 线程T1执行任务ft1Thread T1 = new Thread(ft1);T1.start();// 线程T2执行任务ft2Thread T2 = new Thread(ft2);T2.start();// 等待线程T1执行结果System.out.println(ft1.get());// T1Task需要执行的任务：// 洗水壶、烧开水、泡茶class T1Task implements Callable<String>{  FutureTask<String> ft2;  // T1任务需要T2任务的FutureTask  T1Task(FutureTask<String> ft2){    this.ft2 = ft2;  }
                 利用 Java 并发包提供的 Future 可以很容易获得异步任务的执行结果，无论异步任务是通过线程池 ThreadPoolExecutor 执行的，还是通过手工创建子线程来执行的。Future 可以类比为现实世界里的提货单，比如去蛋糕店订生日蛋糕，蛋糕店都是先给你一张提货单，你拿到提货单之后，没有必要一直在店里等着，可以先去干点其他事，比如看场电影；等看完电影后，基本上蛋糕也做好了，然后你就可以凭提货单领蛋糕了。  
                 利用多线程可以快速将一些串行的任务并行化，从而提高性能；如果任务之间有依赖关系，比如当前任务依赖前一个任务的执行结果，这种问题基本上都可以用 Future 来解决  
           -- CompletableFuture：异步编程没那么难      
               异步化，是并行方案得以实施的基础，更深入地讲其实就是：利用多线程优化性能这个核心方案得以实施的基础。

               Java 在 1.8 版本提供了 CompletableFuture 来支持异步编程，CompletableFuture 有可能是你见过的最复杂的工具类了，不过功能也着实让人感到震撼。

               1无需手工维护线程，没有繁琐的手工维护线程的工作，给任务分配线程的工作也不需要我们关注；
               2语义更清晰，例如 f3 = f1.thenCombine(f2, ()->{}) 能够清晰地表述“任务 3 要等待任务 1 和任务 2 都完成后才能开始”；
               3代码更简练并且专注于业务逻辑，几乎所有代码都是业务逻辑相关的。
               //任务1：洗水壶->烧开水CompletableFuture f1 = CompletableFuture.runAsync(()->{
               //任务2：洗茶壶->洗茶杯->拿茶叶CompletableFuture f2 = CompletableFuture.supplyAsync(()->{
                //任务3：任务1和任务2完成后执行：泡茶CompletableFuture f3 = f1.thenCombine(f2, (__, tf)->{
               创建 CompletableFuture 对象   
                 创建 CompletableFuture 对象主要靠下面代码中展示的这 4 个静态方法，我们先看前两个。在烧水泡茶的例子中，我们已经使用了runAsync(Runnable runnable)和supplyAsync(Supplier<U> supplier)，它们之间的区别是：Runnable 接口的 run() 方法没有返回值，而 Supplier 接口的 get() 方法是有返回值的


                 前两个方法和后两个方法的区别在于：后两个方法可以指定线程池参数。默认情况下 CompletableFuture 会使用公共的 ForkJoinPool 线程池，这个线程池默认创建的线程数是 CPU 的核数（也可以通过 JVM option:-Djava.util.concurrent.ForkJoinPool.common.parallelism 来设置 ForkJoinPool 线程池的线程数）。如果所有 CompletableFuture 共享一个线程池，那么一旦有任务执行一些很慢的 I/O 操作，就会导致线程池中所有线程都阻塞在 I/O 操作上，从而造成线程饥饿，进而影响整个系统的性能。所以，强烈建议你要根据不同的业务类型创建不同的线程池，以避免互相干扰。

                 //可以指定线程池  static CompletableFuture<Void>   runAsync(Runnable runnable, Executor executor)static <U> CompletableFuture<U>   supplyAsync(Supplier<U> supplier, Executor executor) 

                 创建完 CompletableFuture 对象之后，会自动地异步执行 runnable.run() 方法或者 supplier.get() 方法，对于一个异步操作，你需要关注两个问题：一个是异步操作什么时候结束，另一个是如何获取异步操作的执行结果。因为 CompletableFuture 类实现了 Future 接口，所以这两个问题你都可以通过 Future 接口来解决。另外，CompletableFuture 类还实现了 CompletionStage 接口  
               如何理解 CompletionStage 接口 
                 我觉得，你可以站在分工的角度类比一下工作流。任务是有时序关系的，比如有串行关系、并行关系、汇聚关系等。这样说可能有点抽象，这里还举前面烧水泡茶的例子，其中洗水壶和烧开水就是串行关系，洗水壶、烧开水和洗茶壶、洗茶杯这两组任务之间就是并行关系，而烧开水、拿茶叶和泡茶就是汇聚关系。
                 CompletionStage 接口可以清晰地描述任务之间的这种时序关系，例如前面提到的 f3 = f1.thenCombine(f2, ()->{}) 描述的就是一种汇聚关系。烧水泡茶程序中的汇聚关系是一种 AND 聚合关系，这里的 AND 指的是所有依赖的任务（烧开水和拿茶叶）都完成后才开始执行当前任务（泡茶）。既然有 AND 聚合关系，那就一定还有 OR 聚合关系，所谓 OR 指的是依赖的任务只要有一个完成就可以执行当前任务

                 1. 描述串行关系
                   CompletionStage 接口里面描述串行关系，主要是 thenApply、thenAccept、thenRun 和 thenCompose 这四个系列的接口。
                   thenApply 系列函数里参数 fn 的类型是接口 Function<T, R>，这个接口里与 CompletionStage 相关的方法是 R apply(T t)，这个方法既能接收参数也支持返回值，所以 thenApply 系列方法返回的是CompletionStage<R>。
                   而 thenAccept 系列方法里参数 consumer 的类型是接口Consumer<T>，这个接口里与  CompletionStage 相关的方法是 void accept(T t)，这个方法虽然支持参数，但却不支持回值，所以 thenAccept 系列方法返回的是CompletionStage<Void>。
                   thenRun 系列方法里 action 的参数是 Runnable，所以 action 既不能接收参数也不支持返回值，所以 thenRun 系列方法返回的也是CompletionStage<Void>。
                   这些方法里面 Async 代表的是异步执行 fn、consumer 或者 action。其中，需要你注意的是  thenCompose 系列方法，这个系列的方法会新创建出一个子流程，最终结果和 thenApply 系列是相同的。  
                 2. 描述 AND 汇聚关系

                      CompletionStage 接口里面描述 AND 汇聚关系，主要是 thenCombine、thenAcceptBoth 和 runAfterBoth 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同。它们的使用你可以参考上面烧水泡茶的实现程序，
                 3. 描述 OR 汇聚关系    

                       CompletionStage 接口里面描述 OR 汇聚关系，主要是 applyToEither、acceptEither 和  runAfterEither 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同。
                 4. 异常处理      

                    虽然上面我们提到的 fn、consumer、action 它们的核心方法都不允许抛出可检查异常，但是却无法限制它们抛出运行时异常，
                    下面的示例代码展示了如何使用 exceptionally() 方法来处理异常，exceptionally() 的使用非常类似于 try{}catch{}中的 catch{}，但是由于支持链式编程方式，所以相对更简单。既然有 try{}catch{}，那就一定还有 try{}finally{}，whenComplete() 和 handle() 系列方法就类似于 try{}finally{}中的 finally{}，无论是否发生异常都会执行 whenComplete() 中的回调函数 consumer 和 handle() 中的回调函数 fn。whenComplete() 和 handle() 的区别在于 whenComplete() 不支持返回结果，而 handle() 是支持返回结果的。CompletableFuture f0 = CompletableFuture .supplyAsync(()->7/0)) .thenApply(r->r*10) .exceptionally(e->0);System.out.println(f0.join());  
           -- CompletionService：如何批量执行异步任务？          
                利用 CompletionService 实现询价系统
                   因为 Java SDK 并发包里已经提供了设计精良的 CompletionService。利用 CompletionService 不但能帮你解决先获取到的报价先保存到数据库的问题，而且还能让代码更简练。
                   CompletionService 的实现原理也是内部维护了一个阻塞队列，当任务执行结束就把任务的执行结果加入到阻塞队列中，不同的是 CompletionService 是把任务执行结果的 Future 对象加入到阻塞队列中，而上面的示例代码是把任务最终的执行结果放入了阻塞队列中。
                那到底该如何创建 CompletionService 呢
                   CompletionService 接口的实现类是 ExecutorCompletionService，这个实现类的构造方法有两个，分别是：
                   ExecutorCompletionService(Executor executor)；
                   ExecutorCompletionService(Executor executor, BlockingQueue<Future<V>> completionQueue)

                   这两个构造方法都需要传入一个线程池，如果不指定 completionQueue，那么默认会使用无界的 LinkedBlockingQueue。任务执行结果的 Future 对象就是加入到 completionQueue 中。

                    下面的示例代码完整地展示了如何利用 CompletionService 来实现高性能的询价系统。其中，我们没有指定 completionQueue，因此默认使用无界的 LinkedBlockingQueue。之后通过 CompletionService 接口提供的 submit() 方法提交了三个询价操作，这三个询价操作将会被 CompletionService 异步执行。最后，我们通过 CompletionService 接口提供的 take() 方法获取一个 Future 对象（前面我们提到过，加入到阻塞队列中的是任务执行结果的 Future 对象），调用 Future 对象的 get() 方法就能返回询价操作的执行结果了。

                    // 创建线程池ExecutorService executor = Executors.newFixedThreadPool(3);// 创建CompletionServiceCompletionService cs = new ExecutorCompletionService<>(executor);
                    // 异步向电商S1询价cs.submit(()->getPriceByS1());
                    // 异步向电商S2询价cs.submit(()->getPriceByS2());
                    // 异步向电商S3询价cs.submit(()->getPriceByS3());
                    // 将询价结果异步保存到数据库for (int i=0; i<3; i++) { Integer r = cs.take().get(); executor.execute(()->save(r));}  
                CompletionService 接口说明 
                  Future submit(Callable task);
                  Future submit(Runnable task, V result);
                  Future take() throws InterruptedException;
                  Future poll();
                  Future poll(long timeout, TimeUnit unit) throws InterruptedException;
                  CompletionService 接口其余的 3 个方法，都是和阻塞队列相关的，take()、poll() 都是从阻塞队列中获取并移除一个元素；它们的区别在于如果阻塞队列是空的，那么调用 take() 方法的线程会被阻塞，而 poll() 方法会返回 null 值。 poll(long timeout, TimeUnit unit) 方法支持以超时的方式获取并移除阻塞队列头部的一个元素，如果等待了 timeout unit 时间，阻塞队列还是空的，那么该方法会返回 null 值。       
                利用 CompletionService 实现 Dubbo 中的 Forking Cluster   

                   Dubbo 中有一种叫做 Forking 的集群模式，这种集群模式下，支持并行地调用多个查询服务，只要有一个成功返回结果，整个服务就可以返回了
                   例如你需要提供一个地址转坐标的服务，为了保证该服务的高可用和性能，你可以并行地调用 3 个地图服务商的 API，然后只要有 1 个正确返回了结果 r，那么地址转坐标这个服务就可以直接返回 r 了。这种集群模式可以容忍 2 个地图服务商服务异常，但缺点是消耗的资源偏多。

                   利用 CompletionService 可以快速实现 Forking 这种集群模式，比如下面的示例代码就展示了具体是如何实现的。首先我们创建了一个线程池 executor 、一个 CompletionService 对象 cs 和一个Future<Integer>类型的列表 futures，每次通过调用 CompletionService 的 submit() 方法提交一个异步任务，会返回一个 Future 对象，我们把这些 Future 对象保存在列表 futures 中。通过调用 cs.take().get()，我们能够拿到最快返回的任务执行结果，只要我们拿到一个正确返回的结果，就可以取消所有任务并且返回最终结果了。
                   // 创建线程池ExecutorService executor =  Executors.newFixedThreadPool(3);
                   // 创建CompletionServiceCompletionService<Integer> cs =  new ExecutorCompletionService<>(executor);
                   // 用于保存Future对象List<Future<Integer>> futures =  new ArrayList<>(3);//提交异步任务，并保存future到futures 
                   futures.add(  cs.submit(()->geocoderByS1()));
                   futures.add(  cs.submit(()->geocoderByS2()));
                   futures.add(  cs.submit(()->geocoderByS3()));
                   // 获取最快返回的任务执行结果Integer r = 0;try {  // 只要有一个成功返回，则break  for (int i = 0; i < 3; ++i) {    r = cs.take().get();    //简单地通过判空来检查是否成功返回    if (r != null) {      break;    }  }}
                   finally {  //取消所有任务  for(Future<Integer> f : futures)    f.cancel(true);}// 返回结果return r;  
                当需要批量提交异步任务的时候建议你使用 CompletionService。CompletionService 将线程池 Executor 和阻塞队列 BlockingQueue 的功能融合在了一起，能够让批量异步任务的管理更简单。除此之外，CompletionService 能够让异步任务的执行结果有序化，先执行完的先进入阻塞队列，利用这个特性，你可以轻松实现后续处理的有序性，避免无谓的等待，同时还可以快速实现诸如 Forking Cluster 这样的需求。   

                CompletionService 的实现类 ExecutorCompletionService，需要你自己创建线程池，虽看上去有些啰嗦，但好处是你可以让多个 ExecutorCompletionService 的线程池隔离，这种隔离性能避免几个特别耗时的任务拖垮整个应用的风险。
           -- Fork/Join：单机版的MapReduce
              。对于简单的并行任务，你可以通过“线程池 +Future”的方案来解决；如果任务之间有聚合关系，无论是 AND 聚合还是 OR 聚合，都可以通过 CompletableFuture 来解决；而批量的并行任务，则可以通过 CompletionService 来解决。  
              上面提到的简单并行、聚合、批量并行这三种任务模型，基本上能够覆盖日常工作中的并发场景了，但还是不够全面，因为还有一种“分治”的任务模型没有覆盖到。
              分治，顾名思义，即分而治之，是一种解决复杂问题的思维方法和模式；具体来讲，指的是把一个复杂的问题分解成多个相似的子问题，然后再把子问题分解成更小的子问题，直到子问题简单到可以直接求解。

              分治思想在很多领域都有广泛的应用，例如算法领域有分治算法（归并排序、快速排序都属于分治算法，二分法查找也是一种分治算法）；大数据领域知名的计算框架 MapReduce 背后的思想也是分治。既然分治这种任务模型如此普遍，那 Java 显然也需要支持，Java 并发包里提供了一种叫做 Fork/Join 的并行计算框架，就是用来支持分治这种任务模型的。

              分治任务模型

                这里你需要先深入了解一下分治任务模型，分治任务模型可分为两个阶段：一个阶段是任务分解，也就是将任务迭代地分解为子任务，直至子任务可以直接计算出结果；另一个阶段是结果合并，即逐层合并子任务的执行结果，直至获得最终结果

                在这个分治任务模型里，任务和分解后的子任务具有相似性，这种相似性往往体现在任务和子任务的算法是相同的，但是计算的数据规模是不同的。具备这种相似性的问题，我们往往都采用递归算法。  

              Fork/Join 的使用

                Fork/Join 是一个并行计算的框架，主要就是用来支持分治任务模型的，这个计算框架里的 Fork 对应的是分治任务模型里的任务分解，Join 对应的是结果合并。Fork/Join 计算框架主要包含两部分，一部分是分治任务的线程池 ForkJoinPool，另一部分是分治任务 ForkJoinTask  
                这两部分的关系类似于 ThreadPoolExecutor 和 Runnable 的关系，都可以理解为提交任务到线程池，只不过分治任务有自己独特类型 ForkJoinTask。
                ForkJoinTask 是一个抽象类，它的方法有很多，最核心的是 fork() 方法和 join() 方法，其中 fork() 方法会异步地执行一个子任务，而 join() 方法则会阻塞当前线程来等待子任务的执行结果。ForkJoinTask 有两个子类——RecursiveAction 和 RecursiveTask，通过名字你就应该能知道，它们都是用递归的方式来处理分治任务的。这两个子类都定义了抽象方法 compute()，不过区别是 RecursiveAction 定义的 compute() 没有返回值，而 RecursiveTask 定义的 compute() 方法是有返回值的。这两个子类也是抽象类，在使用的时候，需要你定义子类去扩展。

                接下来我们就来实现一下，看看如何用 Fork/Join 这个并行计算框架计算斐波那契数列（下面的代码源自 Java 官方示例）。首先我们需要创建一个分治任务线程池以及计算斐波那契数列的分治任务，之后通过调用分治任务线程池的 invoke() 方法来启动分治任务。由于计算斐波那契数列需要有返回值，所以 Fibonacci 继承自 RecursiveTask。分治任务 Fibonacci 需要实现 compute() 方法，这个方法里面的逻辑和普通计算斐波那契数列非常类似，区别之处在于计算 Fibonacci(n - 1) 使用了异步子任务，这是通过 f1.fork() 这条语句实现的。
                 //创建分治任务线程池    
                 ForkJoinPool fjp =     new ForkJoinPool(4);  
                 //创建分治任务  Fibonacci fib =     new Fibonacci(30);     
                 //启动分治任务    Integer result =     fjp.invoke(fib);
                 //递归任务static class Fibonacci extends     RecursiveTask<Integer>
                 {  final int n;  Fibonacci(int n){this.n = n;}  
                 protected Integer compute(){    if (n <= 1)      return n;  
                   Fibonacci f1 =       new Fibonacci(n - 1);    
                   //创建子任务      
                   f1.fork();    
                   Fibonacci f2 =       new Fibonacci(n - 2);    
                   //等待子任务结果，并合并结果      
                   return f2.compute() + f1.join();  }    

              ForkJoinPool 工作原理  
                  ThreadPoolExecutor 本质上是一个生产者 - 消费者模式的实现，内部有一个任务队列，这个任务队列是生产者和消费者通信的媒介；ThreadPoolExecutor 可以有多个工作线程，但是这些工作线程都共享一个任务队列。   

                  ForkJoinPool 本质上也是一个生产者 - 消费者的实现，但是更加智能，你可以参考下面的 ForkJoinPool 工作原理图来理解其原理。ThreadPoolExecutor 内部只有一个任务队列，而 ForkJoinPool 内部有多个任务队列，当我们通过 ForkJoinPool 的 invoke() 或者 submit() 方法提交任务时，ForkJoinPool 根据一定的路由规则把任务提交到一个任务队列中，如果任务在执行过程中会创建出子任务，那么子任务会提交到工作线程对应的任务队列中。如果工作线程对应的任务队列空了，是不是就没活儿干了呢？不是的，ForkJoinPool 支持一种叫做“任务窃取”的机制，如果工作线程空闲了，那它可以“窃取”其他工作任务队列里的任务，例如下图中，线程 T2 对应的任务队列已经空了，它可以“窃取”线程 T1 对应的任务队列的任务。如此一来，所有的工作线程都不会闲下来了。ForkJoinPool 中的任务队列采用的是双端队列，工作线程正常获取任务和“窃取任务”分别是从任务队列不同的端消费，这样能避免很多不必要的数据竞争。我们这里介绍的仅仅是简化后的原理，ForkJoinPool 的实现远比我们这里介绍的复杂

                  Fork/Join 并行计算框架主要解决的是分治任务。分治的核心思想是“分而治之”：将一个大的任务拆分成小的子任务去解决，然后再把子任务的结果聚合起来从而得到最终结果。这个过程非常类似于大数据处理中的 MapReduce，所以你可以把 Fork/Join 看作单机版的 MapReduce。

                  Fork/Join 并行计算框架的核心组件是 ForkJoinPool。ForkJoinPool 支持任务窃取机制，能够让所有线程的工作量基本均衡，不会出现有的线程很忙，而有的线程很闲的状况，所以性能很好。Java 1.8 提供的 Stream API 里面并行流也是以 ForkJoinPool 为基础的。不过需要你注意的是，默认情况下所有的并行流计算都共享一个 ForkJoinPool，这个共享的 ForkJoinPool 默认的线程数是 CPU 的核数；如果所有的并行流计算都是 CPU 密集型计算的话，完全没有问题，但是如果存在 I/O 密集型的并行流计算，那么很可能会因为一个很慢的 I/O 计算而拖慢整个系统的性能。所以建议用不同的 ForkJoinPool 执行不同类型的计算任务
			  -- 并发控制：
			    	- barrier、countdownlatch、exchanger、future、semaphore
            提供了比sychronized更加高级的同步结构 包括CountDownLatch CyclicBarrier Semaphore
                 比如利用semaphore作为资源控制器 限制同时进行工作的线程数量
              各种线程安全的容器 比如ConcurrentHashMap 有序的ConsurrentSkipListMap 通过类似快照机制，实现线程安全的动态数组
              CopyOnWriteArrayList

              各种并发队列实现  BlockingQueue实现   比较典型的ArrayBlockingQueue,SynchronousQueue针对特定场景PriorityBlockingQueue

              强大的WExecutor框架 可以创建不同类型的线程池，调度任务，绝大多数情况下，不再需要自己从头实线程池和任务调度器

              多线程编程的目的
                开勇多线程提高程序的扩展能力，已达到业务对吞吐量的要求
                协调线程间调度 交互 完成业务逻辑
                线程间传递数据和状态同样实现业务逻辑的需要。

                CountDownLatch 允许一个或多个线程等待某些操作完成

                CyclicBrrier 一种辅助性的听不结构允许多个线程等待到达某个屏障
                
                Semaphore java版本的信号量实现  通过控制一定数量的允许的方式，来达到限制通用资源访问的目的。
                在车，机场等出租车，当有很多空出租车就位时，为防止过度拥挤，调度员指挥排队等待坐车的队伍一次进来5人上车，等这5个人坐车出发，在放进去下一批。

                

                 Semaphore.acquire()
                 semaphore.release()


                 线程试图获得工作允许，得到许可进行任务，然后释放许可，这时等待许可的其他线程，就可获得许可进入工作状态，
                 直到全部处理结束。
                   semaphore本质是个计数器，基本逻辑是基于acquire/release 
                   如果semphore数值被初始化为1.一个线程就可以通过acquire进入互斥状态，本质上和互斥锁是非常相似的，区别也非常明显，比如互斥锁有持有者的，而对于semaphore这种技术结构，虽然有类似功能，不存在正真意义的持有者。

                 CountDownLatch和CyclicBarrier 区别
                    CountDownLatch是不可重置的无法重用，而CylicBarrier则没有这种限制，可以重用
                    CountDownLatch的基本组合是countDown/await  调用await的线程等待阻塞够countDown足够的次数，
                    不管你是在一个线程还是多个线程里countDown,只要次数足够即可。CountDownLatch操作的事事件
                    CylicBarrier的基本操作组合是await ,当所有的伙伴都调用了await才会继续进行任务，并自行进行重置。
                    正常情况下，CylicBarrier的重置是自动发生的，如果我们调用reset方法，淡还有线程在等待，就会导致线程被打扰，抛出BrokenBarrierException异常。CylicBarrier侧重点是线程，而不是调用事件，典型的应用场景是等待并发线程结束。

                    CountDownLatch的调度方式相对简单，后一批次的线程进行await，等待前一批countDown足够多次。
                    CylicBarrier其实反映的线程并行时的协调，使用cylicBarrier特有的barrierAction,当屏障被触发时，java会自动调度该动作，因为CylicBarrier会自动进行重置；   
        -- 并发集合
           -- CopyOnWriteArrayList 源码解析和设计思路
              在 ArrayList 的类注释上，JDK 就提醒了我们，如果要把 ArrayList 作为共享变量的话，是线程不安全的，推荐我们自己加锁或者使用 Collections.synchronizedList 方法，其实 JDK 还提供了另外一种线程安全的 List，叫做 CopyOnWriteArrayList，这个 List 具有以下特征： 
               线程安全的，多线程环境下可以直接使用，无需加锁；
               通过锁 + 数组拷贝 + volatile 关键字保证了线程安全；
               每次数组操作，都会把数组拷贝一份出来，在新数组上进行操作，操作成功之后再赋值回去。        
              整体架构 
                从整体架构上来说，CopyOnWriteArrayList 数据结构和 ArrayList 是一致的，底层是个数组，只不过 CopyOnWriteArrayList 在对数组进行操作的时候，基本会分四步走：
                  加锁；
                  从原数组中拷贝出新数组；
                  在新数组上进行操作，并把新数组赋值给数组容器；
                  解锁。
                除了加锁之外，CopyOnWriteArrayList 的底层数组还被 volatile 关键字修饰，意思是一旦数组被修改，其它线程立马能够感知到，代码如下
                  private transient volatile Object[] array;
                CopyOnWriteArrayList 就是利用锁 + 数组拷贝 + volatile 关键字保证了 List 的线程安全。  
              类注释
                所有的操作都是线程安全的，因为操作都是在新拷贝数组上进行的；
                数组的拷贝虽然有一定的成本，但往往比一般的替代方案效率高；
                迭代过程中，不会影响到原来的数组，也不会抛出 ConcurrentModificationException 异常


              新增
               新增到数组尾部、新增到数组某一个索引位置、批量新增等等，操作的思路还是我们开头说的四步   
                 // 加锁
                 lock.lock();
                 int len = elements.length;
                 // 拷贝到新数组里面，新数组的长度是 + 1 的，因为新增会多一个元素
                 Object[] newElements = Arrays.copyOf(elements, len + 1);
                 // 在新数组中进行赋值，新元素直接放在数组的尾部
                 newElements[len] = e;
                 // 替换掉原来的数组
                 setArray(newElements);
                 finally {
                 lock.unlock();
                还会从老数组中创建出一个新数组，然后把老数组的值拷贝到新数组上，这时候就有一个问题：都已经加锁了，为什么需要拷贝数组，而不是在原来数组上面进行操作呢，原因主要为：
                  volatile 关键字修饰的是数组，如果我们简单的在原来数组上修改其中某几个元素的值，是无法触发可见性的，我们必须通过修改数组的内存地址才行，也就说要对数组进行重新赋值才行。
                  在新的数组上进行拷贝，对老数组没有任何影响，只有新数组完全拷贝完成之后，外部才能访问到，降低了在赋值过程中，老数组数据变动的影响。
                简单 add 操作是直接添加到数组的尾部，接着我们来看下指定位置添加元素的关键源码 
                  // 如果要插入的位置在数组的中间，就需要拷贝 2 次
                  // 第一次从 0 拷贝到 index。
                  // 第二次从 index+1 拷贝到末尾。 
                小结
                  加锁：保证同一时刻数组只能被一个线程操作；
                  数组拷贝：保证数组的内存地址被修改，修改后触发 volatile 的可见性，其它线程可以立马知道数组已经被修改；
                  volatile：值被修改后，其它线程能够立马感知最新值。
                  3 个要素缺一不可，比如说我们只使用 1 和 3 ，去掉 2，这样当我们修改数组中某个值时，并不会触发 volatile 的可见特性的，只有当数组内存地址被修改后，才能触发把最新值通知给其他线程的特性

              删除
                加锁；
                判断删除索引的位置，从而进行不同策略的拷贝；
                解锁。 
                小结
                锁 + try finally +数组拷贝，锁被 final 修饰的，保证了在加锁过程中，锁的内存地址肯定不会被修改，finally 保证锁一定能够被释放，数组拷贝是为了删除其中某个位置的元素。     
              批量删除
                 
              其他方法
           -- ConcurrentHashMap 源码解析和设计思路   
             当我们碰到线程不安全场景下，需要使用 Map 的时候，我们第一个想到的 API 估计就是 ConcurrentHashMap，ConcurrentHashMap 内部封装了锁和各种数据结构来保证访问 Map 是线程安全的
             类注释
               所有的操作都是线程安全的，我们在使用时，无需再加锁；
               多个线程同时进行 put、remove 等操作时并不会阻塞，可以同时进行，和 HashTable 不同，HashTable 在操作时，会锁住整个 Map；
               迭代过程中，即使 Map 结构被修改，也不会抛 ConcurrentModificationException 异常；
               除了数组 + 链表 + 红黑树的基本结构外，新增了转移节点，是为了保证扩容时的线程安全的节点；
               提供了很多 Stream 流式方法，比如说：forEach、search、reduce 等等
             结构 
                 map->concurrentMap&AbstractMap->ConcurrentMap
 
                 ConcurrentHashMap 和 HashMap 两者的相同之处：
                   数组、链表结构几乎相同，所以底层对数据结构的操作思路是相同的（只是思路相同，底层实现不同）；
                   都实现了 Map 接口，继承了 AbstractMap 抽象类，所以大多数的方法也都是相同的，HashMap 有的方法，ConcurrentHashMap 几乎都有，所以当我们需要从 HashMap 切换到 ConcurrentHashMap 时，无需关心两者之间的兼容问题。
                  不同之处：
                    红黑树结构略有不同，HashMap 的红黑树中的节点叫做 TreeNode，TreeNode 不仅仅有属性，还维护着红黑树的结构，比如说查找，新增等等；ConcurrentHashMap 中红黑树被拆分成两块，TreeNode 仅仅维护的属性和查找功能，新增了 TreeBin，来维护红黑树结构，并负责根节点的加锁和解锁；
                     新增 ForwardingNode （转移）节点，扩容的时候会使用到，通过使用该节点，来保证扩容时的线程安全。
             Put 
               大体思路
                 1如果数组为空，初始化，初始化完成之后，走 2；
                 2计算当前槽点有没有值，没有值的话，cas 创建，失败继续自旋（for 死循环），直到成功，槽点有值的话，走 3；
                 3如果槽点是转移节点(正在扩容)，就会一直自旋等待扩容完成之后再新增，不是转移节点走 4；
                 4槽点有值的，先锁定当前槽点，保证其余线程不能操作，如果是链表，新增值到链表的尾部，如果是红黑树，使用红黑树新增的方法新增；
                 5新增完成之后 check 需不需要扩容，需要的话去扩容。
               源码
                  //table是空的，进行初始化
                 if (tab == null || (n = tab.length) == 0)
                   tab = initTable();  
                   //如果当前索引位置没有值，直接创建
                  else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
                  //cas 在 i 位置创建新的元素，当 i 位置是空时，即能创建成功，结束for自循，否则继续自旋
                    //如果当前槽点是转移节点，表示该槽点正在扩容，就会一直等待扩容完成
                  //转移节点的 hash 值是固定的，都是 MOVED
                  else if ((fh = f.hash) == MOVED)
                  tab = helpTransfer(tab, f);  
                  //槽点上有值的
                  else {
                  V oldVal = null;
                   / /锁定当前槽点，其余线程不能操作，保证了安全
                   synchronized (f) {
                   //这里再次判断 i 索引位置的数据没有被修改
                   //binCount 被赋值的话，说明走到了修改表的过程里面
                   if (tabAt(tab, i) == f) {
                     //把新增的元素赋值到链表的最后，退出自旋
                            if ((e = e.next) == null) {
                                pred.next = new Node<K,V>(hash, key,
                                                          value, null);
                    //红黑树，这里没有使用 TreeNode,使用的是 TreeBin，TreeNode 只是红黑树的一个节点
                    //TreeBin 持有红黑树的引用，并且会对其加锁，保证其操作的线程安全
                    else if (f instanceof TreeBin) {
                        Node<K,V> p;
                        binCount = 2;
                        //满足if的话，把老的值给oldVal
                        //在putTreeVal方法里面，在给红黑树重新着色旋转的时候
                        //会锁住红黑树的根节点
                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                       value)) != null) {                                       
                     // 链表是否需要转化成红黑树

                      if (binCount >= TREEIFY_THRESHOLD)
                       treeifyBin(tab, i);
                     if (oldVal != null)
                       return oldVal;   
                  //check 容器是否需要扩容，如果需要去扩容，调用 transfer 方法去扩容
                 //如果已经在扩容中了，check有无完成
                  addCount(1L, binCount);                                       
             数组初始化时的线程安全      
               数组初始化时，首先通过自旋来保证一定可以初始化成功，然后通过 CAS 设置 SIZECTL 变量的值，来保证同一时刻只能有一个线程对数组进行初始化，CAS 成功之后，还会再次判断当前数组是否已经初始化完成，如果已经初始化完成，就不会再次初始化，通过自旋 + CAS + 双重 check 等手段保证了数组初始化时的线程安全
                   //通过自旋保证初始化成功
                 while ((tab = table) == null || tab.length == 0) {
                // 小于 0 代表有线程正在初始化，释放当前 CPU 的调度权，重新发起锁的竞争
                if ((sc = sizeCtl) < 0)
                   Thread.yield(); // lost initialization race; just spin
                // CAS 赋值保证当前只有一个线程在初始化，-1 代表当前只有一个线程能初始化
                // 保证了数组的初始化的安全性
                else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {
                 try {
                // 很有可能执行到这里的时候，table 已经不为空了，这里是双重 check
                if ((tab = table) == null || tab.length == 0) 
             新增槽点值时的线程安全  
               四点优化 
               1通过自旋死循环保证一定可以新增成功。
                在新增之前，通过 for (Node<K,V>[] tab = table;;) 这样的死循环来保证新增一定可以成功，一旦新增成功，就可以退出当前死循环，新增失败的话，会重复新增的步骤，直到新增成功为止。
               2 当前槽点为空时，通过 CAS 新增。 
                 Java 这里的写法非常严谨，没有在判断槽点为空的情况下直接赋值，因为在判断槽点为空和赋值的瞬间，很有可能槽点已经被其他线程赋值了，所以我们采用 CAS 算法，能够保证槽点为空的情况下赋值成功，如果恰好槽点已经被其他线程赋值，当前 CAS 操作失败，会再次执行 for 自旋，再走槽点有值的 put 流程，这里就是自旋 + CAS 的结合。
               3 当前槽点有值，锁住当前槽点。  
                 put 时，如果当前槽点有值，就是 key 的 hash 冲突的情况，此时槽点上可能是链表或红黑树，我们通过锁住槽点，来保证同一时刻只会有一个线程能对槽点进行修改
               4 红黑树旋转时，锁住红黑树的根节点，保证同一时刻，当前红黑树只能被一个线程旋转  
             扩容时的线程安全  
               ConcurrentHashMap 扩容的方法叫做 transfer，从 put 方法的 addCount 方法进去，就能找到 transfer 方法，transfer 方法的主要思路是：
                 首先需要把老数组的值全部拷贝到扩容之后的新数组上，先从数组的队尾开始拷贝；
                 拷贝数组的槽点时，先把原数组槽点锁住，保证原数组槽点不能操作，成功拷贝到新数组时，把原数组槽点赋值为转移节点；
                 这时如果有新数据正好需要 put 到此槽点时，发现槽点为转移节点，就会一直等待，所以在扩容完成之前，该槽点对应的数据是不会发生变化的；
                 从数组的尾部拷贝到头部，每拷贝成功一次，就把原数组中的节点设置成转移节点；
                 直到所有数组数据都拷贝到新数组时，直接把新数组整个赋值给数组容器，拷贝完成。
               源码
                   // 扩容主要分 2 步，第一新建新的空数组，第二移动拷贝每个元素到新数组中去
                   // tab：原数组，nextTab：新数组
                      // 如果新数组为空，初始化，大小为原数组的两倍，n << 1
                  if (nextTab == null) {            // initiating
                  Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n << 1];
                  nextTab = nt；
                   // 无限自旋，i 的值会从原数组的最大值开始，慢慢递减到 0
                   // 结束循环的标志
                  if (--i >= bound || finishing)
                    advance = false;
                 // 已经拷贝完成
                 else if ((nextIndex = transferIndex) <= 0) {
                   i = -1;
                    advance = false;
                   }
                  // 每次减少 i 的值
                  else if (U.compareAndSwapInt
                     (this, TRANSFERINDEX, nextIndex,
                      nextBound = (nextIndex > stride ?
                                   nextIndex - stride : 0))) {
                     bound = nextBound;
                     i = nextIndex - 1;
                     advance = false;
                   }
                    // if 任意条件满足说明拷贝结束了
                  if (i < 0 || i >= n || i + n >= nextn) {
                   int sc;
                     // 拷贝结束，直接赋值，因为每次拷贝完一个节点，都在原数组上放转移节点，所以拷贝完成的节点的数据一定不会再发生变化。
                    // 原数组发现是转移节点，是不会操作的，会一直等待转移节点消失之后在进行操作。
                    // 也就是说数组节点一旦被标记为转移节点，是不会再发生任何变动的，所以不会有任何线程安全的问题
                    // 所以此处直接赋值，没有任何问题。 
                    // 在新数组位置上放置拷贝的值
                        setTabAt(nextTab, i, ln);
                        setTabAt(nextTab, i + n, hn);
                        // 在老数组位置上放上 ForwardingNode 节点



                        // put 时，发现是 ForwardingNode 节点，就不会再动这个节点的数据了
                        setTabAt(tab, i, fwd);
                小结
                  拷贝槽点时，会把原数组的槽点锁住；
                  拷贝成功之后，会把原数组的槽点设置成转移节点，这样如果有数据需要 put 到该节点时，发现该槽点是转移节点，会一直等待，直到扩容成功之后，才能继续 put，可以参考 put 方法中的 helpTransfer 方法；
                  从尾到头进行拷贝，拷贝成功就把原数组的槽点设置成转移节点。
                  等扩容拷贝都完成之后，直接把新数组的值赋值给数组容器，之前等待 put 的数据才能继续 put。         
             get

               ConcurrentHashMap 读的话，就比较简单，先获取数组的下标，然后通过判断数组下标的 key 是否和我们的 key 相等，相等的话直接返回，如果下标的槽点是链表或红黑树的话，分别调用相应的查找数据的方法

           -- 并发 List、Map源码面试题
              1 CopyOnWriteArrayList 相关
                和 ArrayList 相比有哪些相同点和不同点？
                   相同点：底层的数据结构是相同的，都是数组的数据结构，提供出来的 API 都是对数组结构进行操作，让我们更好的使用。

                   不同点：后者是线程安全的，在多线程环境下使用，无需加锁，可直接使用。
                CopyOnWriteArrayList 通过哪些手段实现了线程安全？
                  1. 数组容器被 volatile 关键字修饰，保证了数组内存地址被任意线程修改后，都会通知到其他线程；
                  2对数组的所有修改操作，都进行了加锁，保证了同一时刻，只能有一个线程对数组进行修改，比如我在 add 时，就无法 remove；
                  3 修改过程中对原数组进行了复制，是在新数组上进行修改的，修改过程中，不会对原数组产生任何影响。
                在 add 方法中，对数组进行加锁后，不是已经是线程安全了么，为什么还需要对老数组进行拷贝？
                    对数组进行加锁后，能够保证同一时刻，只有一个线程能对数组进行 add，在同单核 CPU 下的多线程环境下肯定没有问题，但我们现在的机器都是多核 CPU，如果我们不通过复制拷贝新建数组，修改原数组容器的内存地址的话，是无法触发 volatile 可见性效果的，那么其他 CPU 下的线程就无法感知数组原来已经被修改了，就会引发多核 CPU 下的线程安全问题。
                     假设我们不复制拷贝，而是在原来数组上直接修改值，数组的内存地址就不会变，而数组被 volatile 修饰时，必须当数组的内存地址变更时，才能及时的通知到其他线程，内存地址不变，仅仅是数组元素值发生变化时，是无法把数组元素值发生变动的事实，通知到其它线程的
                对老数组进行拷贝，会有性能损耗，我们平时使用需要注意什么么？     
                    批量操作时，尽量使用 addAll、removeAll 方法，而不要在循环里面使用 add、remove 方法，主要是因为 for 循环里面使用 add 、remove 的方式，在每次操作时，都会进行一次数组的拷贝(甚至多次)，非常耗性能，而 addAll、removeAll 方法底层做了优化，整个操作只会进行一次数组拷贝，由此可见，当批量操作的数据越多时，批量方法的高性能体现的越明显。

                为什么 CopyOnWriteArrayList 迭代过程中，数组结构变动，不会抛出ConcurrentModificationException 了
                    主要是因为 CopyOnWriteArrayList 每次操作时，都会产生新的数组，而迭代时，持有的仍然是老数组的引用，所以我们说的数组结构变动，是用新数组替换了老数组，老数组的结构并没有发生变化，所以不会抛出异常了。
                插入的数据正好在 List 的中间，请问两种 List 分别拷贝数组几次？为什么？    
                    ArrayList 只需拷贝一次，假设插入的位置是 2，只需要把位置 2 （包含 2）后面的数据都往后移动一位即可，所以拷贝一次。
                    CopyOnWriteArrayList 拷贝两次，因为 CopyOnWriteArrayList 多了把老数组的数据拷贝到新数组上这一步，可能有的同学会想到这种方式：先把老数组拷贝到新数组，再把 2 后面的数据往后移动一位，这的确是一种拷贝的方式，但 CopyOnWriteArrayList 底层实现更加灵活，而是：把老数组 0 到 2 的数据拷贝到新数组上，预留出新数组 2 的位置，再把老数组 3～ 最后的数据拷贝到新数组上，这种拷贝方式可以减少我们拷贝的数据，虽然是两次拷贝，但拷贝的数据却仍然是老数组的大小，设计的非常巧妙
              2 ConcurrentHashMap 相关
                 ConcurrentHashMap 和 HashMap 的相同点和不同点     
                   相同点：
                    1. 都是数组 + 链表 +红黑树的数据结构，所以基本操作的思想相同；
                    2. 都实现了 Map 接口，继承了 AbstractMap 抽象类，所以两者的方法大多都是相似的，可以互相切换。
                   不同点：
                    1. ConcurrentHashMap 是线程安全的，在多线程环境下，无需加锁，可直接使用；
                    2.数据结构上，ConcurrentHashMap 多了转移节点，主要用于保证扩容时的线程安全
                 ConcurrentHashMap 通过哪些手段保证了线程安全
                   储存 Map 数据的数组被 volatile 关键字修饰，一旦被修改，立马就能通知其他线程，因为是数组，所以需要改变其内存值，才能真正的发挥出 volatile 的可见特性；
                   put 时，如果计算出来的数组下标索引没有值的话，采用无限 for 循环 + CAS 算法，来保证一定可以新增成功，又不会覆盖其他线程 put 进去的值；
                   如果 put 的节点正好在扩容，会等待扩容完成之后，再进行 put ，保证了在扩容时，老数组的值不会发生变化；
                   对数组的槽点进行操作时，会先锁住槽点，保证只有当前线程才能对槽点上的链表或红黑树进行操作；
                   红黑树旋转时，会锁住根节点，保证旋转时的线程安全。    
                 描述一下 CAS 算法在 ConcurrentHashMap 中的应用？  
                    CAS 其实是一种乐观锁，一般有三个值，分别为：赋值对象，原值，新值，在执行的时候，会先判断内存中的值是否和原值相等，相等的话把新值赋值给对象，否则赋值失败，整个过程都是原子性操作，没有线程安全问题。
                    ConcurrentHashMap 的 put 方法中，有使用到 CAS ，是结合无限 for 循环一起使用的，步骤如下：
                      1计算出数组索引下标，拿出下标对应的原值；
                      2CAS 覆盖当前下标的值，赋值时，如果发现内存值和 1 拿出来的原值相等，执行赋值，退出循环，否则不赋值，转到 3；
                      3进行下一次 for 循环，重复执行 1，2，直到成功为止
                 ConcurrentHashMap 是如何发现当前槽点正在扩容的。      

                 发现槽点正在扩容时，put 操作会怎么办

                    无限 for 循环，或者走到扩容方法中去，帮助扩容，一直等待扩容完成之后，再执行 put 操作
                 两种map扩容，有啥区别
                   区别很大，HashMap 是直接在老数据上面进行扩容，多线程环境下，会有线程安全的问题，而 




                   ConcurrentHashMap 就不太一样，扩容过程是这样的：
                       从数组的队尾开始拷贝；
                       拷贝数组的槽点时，先把原数组槽点锁住，拷贝成功到新数组时，把原数组槽点赋值为转移节点；
                       从数组的尾部拷贝到头部，每拷贝成功一次，就把原数组的槽点设置成转移节点；
                       直到所有数组数据都拷贝到新数组时，直接把新数组整个赋值给数组容器，拷贝完成。
                   简单来说，通过扩容时给槽点加锁，和发现槽点正在扩容就等待的策略，保证了 ConcurrentHashMap 可以慢慢一个一个槽点的转移，保证了扩容时的线程安全，转移节点比较重要，平时问的人也比较多。   
                 ConcurrentHashMap 在 Java 7 和 8 中关于线程安全的做法有啥不同？ 
                    非常不一样，拿 put 方法为例，Java 7 的做法是：
                      把数组进行分段，找到当前 key 对应的是那一段；
                      将当前段锁住，然后再根据 hash 寻找对应的值，进行赋值操作。
                    Java 7 的做法比较简单，缺点也很明显，就是当我们需要 put 数据时，我们会锁住改该数据对应的某一段，这一段数据可能会有很多，比如我只想 put 一个值，锁住的却是一段数据，导致这一段的其他数据都不能进行写入操作，大大的降低了并发性的效率。Java 8 解决了这个问题，从锁住某一段，修改成锁住某一个槽点，提高了并发效率。

                    不仅仅是 put，删除也是，仅仅是锁住当前槽点，缩小了锁的范围，增大了效率   
           -- 并发容器：都有哪些“坑”需要我们填
              同步容器及其注意事项  
                Java 中的容器主要可以分为四个大类，分别是 List、Map、Set 和 Queue，但并不是所有的 Java 容器都是线程安全的。例如，我们常用的 ArrayList、HashMap 就不是线程安全的。       
                只要把非线程安全的容器封装在对象内部，然后控制好访问路径就可以了。
                Java SDK 的开发人员也想到了，所以他们在 Collections 这个类中还提供了一套完备的包装类，比如下面的示例代码中，分别把 ArrayList、HashSet 和 HashMap 包装成了线程安全的 List、Set 和 Map   

                组合操作需要注意竞态条件问题，例如上面提到的 addIfNotExist() 方法就包含组合操作。组合操作往往隐藏着竞态条件问题，即便每个操作都能保证原子性，也并不能保证组合操作的原子性，这个一定要注意。

                在容器领域一个容易被忽视的“坑”是用迭代器遍历容器，例如在下面的代码中，通过迭代器遍历容器 list，对每个元素调用 foo() 方法，这就存在并发问题，这些组合的操作不具备原子性。

                而正确做法是下面这样，锁住 list 之后再执行遍历操作。如果你查看 Collections 内部的包装类源码，你会发现包装类的公共方法锁的是对象的 this，其实就是我们这里的 list，所以锁住 list 绝对是线程安全的。

                上面我们提到的这些经过包装后线程安全容器，都是基于 synchronized 这个同步关键字实现的，所以也被称为同步容器。Java 提供的同步容器还有 Vector、Stack 和 Hashtable，这三个容器不是基于包装类实现的，但同样是基于 synchronized 实现的，对这三个容器的遍历，同样要加锁保证互斥。  
              并发容器及其注意事项 
                Java 在 1.5 版本之前所谓的线程安全的容器，主要指的就是同步容器。不过同步容器有个最大的问题，那就是性能差，所有方法都用 synchronized 来保证互斥，串行度太高了。因此 Java 在 1.5 及之后版本提供了性能更高的容器，我们一般称为并发容器
              （一）List 
                List 里面只有一个实现类就是 CopyOnWriteArrayList。CopyOnWrite，顾名思义就是写的时候会将共享变量新复制一份出来，这样做的好处是读操作完全无锁

                CopyOnWriteArrayList 内部维护了一个数组，成员变量 array 就指向这个内部数组，所有的读操作都是基于 array 进行的，如下图所示，迭代器 Iterator 遍历的就是 array 数组

                如果在遍历 array 的同时，还有一个写操作，例如增加元素，CopyOnWriteArrayList 是如何处理的呢？CopyOnWriteArrayList 会将 array 复制一份，然后在新复制处理的数组上执行增加元素的操作，执行完之后再将 array 指向这个新的数组。通过下图你可以看到，读写是可以并行的，遍历操作一直都是基于原 array 执行，而写操作则是基于新 array 进行。

                使用 CopyOnWriteArrayList 需要注意的“坑”主要有两个方面。一个是应用场景，CopyOnWriteArrayList 仅适用于写操作非常少的场景，而且能够容忍读写的短暂不一致。例如上面的例子中，写入的新元素并不能立刻被遍历到。另一个需要注意的是，CopyOnWriteArrayList 迭代器是只读的，不支持增删改。因为迭代器遍历的仅仅是一个快照，而对快照进行增删改是没有意义的。

                （二）Map
                Map 接口的两个实现是 ConcurrentHashMap 和 ConcurrentSkipListMap，它们从应用的角度来看，主要区别在于 ConcurrentHashMap 的 key 是无序的，而 ConcurrentSkipListMap 的 key 是有序的。所以如果你需要保证 key 的顺序，就只能使用 ConcurrentSkipListMap

                使用 ConcurrentHashMap 和 ConcurrentSkipListMap 需要注意的地方是，它们的 key 和 value 都不能为空，否则会抛出NullPointerException这个运行时异常。下面这个表格总结了 Map 相关的实现类对于 key 和 value 的要求，你可以对比学习。

                ConcurrentSkipListMap 里面的 SkipList 本身就是一种数据结构，中文一般都翻译为“跳表”。跳表插入、删除、查询操作平均的时间复杂度是 O(log n)，理论上和并发线程数没有关系，所以在并发度非常高的情况下，若你对 ConcurrentHashMap 的性能还不满意，可以尝试一下 ConcurrentSkipListMap。

                （三）Set
                （四）Queue
                 Java 并发包里面 Queue 这类并发容器是最复杂的，你可以从以下两个维度来分类。一个维度是阻塞与非阻塞，所谓阻塞指的是当队列已满时，入队操作阻塞；当队列已空时，出队操作阻塞。另一个维度是单端与双端，单端指的是只能队尾入队，队首出队；而双端指的是队首队尾皆可入队出队。Java 并发包里阻塞队列都用 Blocking 关键字标识，单端队列使用 Queue 标识，双端队列使用 Deque 标识。

                 1.单端阻塞队列：其实现有 ArrayBlockingQueue、LinkedBlockingQueue、SynchronousQueue、LinkedTransferQueue、PriorityBlockingQueue 和 DelayQueue。内部一般会持有一个队列，这个队列可以是数组（其实现是 ArrayBlockingQueue）也可以是链表（其实现是 LinkedBlockingQueue）；甚至还可以不持有队列（其实现是 SynchronousQueue），此时生产者线程的入队操作必须等待消费者线程的出队操作。而 LinkedTransferQueue 融合 LinkedBlockingQueue 和 SynchronousQueue 的功能，性能比 LinkedBlockingQueue 更好；PriorityBlockingQueue 支持按照优先级出队；DelayQueue 支持延时出队
                 2.双端阻塞队列：其实现是 LinkedBlockingDeque。
                 3.单端非阻塞队列：其实现是 ConcurrentLinkedQueue
                 4.双端非阻塞队列：其实现是 ConcurrentLinkedDeque。
                 另外，使用队列时，需要格外注意队列是否支持有界（所谓有界指的是内部的队列是否有容量限制）。实际工作中，一般都不建议使用无界的队列，因为数据量大了之后很容易导致 OOM。上面我们提到的这些 Queue 中，只有 ArrayBlockingQueue 和 LinkedBlockingQueue 是支持有界的，所以在使用其他无界队列时，一定要充分考虑是否存在导致 OOM 的隐患。
        -- 其他并发模型
             --Actor模型：面向对象原生的并发模型
                 按照面向对象编程的理论，对象之间通信需要依靠消息，而实际上，像 C++、Java 这些面向对象的语言，对象之间通信，依靠的是对象方法。对象方法和过程语言里的函数本质上没有区别，有入参、有出参，思维方式很相似，使用起来都很简单。那面向对象理论里的消息是否就等价于面向对象语言里的对象方法呢？很长一段时间里，我都以为对象方法是面向对象理论中消息的一种实现，直到接触到 Actor 模型，才明白消息压根不是这个实现法。

                 Hello Actor 模型

                    Actor 模型本质上是一种计算模型，基本的计算单元称为 Actor，换言之，在 Actor 模型中，所有的计算都是在 Actor 中执行的。在面向对象编程里面，一切都是对象；在 Actor 模型里，一切都是 Actor，并且 Actor 之间是完全隔离的，不会共享任何变量

                    当看到“不共享任何变量”的时候，相信你一定会眼前一亮，并发问题的根源就在于共享变量，而 Actor 模型中 Actor 之间不共享变量，那用 Actor 模型解决并发问题，一定是相当顺手。的确是这样，所以很多人就把 Actor 模型定义为一种并发计算模型。其实 Actor 模型早在 1973 年就被提出来了，只是直到最近几年才被广泛关注，一个主要原因就在于它是解决并发问题的利器，而最近几年随着多核处理器的发展，并发问题被推到了风口浪尖上


                    但是 Java 语言本身并不支持 Actor 模型，所以如果你想在 Java 语言里使用 Actor 模型，就需要借助第三方类库，目前能完备地支持 Actor 模型而且比较成熟的类库就是 Akka 了



                    在下面的示例代码中，我们首先创建了一个 ActorSystem（Actor 不能脱离 ActorSystem 存在）；之后创建了一个 HelloActor，Akka 中创建 Actor 并不是 new 一个对象出来，而是通过调用 system.actorOf() 方法创建的，该方法返回的是 ActorRef，而不是 HelloActor；最后通过调用 ActorRef 的 tell() 方法给 HelloActor 发送了一条消息 “Actor”

                    //该Actor当收到消息message后，//会打印Hello messagestatic class HelloActor     extends UntypedActor {  @Override  public void onReceive(Object message) {    System.out.println("Hello " + message);  }}

                    public static void main(String[] args) { //创建Actor系统 ActorSystem system = ActorSystem.create("HelloSystem"); //创建HelloActor ActorRef helloActor = system.actorOf(Props.create(HelloActor.class)); //发送消息给HelloActor helloActor.tell("Actor", ActorRef.noSender());}

                    通过这个例子，你会发现 Actor 模型和面向对象编程契合度非常高，完全可以用 Actor 类比面向对象编程里面的对象，而且 Actor 之间的通信方式完美地遵守了消息机制，而不是通过对象方法来实现对象之间的通信。那 Actor 中的消息机制和面向对象语言里的对象方法有什么区别呢？  
                 消息和对象方法的区别
                     Actor 中的消息机制，就可以类比这现实世界里的写信。Actor 内部有一个邮箱（Mailbox），接收到的消息都是先放到邮箱里，如果邮箱里有积压的消息，那么新收到的消息就不会马上得到处理，也正是因为 Actor 使用单线程处理消息，所以不会出现并发问题。你可以把 Actor 内部的工作模式想象成只有一个消费者线程的生产者 - 消费者模式

                     所以，在 Actor 模型里，发送消息仅仅是把消息发出去而已，接收消息的 Actor 在接收到消息后，也不一定会立即处理，也就是说 Actor 中的消息机制完全是异步的。而调用对象方法，实际上是同步的，对象方法 return 之前，调用方会一直等待   

                     除此之外，调用对象方法，需要持有对象的引用，所有的对象必须在同一个进程中。而在 Actor 中发送消息，类似于现实中的写信，只需要知道对方的地址就可以，发送消息和接收消息的 Actor 可以不在一个进程中，也可以不在同一台机器上。因此，Actor 模型不但适用于并发计算，还适用于分布式计算  
                 Actor 的规范化定义
                      Actor 是一种基础的计算单元，具体来讲包括三部分能力，分别是：
                      处理能力，处理接收到的消息。
                      存储能力，Actor 可以存储自己的内部状态，并且内部状态在不同 Actor 之间是绝对隔离的。
                      通信能力，Actor 可以和其他 Actor 之间通信
                      当一个 Actor 接收的一条消息之后，这个 Actor 可以做以下三件事：
                      创建更多的 Actor；
                      发消息给其他 Actor；
                      确定如何处理下一条消息。 

                      其中前两条还是很好理解的，就是最后一条，该如何去理解呢？前面我们说过 Actor 具备存储能力，它有自己的内部状态，所以你也可以把 Actor 看作一个状态机，把 Actor 处理消息看作是触发状态机的状态变化；而状态机的变化往往要基于上一个状态，触发状态机发生变化的时刻，上一个状态必须是确定的，所以确定如何处理下一条消息，本质上不过是改变内部状态。   
                      
                      在多线程里面，由于可能存在竞态条件，所以根据当前状态确定如何处理下一条消息还是有难度的，需要使用各种同步工具，但在 Actor 模型里，由于是单线程处理，所以就不存在竞态条件问题了。
                 用 Actor 实现累加器
                      支持并发的累加器可能是最简单并且有代表性的并发问题了，可以基于互斥锁方案实现，也可以基于原子类实现，但今天我们要尝试用 Actor 来实现。     

                      在下面的示例代码中，CounterActor 内部持有累计值 counter，当 CounterActor 接收到一个数值型的消息 message 时，就将累计值 counter += message；但如果是其他类型的消息，则打印当前累计值 counter。在 main() 方法中，我们启动了 4 个线程来执行累加操作。整个程序没有锁，也没有 CAS，但是程序是线程安全的


                      //累加器static class CounterActor extends UntypedActor {  private int counter = 0;  @Override  public void onReceive(Object message){    //如果接收到的消息是数字类型，执行累加操作，    //否则打印counter的值    if (message instanceof Number) {      counter += ((Number) message).intValue();    } else {      System.out.println(counter);    }  }}public static void main(String[] args) throws InterruptedException {  //创建Actor系统  ActorSystem system = ActorSystem.create("HelloSystem");  //4个线程生产消息  ExecutorService es = Executors.newFixedThreadPool(4);  //创建CounterActor   ActorRef counterActor =     system.actorOf(Props.create(CounterActor.class));  //生产4*100000个消息   for (int i=0; i<4; i++) {    es.execute(()->{      for (int j=0; j<100000; j++) {        counterActor.tell(1, ActorRef.noSender());      }    });  }  //关闭线程池  es.shutdown();  //等待CounterActor处理完所有消息  Thread.sleep(1000);  //打印结果  counterActor.tell("", ActorRef.noSender());  //关闭Actor系统  system.shutdown();}  

                 Actor 可以创建新的 Actor，这些 Actor 最终会呈现出一个树状结构，非常像现实世界里的组织结构，所以利用 Actor 模型来对程序进行建模，和现实世界的匹配度非常高。Actor 模型和现实世界一样都是异步模型，理论上不保证消息百分百送达，也不保证消息送达的顺序和发送的顺序是一致的，甚至无法保证消息会被百分百处理。虽然实现 Actor 模型的厂商都在试图解决这些问题，但遗憾的是解决得并不完美，所以使用 Actor 模型也是有成本的。     
             --软件事务内存：借鉴数据库的并发经验    
                  软件事务内存（Software Transactional Memory，简称 STM）。传统的数据库事务，支持 4 个特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability），也就是大家常说的 ACID，STM 由于不涉及到持久化，所以只支持 ACI。
                  
                  用 STM 实现转账
                     简单地使用 synchronized 将 transfer() 方法变成同步方法并不能解决并发问题，因为还存在死锁问题。
                     该转账操作若使用数据库事务就会非常简单，如下面的示例代码所示。如果所有 SQL 都正常执行，则通过 commit() 方法提交事务；如果 SQL 在执行过程中有异常，则通过 rollback() 方法回滚事务。数据库保证在并发情况下不会有死锁，而且还能保证前面我们说的原子性、一致性、隔离性和持久性，也就是 ACID。

                     //获取数据库连接  conn = DriverManager.getConnection();  //设置手动提交事务  conn.setAutoCommit(false);  //执行转账SQL  ......  //提交事务  conn.commit();} catch (Exception e) {  //出现异常回滚事务  conn.rollback();

                      Java 语言并不支持 STM，不过可以借助第三方的类库来支持，Multiverse就是个不错的选择。下面的示例代码就是借助 Multiverse 实现了线程安全的转账操作，相比较上面线程不安全的 UnsafeAccount，其改动并不大，仅仅是将余额的类型从 long 变成了 TxnLong ，将转账的操作放到了 atomic(()->{}) 中。

                      class Account{  //余额  private TxnLong balance;  //构造函数  public Account(long balance){    this.balance = StmUtils.newTxnLong(balance);  }  //转账  public void transfer(Account to, int amt){    //原子化操作    atomic(()->{      if (this.balance.get() > amt) {        this.balance.decrement(amt);        to.balance.increment(amt);      }    });  }}

                      数据库事务发展了几十年了，目前被广泛使用的是 MVCC（全称是 Multi-Version Concurrency Control），也就是多版本并发控制

                      MVCC 可以简单地理解为数据库事务在开启的时候，会给数据库打一个快照，以后所有的读写都是基于这个快照的。当提交事务的时候，如果所有读写过的数据在该事务执行期间没有发生过变化，那么就可以提交；如果发生了变化，说明该事务和有其他事务读写的数据冲突了，这个时候是不可以提交的

                      为了记录数据是否发生了变化，可以给每条数据增加一个版本号，这样每次成功修改数据都会增加版本号的值。MVCC 的工作原理和我们曾经在《18 | StampedLock：有没有比读写锁更快的锁？》中提到的乐观锁非常相似。有不少 STM 的实现方案都是基于 MVCC 的，例如知名的 Clojure STM  

                  自己实现 STM    
                     我们首先要做的，就是让 Java 中的对象有版本号，在下面的示例代码中，VersionedRef 这个类的作用就是将对象 value 包装成带版本号的对象。按照 MVCC 理论，数据的每一次修改都对应着一个唯一的版本号，所以不存在仅仅改变 value 或者 version 的情况，用不变性模式就可以很好地解决这个问题，所以 VersionedRef 这个类被我们设计成了不可变的

                     所有对数据的读写操作，一定是在一个事务里面，TxnRef 这个类负责完成事务内的读写操作，读写操作委托给了接口 Txn，Txn 代表的是读写操作所在的当前事务， 内部持有的 curRef 代表的是系统中的最新值

                     //带版本号的对象引用public final class VersionedRef<T> {  final T value;  final long version;  //构造方法  public VersionedRef(T value, long version) {    this.value = value;    this.version = version;  }}

                     /支持事务的引用public class TxnRef { //当前数据，带版本号 volatile VersionedRef curRef; //构造方法 public TxnRef(T value) { this.curRef = new VersionedRef(value, 0L); } //获取当前事务中的数据 public T getValue(Txn txn) { return txn.get(this); } //在当前事务中设置数据 public void setValue(T value, Txn txn) { txn.set(this, value); }}

                     STMTxn 是 Txn 最关键的一个实现类，事务内对于数据的读写，都是通过它来完成的。STMTxn 内部有两个 Map：inTxnMap，用于保存当前事务中所有读写的数据的快照；writeMap，用于保存当前事务需要写入的数据。每个事务都有一个唯一的事务 ID txnId，这个 txnId 是全局递增的

                     STMTxn 有三个核心方法，分别是读数据的 get() 方法、写数据的 set() 方法和提交事务的 commit() 方法。其中，get() 方法将要读取数据作为快照放入 inTxnMap，同时保证每次读取的数据都是一个版本。set() 方法会将要写入的数据放入 writeMap，但如果写入的数据没被读取过，也会将其放入 inTxnMap。

                     至于 commit() 方法，我们为了简化实现，使用了互斥锁，所以事务的提交是串行的。commit() 方法的实现很简单，首先检查 inTxnMap 中的数据是否发生过变化，如果没有发生变化，那么就将 writeMap 中的数据写入（这里的写入其实就是 TxnRef 内部持有的 curRef）；如果发生过变化，那么就不能将 writeMap 中的数据写入了

                     //事务接口public interface Txn {  <T> T get(TxnRef<T> ref);  <T> void set(TxnRef<T> ref, T value);}//STM事务实现类public final class STMTxn implements Txn {  //事务ID生成器  private static AtomicLong txnSeq = new AtomicLong(0);    //当前事务所有的相关数据  private Map<TxnRef, VersionedRef> inTxnMap = new HashMap<>();  //当前事务所有需要修改的数据  private Map<TxnRef, Object> writeMap = new HashMap<>();  //当前事务ID  private long txnId;  //构造函数，自动生成当前事务ID  STMTxn() {    txnId = txnSeq.incrementAndGet();  }  //获取当前事务中的数据  @Override  public <T> T get(TxnRef<T> ref) {    //将需要读取的数据，加入inTxnMap    if (!inTxnMap.containsKey(ref)) {      inTxnMap.put(ref, ref.curRef);    }    return (T) inTxnMap.get(ref).value;  }

                     //提交事务  boolean commit() {    synchronized (STM.commitLock) {    //是否校验通过    boolean isValid = true;    //校验所有读过的数据是否发生过变化    for(Map.Entry<TxnRef, VersionedRef> entry : inTxnMap.entrySet()){      VersionedRef curRef = entry.getKey().curRef;      VersionedRef readRef = entry.getValue();      //通过版本号来验证数据是否发生过变化      if (curRef.version != readRef.version) {        isValid = false;        break;      }    }    //如果校验通过，则所有更改生效    if (isValid) {      writeMap.forEach((k, v) -> {        k.curRef = new VersionedRef(v, txnId);      });    }    return isValid;  }}

                     @FunctionalInterfacepublic interface TxnRunnable {  void run(Txn txn);}//STMpublic final class STM {  //私有化构造方法  private STM() {  //提交数据需要用到的全局锁    static final Object commitLock = new Object();  //原子化提交方法  public static void atomic(TxnRunnable action) {    boolean committed = false;    //如果没有提交成功，则一直重试    while (!committed) {      //创建新的事务      STMTxn txn = new STMTxn();      //执行业务逻辑      action.run(txn);      //提交事务      committed = txn.commit();    }  }}}

                     STM 借鉴的是数据库的经验，数据库虽然复杂，但仅仅存储数据，而编程语言除了有共享变量之外，还会执行各种 I/O 操作，很显然 I/O 操作是很难支持回滚的。所以，STM 也不是万能的。目前支持 STM 的编程语言主要是函数式语言，函数式语言里的数据天生具备不可变性，利用这种不可变性实现 STM 相对来说更简单























